<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes on Yeqown</title>
    <link>https://www.yeqown.xyz/categories/Kubernetes/</link>
    <description>Recent content in Kubernetes on Yeqown</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Thu, 08 May 2025 09:35:14 +0800</lastBuildDate>
    <atom:link href="https://www.yeqown.xyz/categories/Kubernetes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>在 Kubernetes 中实现 gRPC 流量的镜像和对比</title>
      <link>https://www.yeqown.xyz/2025/05/08/%E5%9C%A8-Kubernetes%E4%B8%AD%E5%AE%9E%E7%8E%B0gRPC%E6%B5%81%E9%87%8F%E9%95%9C%E5%83%8F/</link>
      <pubDate>Thu, 08 May 2025 09:35:14 +0800</pubDate>
      <guid>https://www.yeqown.xyz/2025/05/08/%E5%9C%A8-Kubernetes%E4%B8%AD%E5%AE%9E%E7%8E%B0gRPC%E6%B5%81%E9%87%8F%E9%95%9C%E5%83%8F/</guid>
      <description>&lt;p&gt;本文主要解决在服务重构过程中如何保证新旧服务行为一致性的问题。&lt;/p&gt;&#xA;&lt;h2 id=&#34;场景描述&#34;&gt;&#xA;  场景描述&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%9c%ba%e6%99%af%e6%8f%8f%e8%bf%b0&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;现有一个 python 开发的 gRPC 微服务提供了一些 &lt;strong&gt;数据查询&lt;/strong&gt; 接口 供 上层应用使用，随着业务流量的增加运维这个服务的成本也逐渐增加，为了降低运维成本和提高性能 (木有擅长 python 高性能的开发)，因此选择了使用 go 语言对这个服务进行重写。在开发完成之后，需要对新服务的 gRPC 接口进行验证。&lt;/p&gt;&#xA;&lt;p&gt;这种场景对测试开发人员来说，实在是太熟悉了吧？典型的 &lt;strong&gt;重放验证&lt;/strong&gt;，马上能想到的验证手段就是：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;如果有存量的单元测试，那么直接重新跑一遍单元测试就能快速的完成验证。&lt;/li&gt;&#xA;&lt;li&gt;没有单元测试的情况，那么可以将新服务部署起来，通过流量复制的方式将旧服务的流量复制到新服务上，然后对比两个服务的返回结果是否一致。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&#xA;&lt;script src=&#34;https://www.yeqown.xyz/mermaid.min.js&#34;&gt;&lt;/script&gt;&#xA;&#xA;  &lt;script&gt;mermaid.initialize({&#xA;  &#34;flowchart&#34;: {&#xA;    &#34;useMaxWidth&#34;:true&#xA;  },&#xA;  &#34;theme&#34;: &#34;default&#34;&#xA;}&#xA;)&lt;/script&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;flowchart LR&#xA;    %% 定义布局方向和间距&#xA;    subgraph s1[&#34;方案一: 单元测试验证&#34;]&#xA;        direction TB&#xA;        UT[单元测试] --&gt;|执行| NS1[新服务]&#xA;    end&#xA;    &#xA;    subgraph s2[&#34;方案二: 流量复制验证&#34;]&#xA;        direction TB&#xA;        C[客户端] --&gt;|请求| OS[旧服务]&#xA;        OS --&gt;|响应| C&#xA;        OS --&gt;|复制流量| NS2[新服务]&#xA;        NS2 --&gt;|对比响应| OS&#xA;    end&#xA;&#xA;    %% 设置布局方向和对齐方式&#xA;    s1 ~~~ s2&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;但是很遗憾 😭，并没有成熟的单元测试；测试人员也都是人肉测试，对于内部服务的接口验证帮助不大，因此这里采用第二种方式进行验证。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Istio Idle Timeout问题复现和解决</title>
      <link>https://www.yeqown.xyz/2024/03/25/istio-idle-timeout%E9%97%AE%E9%A2%98%E5%A4%8D%E7%8E%B0%E5%92%8C%E8%A7%A3%E5%86%B3/</link>
      <pubDate>Mon, 25 Mar 2024 09:37:25 +0800</pubDate>
      <guid>https://www.yeqown.xyz/2024/03/25/istio-idle-timeout%E9%97%AE%E9%A2%98%E5%A4%8D%E7%8E%B0%E5%92%8C%E8%A7%A3%E5%86%B3/</guid>
      <description>&lt;h2 id=&#34;更新&#34;&gt;&#xA;  更新&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9b%b4%e6%96%b0&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;更新-2024-04-01&#34;&gt;&#xA;  更新 2024-04-01&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9b%b4%e6%96%b0-2024-04-01&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;通过调整 tcp_proxy 的 idle_timeout 参数后，部分中间件（redis, mongo）的异常问题不再出现，但是 mysql(sharding-spere) 和 memcached 仍然存在 &amp;ldquo;invalid connection&amp;rdquo; 错误，所以还需要找到能够解决 mysql(sharding-spere) 和 memcached 的方法。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;上述描述的现象非常主观，不一定正确也不能作为最终结论，但是 idle_timeout 配置确实没有解决所有的问题。&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;这里，需要知道 istio 在 inject 时会通过 iptables 对应用的流量进行劫持，对于 outbound 的流量 iptables 规则拦截转发到 OUTPUT 链。OUTPUT 的链转发流量到 ISTIO_OUTPUT，这个链会决定服务访问外部服务的流量发往何处。&lt;/p&gt;&#xA;&lt;p&gt;这样产生的效果是，除了应用自己建立的连接之外 envoy 也会创建一个代理连接。&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ netstat -notp | grep &lt;span style=&#34;color:#ae81ff&#34;&gt;3307&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tcp         &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;             172.23.105.25:41030        x.x.x.x:3307         ESTABLISHED -      off&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;0.00/0/0&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tcp         &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;             172.23.105.25:41022        x.x.x.x:3307         ESTABLISHED 1/./app      keepalive&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;0.88/0/0&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;我遇到的问题可以确定问题就出在 envoy 建立的连接上，因为应用自己建立的连接是正常的，同时还可以看到应用自己的连接开启了 keepalive, 而 envoy 建立的连接没有开启。这样可能会出现这个连接会因为超时而被关闭的情况，或者其他原因导致连接被释放。那如果可以避免将中间件的流量通过 envoy 代理，这样就可以避免这个问题。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kubernetes中gRPC Load Balancing分析和解决</title>
      <link>https://www.yeqown.xyz/2020/09/22/Kubernetes%E4%B8%ADgRPC-Load-Balancing%E5%88%86%E6%9E%90%E5%92%8C%E8%A7%A3%E5%86%B3/</link>
      <pubDate>Tue, 22 Sep 2020 13:33:20 +0800</pubDate>
      <guid>https://www.yeqown.xyz/2020/09/22/Kubernetes%E4%B8%ADgRPC-Load-Balancing%E5%88%86%E6%9E%90%E5%92%8C%E8%A7%A3%E5%86%B3/</guid>
      <description>&lt;h3 id=&#34;背景&#34;&gt;&#xA;  背景&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%83%8c%e6%99%af&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;第一次，线上遇到大量接口RT超过10s触发了系统告警，运维反馈k8s集群无异常，负载无明显上升。将报警接口相关的服务重启一番后发现并无改善。但是开发人员使用链路追踪系统发现，比较慢的请求总是某个gRPC服务中的几个POD导致，由其他POD处理的请求并不会出现超时告警。&lt;/p&gt;&#xA;&lt;p&gt;第二次，同样遇到接口RT超过阈值触发告警，从k8s中查到某个gRPC服务（关键服务）重启次数异常，查看重启原因时发现是&lt;code&gt;OOM Killed&lt;/code&gt;，&lt;code&gt;OOM killed&lt;/code&gt;并不是负载不均衡直接导致的，但是也有一定的关系，这个后面再说。前两次由于监控不够完善（于我而言，运维的很多面板都没有权限，没办法排查）。期间利用pprof分析了该服务内存泄漏点，并修复上线观察。经过第二次问题并解决之后，线上超时告警恢复正常水平，但是该 deployment 下的几个POD占用资源（Mem / CPU / Network-IO），差距甚大。&lt;/p&gt;&#xA;&lt;img src=&#34;https://www.yeqown.xyz/images/k8s-grpc-lb-mem1.jpg&#34;/&gt;&#xA;&lt;img src=&#34;https://www.yeqown.xyz/images/k8s-grpc-lb-mem2.jpg&#34;/&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;第二张图是运维第一次发现该服务OOM killed 之后调整了内存上限从 512MB =&amp;gt; 1G，然而只是让它死得慢一点而已。&#xA;从上面两张图能够石锤的是该服务一定存在内存泄漏。Go项目内存占用的分析，我总结了如下的排查步骤：&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;1. 代码泄漏（pprof）（可能原因 goroutine泄漏；闭包）&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;2. Go Runtime + Linux 内核（RSS虚高导致OOM）https://github.com/golang/go/issues/23687&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;3. 采集指标不正常（container_memory_working_set_bytes）&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;2，3 是基于第1点能基本排除代码问题的后续步骤。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;解决和排查手段：&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;1. pprof 通过heap + goroutine 是否异常，来定位泄漏点&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;运行&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;go tool pprof&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;命令时加上--nodefration&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;0.05参数，表示如果调用的子函数使用的CPU、memory不超过 5%，就忽略它。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;2. 确认go版本和内核版本，确认是否开启了MADV_FREE，导致RSS下降不及时&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;1.12+ 和 linux内核版本大于 4.5&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;3.  RSS + Cache 内存检查&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;gt; Cache 过大的原因 https://www.cnblogs.com/zh94/p/11922714.html &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;// IO密集：手动释放或者定期重启&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;查看服务器内存使用情况： &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;free -g&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;查看进程内存情况：      &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;pidstat -rI -p 13744&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;查看进程打开的文件：    &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;lsof -p 13744&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;查看容器内的PID：      &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;docker inspect --format &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;{{ .State.Pid}}&amp;#34;&lt;/span&gt; 6e7efbb80a9d&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;查看进程树，找到目标:   &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;pstree -p 13744&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;参考：https://eddycjy.com/posts/why-container-memory-exceed/&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;通过上述步骤，我发现了该POD被&lt;code&gt;OOM killed&lt;/code&gt;还有另一个元凶就是，&lt;strong&gt;日志文件占用&lt;/strong&gt;。这里就不过多的详述了，搜索方向是 “一个运行中程序在内存中如何组织 + Cache内存是由哪些部分构成的”。这部分要达到的目标是：一个程序运行起来它为什么占用了这么些内存，而不是更多或者更少。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
