<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Load Balancing on Yeqown</title>
    <link>https://www.yeqown.xyz/tags/Load-Balancing/</link>
    <description>Recent content in Load Balancing on Yeqown</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Tue, 22 Sep 2020 13:33:20 +0800</lastBuildDate>
    <atom:link href="https://www.yeqown.xyz/tags/Load-Balancing/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Kubernetes中gRPC Load Balancing分析和解决</title>
      <link>https://www.yeqown.xyz/2020/09/22/Kubernetes%E4%B8%ADgRPC-Load-Balancing%E5%88%86%E6%9E%90%E5%92%8C%E8%A7%A3%E5%86%B3/</link>
      <pubDate>Tue, 22 Sep 2020 13:33:20 +0800</pubDate>
      <guid>https://www.yeqown.xyz/2020/09/22/Kubernetes%E4%B8%ADgRPC-Load-Balancing%E5%88%86%E6%9E%90%E5%92%8C%E8%A7%A3%E5%86%B3/</guid>
      <description>&lt;h3 id=&#34;背景&#34;&gt;&#xA;  背景&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%83%8c%e6%99%af&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;第一次，线上遇到大量接口RT超过10s触发了系统告警，运维反馈k8s集群无异常，负载无明显上升。将报警接口相关的服务重启一番后发现并无改善。但是开发人员使用链路追踪系统发现，比较慢的请求总是某个gRPC服务中的几个POD导致，由其他POD处理的请求并不会出现超时告警。&lt;/p&gt;&#xA;&lt;p&gt;第二次，同样遇到接口RT超过阈值触发告警，从k8s中查到某个gRPC服务（关键服务）重启次数异常，查看重启原因时发现是&lt;code&gt;OOM Killed&lt;/code&gt;，&lt;code&gt;OOM killed&lt;/code&gt;并不是负载不均衡直接导致的，但是也有一定的关系，这个后面再说。前两次由于监控不够完善（于我而言，运维的很多面板都没有权限，没办法排查）。期间利用pprof分析了该服务内存泄漏点，并修复上线观察。经过第二次问题并解决之后，线上超时告警恢复正常水平，但是该 deployment 下的几个POD占用资源（Mem / CPU / Network-IO），差距甚大。&lt;/p&gt;&#xA;&lt;img src=&#34;https://www.yeqown.xyz/images/k8s-grpc-lb-mem1.jpg&#34;/&gt;&#xA;&lt;img src=&#34;https://www.yeqown.xyz/images/k8s-grpc-lb-mem2.jpg&#34;/&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;第二张图是运维第一次发现该服务OOM killed 之后调整了内存上限从 512MB =&amp;gt; 1G，然而只是让它死得慢一点而已。&#xA;从上面两张图能够石锤的是该服务一定存在内存泄漏。Go项目内存占用的分析，我总结了如下的排查步骤：&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;1. 代码泄漏（pprof）（可能原因 goroutine泄漏；闭包）&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;2. Go Runtime + Linux 内核（RSS虚高导致OOM）https://github.com/golang/go/issues/23687&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;3. 采集指标不正常（container_memory_working_set_bytes）&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;2，3 是基于第1点能基本排除代码问题的后续步骤。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;解决和排查手段：&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;1. pprof 通过heap + goroutine 是否异常，来定位泄漏点&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;运行&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;go tool pprof&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;命令时加上--nodefration&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;0.05参数，表示如果调用的子函数使用的CPU、memory不超过 5%，就忽略它。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;2. 确认go版本和内核版本，确认是否开启了MADV_FREE，导致RSS下降不及时&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;1.12+ 和 linux内核版本大于 4.5&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;3.  RSS + Cache 内存检查&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;gt; Cache 过大的原因 https://www.cnblogs.com/zh94/p/11922714.html &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;// IO密集：手动释放或者定期重启&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;查看服务器内存使用情况： &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;free -g&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;查看进程内存情况：      &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;pidstat -rI -p 13744&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;查看进程打开的文件：    &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;lsof -p 13744&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;查看容器内的PID：      &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;docker inspect --format &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;{{ .State.Pid}}&amp;#34;&lt;/span&gt; 6e7efbb80a9d&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;查看进程树，找到目标:   &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;pstree -p 13744&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;参考：https://eddycjy.com/posts/why-container-memory-exceed/&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;通过上述步骤，我发现了该POD被&lt;code&gt;OOM killed&lt;/code&gt;还有另一个元凶就是，&lt;strong&gt;日志文件占用&lt;/strong&gt;。这里就不过多的详述了，搜索方向是 “一个运行中程序在内存中如何组织 + Cache内存是由哪些部分构成的”。这部分要达到的目标是：一个程序运行起来它为什么占用了这么些内存，而不是更多或者更少。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
