[{"id":0,"href":"/docs/aboutme/","title":"About Me","section":"Documentation","content":" Hi, This is yeqown # Welcome to my GitHub profile! I\u0026rsquo;m a backend software engineer from China 🇨🇳, and I\u0026rsquo;m trying to be a full-stack engineer and a DevOps engineer. I\u0026rsquo;d like to contribute to open source projects and share my knowledge with others.\nAbout me # 🎓 Education: Computer Science and Technology, SouthWest Petroleum University 🌱 Focusing on (now): k8s, service mesh, wasm, lua, LSM, [Rust[(#) 💬 Topics I like: Microservices, DevOps, Distributed Systems, Linux, Open Source, etc. We can talk these topics together. Skills # Languages: go, python, shell, rust, javascript, c, lua, etc. Frameworks and Libraries: kratos, gin, gorm, asynq etc. Network: tcp/ip, http1/2, grpc, ICMP, etc. Tools: redis, kafka, mysql, rabbitmq, openresty/nginx, k8s, etc. Architecture Build Kit: CDC, Data Sharding (Sharding Sphere), Data Warehouse (Doris). Other Skills: performance tuning, distributed systems design, CI/CD, etc. I\u0026rsquo;d using these skills to build a distributed system, and I\u0026rsquo;m trying to learn more about distributed systems design and implementation.\nOpen Source Projects # I\u0026rsquo;d like to contribute to open source projects, you can find them in my GitHub repositories. Such as:\nkratos - A Go framework for microservices. go-gorm - The fantastic ORM library for Golang, aims to be developer friendly. asynq - Asynq: simple, reliable, and efficient distributed task queue in Go. gocache - A complete Go cache library that brings you multiple ways of managing your caches. go-qrcode - A golang lib to generate QRCode. gitlab-flow - A git working flow CLI to improve development efficiency. Memcached - Memcached client package supports a connection pool for Gopher and provides a CLI easy to interactive with Memcached server. Recent Targets or Plans # I\u0026rsquo;m trying to do these things recently:\nImplement a KV storage Learning openresty/nginx skills deeply Studying lua More distributed systems design Distributed Theory Linux kernel and OS design Contact me # You can find and get touch with me on these accounts!\nGitHub: https://github.com/yeqown Email: yeqown@gmail.com Social Contact：twitter, TG If you interested in my projects, you can get in touch with me to discuss more details.\nThanks for visiting my GitHub profile and have a nice day!\n"},{"id":1,"href":"/2025/10/16/debezium-Mongodb-Connector-%E5%A2%9E%E9%87%8F%E5%BF%AB%E7%85%A7%E4%BD%BF%E7%94%A8/","title":"Debezium Mongodb Connector 增量快照使用","section":"Posts","content":" 之前的博客 Kafka Connect Mongodb 反复快照 - 大数据集快照\n本文中使用的 MongodbSourceConnector 是 io.debezium.connector.mongodb.MongoDbConnector 2.2.1.Final。\n机制简介 # 为了提供管理快照的灵活性，Debezium 包含一个补充快照机制，称为增量快照。增量快照依赖于 Debezium 机制向 Debezium 连接器发送信号。‼️ 增量快照运行时，不会阻塞变更流事件处理。\n初始快照会先保存 change stream 的位点，开始执行全量快照，全量快照完成后，再从保存的位点开始增量处理变更事件。\n目前 Debezium 支持增量快照的连接器有：\nDb2 MariaDB (Technology Preview) MongoDB MySQL Oracle PostgreSQL SQL Server 发送这个信号支持多种方式，通过配置 signal.enabled.channels 来指定，默认为 source（也就是数据集合方式），可选值有：source、kafka、file 和 jmx：\nsource 源数据库： 配置 signal.data.collection 来指定集合 kafka: 配置 signal.kafka.topic 来指定 topic file: 配置 signal.file 来指定文件路径，写入文件的格式数据为 JSON，字段取值参考下面的表格。 jmx: 启用 JMX MBean Server 来暴露 signaling bean 需要启用增量快照时，只需要向特定方式中写入数据即可。如果是 source 只需要向数据库中插入一条数据，如果是 kafka 那么则是投递一条消息。\nsource(mongodb) 集合数据格式：\n\u0026lt;signalDataCollection\u0026gt;.insert({ \u0026#34;id\u0026#34; : _\u0026lt;idNumber\u0026gt;, // 增量快照信号文档的 ID，必须唯一, 使用 UUID 等方式生成, mongodb 中作为 _id 字段 \u0026#34;type\u0026#34; : \u0026lt;snapshotType\u0026gt;, // 信号类型，固定值: execute-snapshot, stop-snapshot, pause-snapshot, resume-snapshot \u0026#34;data\u0026#34; : { // 信号数据，根据 type 不同而不同, 这里以 execute-snapshot 为例 \u0026#34;data-collections\u0026#34; : [ // 要执行增量快照的数据集合，必须是已注册的集合 \u0026#34;\u0026lt;collectionName\u0026gt;\u0026#34;, \u0026#34;\u0026lt;collectionName\u0026gt;\u0026#34; ], \u0026#34;type\u0026#34;: \u0026lt;snapshotType\u0026gt;, // 快照类型: `incremental` 和 `blocking` // 注意：2.2.1-Final 中不支持 additional-conditions，2.7.0-Alpha1 开始支持 \u0026#34;additional-conditions\u0026#34;: [ // 可选，增量快照时的筛选条件 { \u0026#34;data-collection\u0026#34; : \u0026#34;\u0026lt;collectionName\u0026gt;\u0026#34;, // 指定集合 \u0026#34;filter\u0026#34; : \u0026#34;\u0026lt;additional-condition\u0026gt;\u0026#34; // 可选，增量快照时的筛选条件，如：`age \u0026gt; 18` } ] } }); 举例如下：\n{ \u0026#34;id\u0026#34;: \u0026#34;execute-mode-snapshot-sample\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;execute-snapshot\u0026#34;, \u0026#34;data\u0026#34;: { \u0026#34;data-collections\u0026#34;: [ \u0026#34;test.users\u0026#34; ], \u0026#34;type\u0026#34;: \u0026#34;incremental\u0026#34;, \u0026#34;additional-conditions\u0026#34;: [ { \u0026#34;data-collection\u0026#34;: \u0026#34;test.users\u0026#34;, \u0026#34;filter\u0026#34;: \u0026#34;age \u0026gt; 18\u0026#34; } ] } } kafka 消息格式：\nKey = `test_connector` Value = `{\u0026#34;type\u0026#34;:\u0026#34;execute-snapshot\u0026#34;,\u0026#34;data\u0026#34;: {\u0026#34;data-collections\u0026#34;: [\u0026#34;schema1.table1\u0026#34;, \u0026#34;schema1.table2\u0026#34;], \u0026#34;type\u0026#34;: \u0026#34;INCREMENTAL\u0026#34;}}` 其实现基于 DDD-3 设计文档 ；官方详细介绍在 debezium mongodb incremental-snapshots。\n使用场景 # 增量快照提出的背景是：\nDebezium 通常通过数据库事务日志流式传输变更数据。然而，初始快照（即捕获既有数据）是全表扫描。这种方式不仅耗时长，而且一旦中断就需要从头开始。 在某些场景下，我们不需要立即获取全部历史数据。有时，我们还需要在 connector 运行期间动态添加新表，且不希望暂停数据流转。 具体举例来说：\n假设 Source Connector 已经将变更同步到 Kafka 中。但在 Sink Connector 消费之前，Kafka 中的 topic 被错误删除。这时如果想要恢复这些数据，使用初始化快照会非常笨重和麻烦。 假设集合 A 拥有超大数据量，执行完整快照需要 5 个小时。但数据库的 oplog 最多只能保存 1 个小时的变更事件。在这种情况下，如果使用初始快照，就无法实现完整的数据同步。 使用演示 # 假设已经搭建好了一整套 CDC 系统: 将 mongodb 中的 test.users 同步到 ES 中。\n1. 创建存量数据 # 为了演示增量快照的使用，我们先向 test.users 集合中插入 5 条数据：\nfor(let i = 1; i \u0026lt;= 5; i++) { db.users.insertOne({ name: \u0026#39;Pre-connector User \u0026#39; + i, email: \u0026#39;pre\u0026#39; + i + \u0026#39;@example.com\u0026#39;, created_at: new Date() }); } 2. 创建 Connector # 其中 Source Connector 配置如下：\n{ \u0026#34;name\u0026#34;: \u0026#34;mongodb-source\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;connector.class\u0026#34;: \u0026#34;io.debezium.connector.mongodb.MongoDbConnector\u0026#34;, \u0026#34;mongodb.connection.string\u0026#34;: \u0026#34;mongodb://mongodb:27017/?replicaSet=rs0\u0026#34;, \u0026#34;topic.prefix\u0026#34;: \u0026#34;mongodb-cdc\u0026#34;, \u0026#34;database.include.list\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;collection.include.list\u0026#34;: \u0026#34;test.users\u0026#34;, \u0026#34;snapshot.mode\u0026#34;: \u0026#34;never\u0026#34;, // 快照模式，never 代表不执行快照，只处理变更流（如果是大数据集合，建议使用 never，通过增量快照来处理存量数据） \u0026#34;incremental.snapshot.chunk.size\u0026#34;: \u0026#34;1024\u0026#34;, // 增量快照每次处理的文档数 \u0026#34;incremental.snapshot.allow.schema.changes\u0026#34;: \u0026#34;true\u0026#34;, // 是否允许增量快照时集合的 schema 发生变化 \u0026#34;signal.data.collection\u0026#34;: \u0026#34;test.debezium_signal\u0026#34; // 增量快照信号集合 } } # 创建 Source Connector curl -X POST http://localhost:8083/connectors -H \u0026#34;Content-Type: application/json\u0026#34; -d @connector-tasks/mongodb-source.test.users.json # 创建 Sink Connector curl -X POST http://localhost:8083/connectors -H \u0026#34;Content-Type: application/json\u0026#34; -d @connector-tasks/es-sink.mongodb.users.json 3. 检查存量数据是否被同步到 ES 中 # 我们可以通过查询 ES 中的 mongodb-cdc-test-users 索引来检查存量数据是否被同步到 ES 中：\n\u0026gt; sleep 10 \u0026amp;\u0026amp; curl -X GET \u0026#34;http://localhost:9200/_cat/indices?v\u0026#34; \u0026amp;\u0026amp; \\ echo \u0026amp;\u0026amp; curl -X GET \u0026#34;http://localhost:9200/mongodb-cdc.test.users/_count\u0026#34; % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 83 100 83 0 0 1659 0 --:--:-- --:--:-- --:--:-- 1693 health status index uuid pri rep docs.count docs.deleted store.size pri.store.size { \u0026#34;error\u0026#34;: { \u0026#34;root_cause\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;index_not_found_exception\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;no such index [mongodb-cdc.test.users]\u0026#34;, \u0026#34;resource.type\u0026#34;: \u0026#34;index_or_alias\u0026#34;, \u0026#34;resource.id\u0026#34;: \u0026#34;mongodb-cdc.test.users\u0026#34;, \u0026#34;index_uuid\u0026#34;: \u0026#34;_na_\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;mongodb-cdc.test.users\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;index_not_found_exception\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;no such index [mongodb-cdc.test.users]\u0026#34;, \u0026#34;resource.type\u0026#34;: \u0026#34;index_or_alias\u0026#34;, \u0026#34;resource.id\u0026#34;: \u0026#34;mongodb-cdc.test.users\u0026#34;, \u0026#34;index_uuid\u0026#34;: \u0026#34;_na_\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;mongodb-cdc.test.users\u0026#34; }, \u0026#34;status\u0026#34;: 404 } 可以看到，ES 中没有 mongodb-cdc-test-users 索引，说明存量数据还没有被同步到 ES 中。\n4. 插入数据验证变更流处理正常 # 为了验证变更流处理正常，我们向 test.users 集合中插入一条数据：\ndb.users.insertOne({ name: \u0026#39;CDC Test User\u0026#39;, email: \u0026#39;cdc@example.com\u0026#39;, created_at: new Date() }); 再检查 ES 中的 mongodb-cdc-test-users 索引，可以看到新增的数据：\n\u0026gt; sleep 10 \u0026amp;\u0026amp; curl -X GET \u0026#34;http://localhost:9200/mongodb-cdc.test.users/_count\u0026#34; { \u0026#34;count\u0026#34;: 1, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 } } 5. 触发增量快照-模拟大数据集合 # 现在向 test.debezium_signal 集合中插入一条文档：\ndb.debezium_signal.insertOne({ _id: \u0026#39;never-mode-snapshot\u0026#39;, // 增量快照信号文档的 ID，必须唯一, 使用 UUID 等方式生成 type: \u0026#39;execute-snapshot\u0026#39;, data: { \u0026#39;data-collections\u0026#39;: [\u0026#39;test.users\u0026#39;], \u0026#39;type\u0026#39;: \u0026#39;incremental\u0026#39; } }); 再检查 ES 中的 mongodb-cdc-test-users 索引，可以看到增量快照处理完成后，数据总数为 6：\n\u0026gt; sleep 10 \u0026amp;\u0026amp; curl -X GET \u0026#34;http://localhost:9200/mongodb-cdc.test.users/_count\u0026#34; { \u0026#34;count\u0026#34;: 6, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 } } 场景 Mongodb 中数据 ES 中数据 kafka 中消息 触发前 6 条 1 条 1 条 触发后 6 条 6 条 7 条 6. 触发增量快照-重新快照 # 这时候再触发一次增量快照，模拟重新触发增量快照的场景：\ndb.debezium_signal.insertOne({ _id: \u0026#39;never-mode-snapshot-again\u0026#39;, // 增量快照信号文档的 ID，必须唯一, 使用 UUID 等方式生成 type: \u0026#39;execute-snapshot\u0026#39;, data: { \u0026#39;data-collections\u0026#39;: [\u0026#39;test.users\u0026#39;], \u0026#39;type\u0026#39;: \u0026#39;incremental\u0026#39; } }); 这时候再检查 ES 中的 mongodb-cdc-test-users 索引，可以看到增量快照处理完成后，数据总数为 6（ES Sink 采用了 upsert 模式）：\n\u0026gt; sleep 10 \u0026amp;\u0026amp; curl -X GET \u0026#34;http://localhost:9200/mongodb-cdc.test.users/_count\u0026#34; { \u0026#34;count\u0026#34;: 6, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 } } 场景 Mongodb 中数据 ES 中数据 kafka 中消息 触发前 6 条 6 条 7 条 触发后 6 条 6 条 13 条 可以看到，kafka 中消息增加了 6 条，说明全部的 6 条数据又被快照了一次。\n实现原理 # Debezium 的实现方案参考了：DBLog: A Watermark Based Change-Data-Capture Framework 这篇论文。\nWe wanted to (a) trigger the full state capture at any point in time. That is because the full state may not only be needed initially and may be needed at any time afterwards. For instance if the database is restored from a backup or for repairs if there is data loss or corruption downstream. There are also cases where only a subset of data needs to be repaired, for example if a specific set of rows has been identified to be corrupt downstream. (b) pause or resume at any time so that full state capture does not need to start from the beginning for large tables after restarting the process. (c) capture transaction log events and the full state side by side without stalling one or the other. There are use cases that require high availability of transaction log events so that the replication lag to the source is kept to a minimum. (d) prevent time-travel, by preserving the order of history when transmitting events to a derived datastore. This way an earlier version of a row (like the residential address of a member account) is not delivered after a later version. Hence, a solution had to combine transaction log events and the full state in a way that preserves the history of changes. (e) offer this as a platform. Hence it was crucial to minimize the impact on the source database. Otherwise this can hinter adoption of the platform, especially for use cases that have high traffic. In that regard we want to avoid primitives such as table locks which can block application write traffic. (f) function across a variety of Relational Database Management Systems (RDMBS), such as MySQL, PostgreSQL, Aurora [ 19 ] etc, that we use in production. In order to achieve that we wanted to avoid using vendor specific features\n可以看到 DBLog 想要解决以下的问题：\na) 随时触发全量快照，更灵活\nb) 随时暂停或恢复，让大表不需要重头开始快照\nc) 快照和实时变更事件可以并行处理，互不干扰\nd) 防止时间旅行（变更事件的时序不能错乱），按变更顺序处理事件\ne) 提供平台化的解决方案，最小化对源数据库的影响\nf) 支持跨多种 RDBMS，如 MySQL, PostgreSQL, Aurora 等\n1. DBLog - 顶层架构 # DBLog 顶层架构 DBLog 将从 Source 按照 chunk 大小分块依次读取全表，并同时处理 change log，将处理好的数据写入到 Output 中去，于此同时通过 State 组件跟踪记录。\n它采用了主从架构来提供高可用支持, 这不是我们关注的重点，就不展开了。\n2. DBLog - 实现细节 # 想要实现这样的解决方案，绕不开两方面的数据捕获，分别是：\na) 增量变更事件\n增量变更事件代表了数据库中发生的每次变更操作，包括插入、更新和删除，这些事件会被实时捕获并处理。\nb) 全量快照\n全量快照是指对数据库中的所有数据进行一次完整的备份或复制。\n但是很明显，增量和全量不能简单的并行处理。按照其他方案的处理方式，全量快照要么会暂停增量捕获（如 Maxwell 和 Debezium），要么需要创建一个副本（MySQLStreamer），还有些会加上表锁（如 Debezium, 锁定的取决于数据库和实现）。这些方法这对于大型数据库来说是不可接受的。\n为了解决这一问题，DBLog 提出的解决方案是：将增量和全量快照合并处理。具体来说：\n其算法描述如下图：\nDBLog 算法描述 举例演示如下图：\nDBLog 举例演示 总结来说，DBLog 提出的解决方案是：\n全量快照采用 分块 的方式从数据集合中 读出（这要求表/集合的 PK 是有序的且稳定的）。 向变更流中插入 Low Watermark（LW）和 High Watermark（HW），分别表示全量快照的开始和结束位置。 将全量 Chunk 和 增量变更事件合并，生成全新的事件流。 通过分块读，可以实现对大部分数据库的兼容，不需要依赖于数据库的特定功能。chunk 跟 增量事件合并实现了宏观上的并行处理，保证了增量事件的实时性。水印则可以用来控制 chunk 的大小 和 控制快照的起停。\n3. Debezium 实现源码 # 知道了整个增量快照的实现思路后，现在来看看 Debezium 是如何实现的。io.debezium.pipeline.signal.AbstractSnapshotSignal 抽象了增量快照的几个信号：\npublic abstract class AbstractSnapshotSignal\u0026lt;P extends Partition\u0026gt; implements Signal.Action\u0026lt;P\u0026gt; { private static final Logger LOGGER = LoggerFactory.getLogger(AbstractSnapshotSignal.class); protected static final String FIELD_DATA_COLLECTIONS = \u0026#34;data-collections\u0026#34;; protected static final String FIELD_TYPE = \u0026#34;type\u0026#34;; protected static final String FIELD_ADDITIONAL_CONDITION = \u0026#34;additional-condition\u0026#34;; protected static final String FIELD_SURROGATE_KEY = \u0026#34;surrogate-key\u0026#34;; // 忽略其他代码... } 信号创建后，会触发对应信号的 arrived 方法，这里以 io.debezium.pipeline.signal.ExecuteSnapshot 为例：\npublic boolean arrived(Payload\u0026lt;P\u0026gt; signalPayload) throws InterruptedException { final List\u0026lt;String\u0026gt; dataCollections = getDataCollections(signalPayload.data); if (dataCollections == null) { return false; } SnapshotType type = getSnapshotType(signalPayload.data); Optional\u0026lt;String\u0026gt; additionalCondition = getAdditionalCondition(signalPayload.data); Optional\u0026lt;String\u0026gt; surrogateKey = getSurrogateKey(signalPayload.data); LOGGER.info(\u0026#34;Requested \u0026#39;{}\u0026#39; snapshot of data collections \u0026#39;{}\u0026#39; with additional condition \u0026#39;{}\u0026#39; and surrogate key \u0026#39;{}\u0026#39;\u0026#34;, type, dataCollections, additionalCondition.orElse(\u0026#34;No condition passed\u0026#34;), surrogateKey.orElse(\u0026#34;PK of table will be used\u0026#34;)); switch (type) { case INCREMENTAL: dispatcher.getIncrementalSnapshotChangeEventSource().addDataCollectionNamesToSnapshot( signalPayload.partition, dataCollections, additionalCondition, surrogateKey, signalPayload.offsetContext); break; } return true; } 可以看到，如果是增量快照，会调用 IncrementalSnapshotChangeEventSource 的 MongoDbIncrementalSnapshotChangeEventSource#addDataCollectionNamesToSnapshot 方法：\n此方法会触发 readChunk 方法; readChunk 方法会先触发 emitWindowOpen 方法，再执行实际的分块读取和处理逻辑 createDataEventsForDataCollection; readChunk 方法会触发 emitWindowClose 方法; emitWindowClose 实际上会发出 snapshot-window-close 信号，被 io.debezium.pipeline.source.snapshot.incremental.CloseIncrementalSnapshotWindow 处理; public void addDataCollectionNamesToSnapshot(MongoDbPartition partition, List\u0026lt;String\u0026gt; dataCollectionIds, Optional\u0026lt;String\u0026gt; additionalCondition, Optional\u0026lt;String\u0026gt; surrogateKey, OffsetContext offsetContext) throws InterruptedException { // 忽略其他代码... final boolean shouldReadChunk = !context.snapshotRunning(); if (shouldReadChunk) { readChunk(partition); } } protected void readChunk(MongoDbPartition partition) throws InterruptedException { try { // 触发窗口打开事件 emitWindowOpen(); while (context.snapshotRunning()) { // 读取 chunk 数据到 window 中 createDataEventsForDataCollection(partition); // 如果 window 为空，说明当前数据集合已经读取完毕 if (window.isEmpty()) { LOGGER.info(\u0026#34;No data returned by the query, incremental snapshotting of table \u0026#39;{}\u0026#39; finished\u0026#34;, currentDataCollectionId); collectionScanCompleted(partition); nextDataCollection(partition); } else { break; } } // 触发窗口关闭事件 emitWindowClose(); } // 省略其他代码... } private void createDataEventsForDataCollection(MongoDbPartition partition) throws InterruptedException { mongo.execute(\u0026#34;chunk query key for \u0026#39;\u0026#34; + currentCollection.id() + \u0026#34;\u0026#39;\u0026#34;, client -\u0026gt; { // 构造查询条件：_id \u0026lt;= maxKey (快照启动时，整个文档的最大主键值) final Document maxKeyPredicate = new Document(); final Document maxKeyOp = new Document(); maxKeyOp.put(\u0026#34;$lte\u0026#34;, context.maximumKey().get()[0]); maxKeyPredicate.put(DOCUMENT_ID, maxKeyOp); Document predicate = maxKeyPredicate; // 如果上一次读取的最大主键值不为空，那么构造查询条件：_id \u0026gt; chunkEndPosition (上一次读取的最大主键值) if (context.chunkEndPosititon() != null) { final Document chunkEndPredicate = new Document(); final Document chunkEndOp = new Document(); chunkEndOp.put(\u0026#34;$gt\u0026#34;, context.chunkEndPosititon()[0]); chunkEndPredicate.put(DOCUMENT_ID, chunkEndOp); predicate = new Document(); predicate.put(\u0026#34;$and\u0026#34;, Arrays.asList(chunkEndPredicate, maxKeyPredicate)); } // 按照 PK 主键升序查询，限制返回的记录数为 incremental.snapshot.chunk.size for (BsonDocument doc : collection.find(predicate).sort(new Document(DOCUMENT_ID, 1)) .limit(connectorConfig.getIncrementalSnashotChunkSize())) { // 放入 window 中， keyStruct 为主键 window.put(keyStruct, row); } // 设置下一次 read chunk 的下限为这一次读取的最大主键值 context.nextChunkPosition(lastKey); }); } 而 CloseIncrementalSnapshotWindow 实际上会回调 IncrementalSnapshotChangeEventSource#closeWindow 方法\n// class CloseIncrementalSnapshotWindow public boolean arrived(Payload\u0026lt;P\u0026gt; signalPayload) throws InterruptedException { dispatcher.getIncrementalSnapshotChangeEventSource().closeWindow(signalPayload.partition, signalPayload.id, signalPayload.offsetContext); return true; } 因此逻辑又回到了 MongoDbIncrementalSnapshotChangeEventSource 中：\npublic void closeWindow(MongoDbPartition partition, String id, OffsetContext offsetContext) throws InterruptedException { // 将 window 中的数据发送 sendWindowEvents(partition, offsetContext); // 触发下一个分块读取 readChunk(partition); } 另外在 Debezium 中, ChangeEventSourceCoordinator 会协调 SnapshotChangeEventSource 和 StreamingChangeEventSource 的执行，这两者产生的事件会进入到 EventDispatcher 中的共享队列中。\n// ChangeEventSourceCoordinator#executeChangeEventSources protected void executeChangeEventSources(CdcSourceTaskContext taskContext, SnapshotChangeEventSource\u0026lt;P, O\u0026gt; snapshotSource, Offsets\u0026lt;P, O\u0026gt; previousOffsets, AtomicReference\u0026lt;LoggingContext.PreviousContext\u0026gt; previousLogContext, ChangeEventSourceContext context) throws InterruptedException { final P partition = previousOffsets.getTheOnlyPartition(); final O previousOffset = previousOffsets.getTheOnlyOffset(); previousLogContext.set(taskContext.configureLoggingContext(\u0026#34;snapshot\u0026#34;, partition)); SnapshotResult\u0026lt;O\u0026gt; snapshotResult = doSnapshot(snapshotSource, context, partition, previousOffset); if (running \u0026amp;\u0026amp; snapshotResult.isCompletedOrSkipped()) { previousLogContext.set(taskContext.configureLoggingContext(\u0026#34;streaming\u0026#34;, partition)); streamEvents(context, partition, snapshotResult.getOffset()); } } 在梳理过程中，我没有发现 Debezium 中有 DBLog 中提到的 Low Watermark 和 High Watermark 相关的逻辑，而是在 EventDispatcher 中，当 StreamingChangeEvent 提交时 (dispatchDataChangeEvent 方法)，通知增量快照对窗口中的事件进行去重。\n这里我个人没有梳理清楚，增量快照 和 实时流变更 的事件顺序是怎么样的，看起来是和 DBLog 中的实现不完全一致。\nDebezium 这一块的架构梳理如下：\nDebezium Source Connector Architecture 总结 # 增量快照相比初始快照，更加灵活可控；对于大型数据库或者集合的快照过程更友好，不会影响实时流变更的处理。\n"},{"id":2,"href":"/2025/10/14/Apache-Doris-PreparedStatement%E9%97%AE%E9%A2%98%E6%8E%A2%E7%A9%B6/","title":"Apache Doris PreparedStatement 之谜","section":"Posts","content":" 使用的 Apache Doris 版本：3.0.3, 对应的 Doris 源码如果没有特意说明，均为 3.0.3 版本。\n背景 # 运营反馈了一些业务异常，这些异常都指向了一个 Doris 数据查询服务（后都用 DQ-1 指代），该服务日志中存在大量的错误日志，如下：\nplatformAwardBiz.GetByID failed: Error 1047 (08S01): msg: Not supported such prepared statement 日志中发现，全部的查询均报了 Not supported such prepared statement 错误，而不是存在特定的查询语句。而与此同时，另外的 Doris 查询服务 (后使用 DQ-2 指代) 并没有出现该错误日志。\nDQ-1 使用 golang 编写，使用了 gorm 作为连接 Doris 的查询工具。DQ-2 使用了 python3编写，使用了 pymysql 作为连接 Doris 的查询工具。\n发现问题后，猜测是 prepared statement 机制的相关问题，快速解决方案选择了重启 DQ-1 和 Doris FE 节点，然后错误消失。\n问题分析 # 首先从日志入手，采集了 FE 的错误日志，但经过排查没有发现明显的异常日志。经过检索搜索引擎和社区，也没有发现有相关的 issue。\n那只能从源码入手了。在进去源码分析之前，这里带着几个疑问：\n为什么 DQ-1 会报这个错误，而 DQ-2 不会报这个错误？ 为什么重启 DQ-1 和 Doris FE 节点可以解决问题？ 是 Doris FE 节点的问题，还是 DQ-1 的问题？ DQ-1 的嫌疑非常小，因为项目内有相同的代码连接另外的 MySQL 数据库，并没有出现类似的问题。\nApache Doris 源码分析1 # 尝试在客户端检索 Not supported such prepared statement 关键词，发现没有任何结果，说明这个错误信息是 Doris FE 节点报出的。\n在源码中搜索 Not supported such prepared statement，发现该错误信息定义在 fe/fe-core/src/main/java/org/apache/doris/qe/MysqlConnectProcessor.java 文件中：\n// process COM_EXECUTE, parse binary row data // https://dev.mysql.com/doc/dev/mysql-server/latest/page_protocol_com_stmt_execute.html private void handleExecute() { packetBuf = packetBuf.order(ByteOrder.LITTLE_ENDIAN); // parse stmt_id, flags, params int stmtId = packetBuf.getInt(); // flag packetBuf.get(); // iteration_count always 1, packetBuf.getInt(); // nererids PreparedStatementContext preparedStatementContext = ctx.getPreparedStementContext(String.valueOf(stmtId)); if (preparedStatementContext == null) { if (LOG.isDebugEnabled()) { LOG.debug(\u0026#34;No such statement in context, stmtId:{}\u0026#34;, stmtId); } ctx.getState().setError(ErrorCode.ERR_UNKNOWN_COM_ERROR, \u0026#34;msg: Not supported such prepared statement\u0026#34;); return; } handleExecute(preparedStatementContext.command, stmtId, preparedStatementContext); } 从这一段代码，就可以得到以下几个结论：\nDoris 支持 MySQL 协议的 PreparedStatement 机制 这个错误是在COM_STMT_EXECUTE 阶段报出的 该错误是因为 PreparedStatementContext 对象为 null 导致的 这个 stmtId 是从客户端传递过来的 简单总结一下，这个错误的根本原因是 Doris FE 节点没有找到 stmtId 对应的 PreparedStatementContext 对象导致的。那为什么客户端会传递一个 FE 节点没有的 stmtId 呢？\n再进一步深入代码前，发现自己对 MySQL 的 PreparedStatement 机制并不是很了解，于是先去补充了一下相关知识。\nMySQL Prepared Statement 协议 # MySQL 协议 - Prepared Statments\nPrepared Statments 协议是从 MySQL 4.1 版本开始支持的，主要是为了提高 SQL 语句的执行效率和安全性。协议约定了以下几个命令：\nCommand Description COM_STMT_PREPARE 准备一个 SQL 语句，返回一个 stmt_id COM_STMT_EXECUTE 执行一个已经准备好的 SQL 语句，使用 stmt_id COM_STMT_CLOSE 关闭一个已经准备好的 SQL 语句，释放资源 COM_STMT_RESET 重置 COM_STMT_SEND_LONG_DATA 命令发送的参数, 并关闭 COM_STMT_EXECUTE 打开的游标 COM_STMT_SEND_LONG_DATA 发送长数据参数给一个已经准备好的 SQL 语句 COM_STMT_FETCH 获取一个已经准备好的 SQL 语句的结果集 PreparedStatement 的工作流程如下：\nPreparedStatement 流程图 从 Doris 源码中，已经得知是在 COM_STMT_EXECUTE 阶段报出的错误，而 COM_STMT_EXECUTE 传递的 stmt_id来自于 COM_STMT_PREPARE 阶段。那么这里再重点关注下这两个命令的协议细节。\nCOM_STMT_PREPARE # COM_STMT_PREPARE 命令的请求包格式如下：\nType Name Description int\u0026lt;1\u0026gt; status [0x16] COM_STMT_PREPARE string query The query to prepare 响应包格式如下：\nType Name Description int\u0026lt;1\u0026gt; status [0x00] OK int\u0026lt;4\u0026gt; statement_id The statement identifier int\u0026lt;2\u0026gt; num_columns The number of columns in the result set int\u0026lt;2\u0026gt; num_params The number of parameters in the statement int\u0026lt;1\u0026gt; reserved_1 [0x00] Filler int\u0026lt;2\u0026gt; warning_count The number of warnings 省略其余字段 COM_STMT_EXECUTE # COM_STMT_EXECUTE 命令的请求包格式如下：\nType Name Description int\u0026lt;1\u0026gt; status [0x17] COM_STMT_EXECUTE int\u0026lt;4\u0026gt; statement_id ID of the prepared statement to execute int\u0026lt;1\u0026gt; flags Flags. See enum_cursor_type int\u0026lt;4\u0026gt; iteration_count Number of times to execute the statement. Currently always 1. 省略其余字段 响应包格式于 COM_QUERY 类似，返回 OK 包、错误包和结果集包之一，取决于执行结果。与这里的问题无关，就不赘述了。\n到这里，已经对 PreparedStatement 协议有了一个基本的了解，也得到了以下信息：\nstmt_id 是由服务端在 COM_STMT_PREPARE 阶段生成的，并返回给客户端。 客户端在 COM_STMT_EXECUTE 阶段会携带 stmt_id 来执行对应的 SQL 语句。 stmt_id 是一个 int\u0026lt;4\u0026gt;(四字节) 的整数。 那么这个问题的指向更清晰了，还是要回到 Doris 源码中，继续深入分析 stmt_id 的生成和管理。\nApache Doris 源码分析2 # 继续深入 Doris 源码，重点关注 statement_id 的生成和管理。\n从 PrepareCommand.java 文件中，发现了返回给客户端的 statement_id 是通过 ConnectionContext 对象的 getStmtId() 方法获取：\n// register prepared statement with attached statement id @Override public void run(ConnectContext ctx, StmtExecutor executor) throws Exception { List\u0026lt;String\u0026gt; labels = getLabels(); // register prepareStmt // ... if (logicalPlan instanceof InsertIntoTableCommand \u0026amp;\u0026amp; ((InsertIntoTableCommand) logicalPlan).getLabelName().isPresent()) { throw new org.apache.doris.common.UserException(\u0026#34;Only support prepare InsertStmt without label now\u0026#34;); } ctx.addPreparedStatementContext(name, new PreparedStatementContext(this, ctx, ctx.getStatementContext(), name)); if (ctx.getCommand() == MysqlCommand.COM_STMT_PREPARE) { // !!!! 关键线索 !!!! executor.sendStmtPrepareOK((int) ctx.getStmtId(), labels); } } executor.sendStmtPrepareOK((int) ctx.getStmtId(), labels); 这一行代码中有一个关键的线索就是 ctx.getStmtId() 返回的值进行了类型转换，转换成了 int 类型。这不禁让人联想到一个可能性：\nstmtId 超过了 int 的范围，但是由于这里的类型转换，导致了客户端接收到的 stmtId 和服务端实际存储的不一致，进而导致在 COM_STMT_EXECUTE 阶段，Doris FE 节点找不到对应的 stmtId。\n再检查下 ConnectionContext 类中的 stmtID 属性：\n// ConnectionContext protected volatile long stmtId; public long getStmtId() { return stmtId; } // 由 StmtExecutor 调用 public void setStmtId(long stmtId) { this.stmtId = stmtId; } 继续追踪 stmtId 的生成，发现是在 StmtExecutor 类中：\n// StmtExecutor private static final AtomicLong STMT_ID_GENERATOR = new AtomicLong(0); // 初始值为 0 private void executeByNereids(TUniqueId queryId) throws Exception { // ... // stmtId 原子自增 context.setStmtId(STMT_ID_GENERATOR.incrementAndGet()); // ... } 至此，答案也已经呼之欲出了： stmtId 在内存中类型是 long (八字节)，而返回给客户端的时候进行了强制类型转换成了 int (四字节)。stmtId 一直自增，当 stmtId 超过了四字节的容量时，就会出现客户端接收到的 stmtId 和服务端实际存储的不一致，进而导致此问题。\n解决 # 找到问题后，再去 Doris 仓库切换到最新的代码时，发现这部分逻辑已经被修改了，相关的 PR 在：https://github.com/apache/doris/pull/47950\n那就更简单了，直接升级 Doris 版本到最新即可。\n新版本中的逻辑是：ConnectionContext 新增了一个 preparedStmtId 属性，类型是 int：\n// range [Integer.MIN_VALUE, Integer.MAX_VALUE] protected int preparedStmtId = Integer.MIN_VALUE; public void incPreparedStmtId() { ++preparedStmtId; } Wireshark 抓包 # 在测试环境中升级 Doris 版本后，使用 Wireshark 抓包，验证一下 stmtId 是否如新版本中的逻辑工作。\n在以前的版本 stmtId 从 0 开始自增，而在新版本中 preparedStmtId 从 Integer.MIN_VALUE 开始自增。因此只需要关注前面几个 COM_STMT_PREPARE 包的响应，其中的 statement_id 是从 0 开始，还是 2147483648 (即 Integer.MIN_VALUE 无符号数) 开始。\nWireshark 抓包 - COM_STMT_PREPARE 很明显，statement_id 是从 2147483648 开始的，说明新版本中的逻辑已经生效。\n答疑 # 为什么 DQ-1 会报这个错误，而 DQ-2 不会报这个错误？\nDQ-2 使用的客户端拼接 SQL 语句，并没有使用到 PreparedStatement 机制。而 DQ-1 (golang) 底层 mysql 驱动 (github.com/go-sql-driver/mysql) 会优先尝试客户端 prepare (使用 query 而不是服务器 prepare), 如果没有开启则会使用服务器 prepare, 使用 interpolateParams 控制开启\n也就是说如果想要避免这个问题，可以在 DQ-1 的数据库连接字符串中添加 interpolateParams=true 参数，避免使用服务器 prepare 规避 statement_id 增长溢出。\n另外 gorm 也由一个 PrepareStmt 的配置选项，会缓存已经 prepared 的 sql.Stmt, 才会合理利用上服务端 prepare 机制，同时减少客户端的 prepare 次数。\n为什么重启 DQ-1 和 Doris FE 节点可以解决问题？\n实际上是重启 Doris FE 节点解决了问题，重启后 stmtId 重新从 0 开始自增，暂时避免了溢出。\n是 Doris FE 节点的问题，还是 DQ-1 的问题？\n是 Doris FE 中 PreparedStatement 实现存在缺陷。\n总结 # 本文复盘了在使用 Apache Doris PreparedStatement 机制时出现 “Not supported such prepared statement” 错误的排查历程。通过源码追踪，发现 Doris FE 节点在管理 stmtId 时存在类型转换问题，导致溢出后客户端与服务端的 stmtId 不一致，最终引发异常。\n升级 Doris 至最新版本或在客户端配置参数避免服务端 prepare，均可有效规避该问题。\n在排查过程，系统性的了解 MySQL 的 PreparedStatement 协议以及服务端和客户端在 PreparedStatement 机制中的实现细节。\n总的来说，知其然更要知其所以然，理解底层原理才能更好地解决问题。\n"},{"id":3,"href":"/2025/09/28/Apache-Kafka-MirrorMaker2%E4%BB%8E%E4%BD%BF%E7%94%A8%E5%88%B0%E8%BF%81%E7%A7%BB/","title":"Kafka MirrorMaker2 从使用到迁移","section":"Posts","content":" 本文中使用的 Kafka 版本为 v3.3.2\n引言 # Kafka MirrorMaker2 是 Kafka 官方提供的跨集群数据复制工具, 它是基于 Kafka Connect 框架构建的。MirrorMaker2 支持多种部署模式, 包括 Dedicated 模式和 Connect 集群模式，还有 standalone 模式。\n其中, Dedicated 模式有一个启动脚本 kafka-mirror-maker.sh, 该脚本会启动一个独立的 MirrorMaker2 实例, 而不需要依赖 Kafka Connect 集群。Dedicated 模式适合小规模的复制任务, 但在大规模部署中, 它缺乏可扩展性和高可用性。\n相比之下, Connect 集群模式则是先搭建出一个 Kafka Connect 集群, 再提交 MirrorMaker2 的 MirrorSourceConnector 任务。这种模式下, 可以通过增加或减少 Connect 工作节点来动态调整复制任务的资源, 具备更好的弹性和容错能力。\n当然配置上也会更复杂一些, 需要管理 Connect 集群的配置和任务。\n那么, 如果我们已经在使用 Dedicated 模式部署了 MirrorMaker2, 但现在需要切换到 Connect 集群模式, 应该如何操作呢? 本文将介绍从 Dedicated 模式迁移到 Connect 集群模式时，怎么处理已经同步的 offset 进度, 以确保数据的一致性和连续性。\nKafka Connect 的设计 # Kafka Connect 是 Apache Kafka 生态系统中的框架和工具集，旨在在 Kafka 和其他数据系统（例如数据库、云服务和文件系统）之间可靠且可扩展地传输数据。主要特性包括分布式架构、基于配置的操作以及对数据转换和各种序列化格式的支持，使数据集成更简单、更解耦。\nKafka Connect 架构 在整个框架中有以下几个核心概念：\nWorkers: 负责实际执行任务执行, 每个 Worker 都有一个线程池, 用于执行多个任务。\nConnectors: 连接其他系统和 kafka 的组件，也是整个 Kafka Connect 框架中最重要的扩展点。\nSource Connector: 从其他系统（MySQL, Mongo）导入数据到 Kafka。 Sink Connector: 将 Kafka 中的数据导出到其他系统。 Connector 自身不执行数据复制，而是负责将整个复制任务拆分成一组可以执行的 Task 交给 Kafka Connect Worker 执行。因此要实现一个 Connector 也需要确定要使用的 Task 类）。\nTasks: 则负责实际执行任务执行。\nSource Task: 从源系统读取数据, 并将其写入 Kafka。 Sink Task: 从 Kafka 读取数据, 并将其写入目标系统。 除了上述核心组件外, Kafka Connect 还包括以下几个重要概念:\nConverters: Kafka Connect 要和外部系统交互，不可避免的需要对数据进行序列化和反序列化。Converters 负责将数据从一种格式转换为另一种格式。 Transformers: 用于在数据复制过程中对数据进行转换和处理, 比如字段重命名，增加/删除字段等等，当然也可以进行自定义。 Kafka Connect 模型 Kafka Connect 会在 Connector 实例运行时, 跟踪其偏移量，以便连接器在发生故障或者维护时，可以从先前的位置恢复。参考 Kafka Connect 文档。\n在对 Kafka Connect 框架有了一定的了解后，我们就可以再深入去理解 MirrorMaker 的设计原理了，一言以蔽之：一个把数据从源 Kafka 复制到目标 Kafka 的 MirrorSourceConnector。\n思考题：为什么是实现 SourceConnector 而不是实现 SinkConnector？\nMirrorSourceConnector 的设计 # 这部分不会关注两种部署模式的区别，重点关注 MirrorMaker2 的核心组件 MirrorSourceConnector。至于 MirrorCheckpointConnector 是用来同步 groups，一个是用来探测与 kafka 集群的联通性，按需使用即可。\nMirrorMaker2 的核心组件是 MirrorSourceConnector, 它负责从源集群读取数据并写入目标集群。MirrorSourceConnector 实现了 Kafka Connect 的 SourceConnector 类, 由此可见它的调度和任务管理都依赖于 Kafka Connect 框架。\nMirrorSourceConnector 的主要作用就是把源集群的 topic 数据复制到目标集群。它的基本工作流程可以分为以下几个阶段:\n初始化 Task定义和任务分配 动态调整/维护 1. 初始化 # 这部分逻辑集中在 MirrorSourceConnector 的 start 方法中, 该方法会读取配置参数, 并初始化一些内部状态, 包括:\n配置解析： 通过 MirrorConnectorConfig 解析配置参数（如源/目标集群别名、复制策略、过滤规则等）。若连接器未启用（config.enabled=false），则直接返回。\n资源初始化： 创建源/目标集群的 AdminClient（用于集群管理操作）、Scheduler（用于定时任务），并初始化 replicationPolicy（ topic 命名转换策略）、topicFilter（ topic 过滤规则）等核心组件。\n初始任务调度：\n通过 Scheduler 执行一次性初始化任务：\n创建 offset-syncs 主题（用于同步消费者偏移量）。 加载源/目标集群的初始 topic-partition 元数据。 在目标集群创建缺失的 topic 或分区。 2. Task定义和任务分配 # Task 是实际执行数据复制的工作单元，这也是 Kafka Connect 的概念。MirrorSourceConnector 对应的 Task 类是 MirrorSourceTask。 参见下代码片段：\n@Override public Class\u0026lt;? extends Task\u0026gt; taskClass() { return MirrorSourceTask.class; } // divide topic-partitions among tasks // since each mirrored topic has different traffic and number of partitions, to balance the load // across all mirrormaker instances (workers), \u0026#39;roundrobin\u0026#39; helps to evenly assign all // topic-partition to the tasks, then the tasks are further distributed to workers. // For example, 3 tasks to mirror 3 topics with 8, 2 and 2 partitions respectively. // \u0026#39;t1\u0026#39; denotes \u0026#39;task 1\u0026#39;, \u0026#39;t0p5\u0026#39; denotes \u0026#39;topic 0, partition 5\u0026#39; // t1 -\u0026gt; [t0p0, t0p3, t0p6, t1p1] // t2 -\u0026gt; [t0p1, t0p4, t0p7, t2p0] // t3 -\u0026gt; [t0p2, t0p5, t1p0, t2p1] @Override public List\u0026lt;Map\u0026lt;String, String\u0026gt;\u0026gt; taskConfigs(int maxTasks) { if (!config.enabled() || knownSourceTopicPartitions.isEmpty()) { return Collections.emptyList(); } int numTasks = Math.min(maxTasks, knownSourceTopicPartitions.size()); List\u0026lt;List\u0026lt;TopicPartition\u0026gt;\u0026gt; roundRobinByTask = new ArrayList\u0026lt;\u0026gt;(numTasks); for (int i = 0; i \u0026lt; numTasks; i++) { roundRobinByTask.add(new ArrayList\u0026lt;\u0026gt;()); } int count = 0; for (TopicPartition partition : knownSourceTopicPartitions) { int index = count % numTasks; roundRobinByTask.get(index).add(partition); count++; } return roundRobinByTask.stream().map(config::taskConfigForTopicPartitions) .collect(Collectors.toList()); } 从上代码可以看到, MirrorSourceConnector 会根据配置的 maxTasks 参数, 将所有需要复制的 topic-partition 划分成多个子集, 每个子集对应一个 Task 的配置。划分策略采用 round-robin 方式, 以尽量均衡每个 Task 的负载。\n使用时，需要注意 task 数量的设置, 合理的根据任务来设置 task 数量，避免过多或过少。\n3. 持续维护 # MirrorSourceConnector Scheduler 会定期执行维护任务, 以确保复制过程的一致性。主要包括:\n同步 topic 的 ACL 和 配置。 刷新 topic 的分区信息，若发现源集群新增分区或目标集群缺失分区，会触发 computeAndCreateTopicPartitions，在目标集群创建 topic 或扩容分区。 MirrorSourceTask 的设计 # MirrorSourceTask 实现了 SourceTask, 它负责实际的数据复制工作。其主要工作流程可以分为以下几个阶段:\n初始化 数据拉取和写入 偏移量管理（这里特指 topic 同步的 offset 进度） 1. 初始化 # 配置解析： 加载连接器任务配置（如源集群别名、偏移量同步主题、消费者超时时间等）。\n资源初始化： 创建 Kafka 消费者（拉取源集群数据）、Kafka 生产者（发送偏移量同步信息）、信号量（控制消费者并发访问）等核心资源。\n分区与偏移量准备：\n从 \u0026lt;offset.storage.topic\u0026gt; 主题加载历史偏移量（通过 loadOffsets 方法）。 将消费者分配到指定分区，并定位到上次复制的偏移量位置（consumer.seek）。 @Override public void start(Map\u0026lt;String, String\u0026gt; props) { MirrorTaskConfig config = new MirrorTaskConfig(props); // 初始化消费者、生产者 等资源 consumer = MirrorUtils.newConsumer(config.sourceConsumerConfig()); offsetProducer = MirrorUtils.newProducer(config.offsetSyncsTopicProducerConfig()); // 加载历史偏移量并定位消费者 Map\u0026lt;TopicPartition, Long\u0026gt; topicPartitionOffsets = loadOffsets(config.taskTopicPartitions()); consumer.assign(topicPartitionOffsets.keySet()); topicPartitionOffsets.forEach(consumer::seek); } 其中 loadOffsets 的实现在 Dedicated 模式和 Connect 模式下是不同的:\n在 Dedicated 模式下, 偏移量存储在 mm2-offsets.\u0026lt;source-cluster\u0026gt; 主题中, 需要从该主题读取偏移量。 在 Connect 模式下, 偏移量存储在 \u0026lt;offset.storage.topic\u0026gt; 主题中, 需要从该主题读取偏移量。 但无论哪种模式, 读取偏移量的逻辑都是类似的：\n依赖于 OffsetStorageReader 接口, 同时在 OffsetStorageReader 的具体实现 OffsetStorageReaderImpl 中又依赖于 OffsetBackingStore 来具体决定偏移量的存储介质（如 Kafka 主题 、文件 等，内存等）。\nMirrorMaker2 使用的是 KafkaOffsetBackingStore 来存储偏移量, 顾名思义, 它是基于 Kafka 主题的存储方式, 也就是 mm2-offsets.\u0026lt;source-cluster\u0026gt;.internal 或 \u0026lt;offset.storage.topic\u0026gt; 主题。\noffset 主题中的消息格式如下，Key 和 Value 都是序列化后的字节数组：\n[\u0026#34;\u0026lt;connector-name\u0026gt;\u0026#34;,{\u0026#34;cluster\u0026#34;:\u0026#34;\u0026lt;source-cluster-alias\u0026gt;\u0026#34;,\u0026#34;partition\u0026#34;:\u0026lt;partition\u0026gt;,\u0026#34;topic\u0026#34;:\u0026#34;\u0026lt;topic-name\u0026gt;\u0026#34;}] // Key {\u0026#34;offset\u0026#34;: \u0026lt;offset\u0026gt;} // Value 在 Dedicated 模式下, \u0026lt;connector-name\u0026gt; 通常是 MirrorSourceConnector，而在 Connect 模式下, \u0026lt;connector-name\u0026gt; 则是用户创建的 Connector 名称。\n‼️ 所以我们也就知道了，MirrorMaker2 在启动时，会尝试 恢复 之前的复制进度（偏移量），以确保数据复制的连续性和一致性。只是两种模式下，偏移量的存储位置不同而已。\n2. 数据拉取和写入 # MirrorSourceTask 的拉取逻辑由 poll 方法实现, 它由 Kafka Connect 框架控制调用。poll 方法的主要职责是从源集群拉取数据, 并将其转换为 Connect 框架的 SourceRecord 以供后续处理。\n@Override public List\u0026lt;SourceRecord\u0026gt; poll() { // ... try { ConsumerRecords\u0026lt;byte[], byte[]\u0026gt; records = consumer.poll(pollTimeout); List\u0026lt;SourceRecord\u0026gt; sourceRecords = new ArrayList\u0026lt;\u0026gt;(records.count()); for (ConsumerRecord\u0026lt;byte[], byte[]\u0026gt; record : records) { SourceRecord converted = convertRecord(record); sourceRecords.add(converted); TopicPartition topicPartition = new TopicPartition(converted.topic(), converted.kafkaPartition()); } // 省略部分代码 return sourceRecords; } catch (WakeupException e) { // Ignore exception handling } } SourceRecord convertRecord(ConsumerRecord\u0026lt;byte[], byte[]\u0026gt; record) { String targetTopic = formatRemoteTopic(record.topic()); // 处理为 \u0026lt;source-cluster-alias\u0026gt;.\u0026lt;topic\u0026gt; Headers headers = convertHeaders(record); return new SourceRecord( MirrorUtils.wrapPartition(new TopicPartition(record.topic(), record.partition()), sourceClusterAlias), MirrorUtils.wrapOffset(record.offset()), targetTopic, record.partition(), Schema.OPTIONAL_BYTES_SCHEMA, record.key(), Schema.BYTES_SCHEMA, record.value(), record.timestamp(), headers); } 3. 偏移量管理 # Kafka Connect 框架会自动管理偏移量的提交, 因此 MirrorSourceTask 不需要显式地提交偏移量。框架会定期将偏移量写入 \u0026lt;offset.storage.topic\u0026gt; 主题, 以确保在任务重启时能够恢复到正确的位置。\n在 poll 方法中已经在 SourceRecord 中封装了偏移量信息, 框架会根据这些信息来更新偏移量。\nKafkaOffsetBackingStore 的设计 # 前面提到了 MirrorSourceTask 使用了 KafkaOffsetBackingStore 来管理 topic 同步的 offset 进度，那么它具体是怎么进行管理的？ 如果只进不出，topic 随着时间的推移里面的消息会越来越多，它怎么从这么多数据中获取到正确的偏移量值？投递进去的消息难道就一直保存着吗？\n在 KafkaOffsetBackingStore 关于这些 topic 的创建如下，可以看到指定了 topic 为 compacted：\nprotected NewTopic newTopicDescription(final String topic, final WorkerConfig config) { Map\u0026lt;String, Object\u0026gt; topicSettings = config instanceof DistributedConfig ? ((DistributedConfig) config).offsetStorageTopicSettings() : Collections.emptyMap(); return TopicAdmin.defineTopic(topic) .config(topicSettings) // first so that we override user-supplied settings as needed .compacted() .partitions(config.getInt(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG)) .replicationFactor(config.getShort(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG)) .build(); } 而 compacted 实际对应了 topic 的 cleanup.policy = compact。\npublic NewTopicBuilder compacted() { this.configs.put(CLEANUP_POLICY_CONFIG, CLEANUP_POLICY_COMPACT); return this; } KafkaOffsetBackingStore 实现了 OffsetBackingStore，它实际又依赖了 KafkaBasedLog, 它明确的注释了：\nKafkaBasedLog provides a generic implementation of a shared, compacted log of records stored in Kafka that all clients need to consume and, at times, agree on their offset / that they have read to the end of the log.\n其内部创建了一个从 earliest 位点开始消费的 consumer，用来读取已存储的偏移量信息。\nprotected Consumer\u0026lt;K, V\u0026gt; createConsumer() { // Always force reset to the beginning of the log since this class wants to consume all available log data consumerConfigs.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \u0026#34;earliest\u0026#34;); // Turn off autocommit since we always want to consume the full log consumerConfigs.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false); return new KafkaConsumer\u0026lt;\u0026gt;(consumerConfigs); } /** * This method finds the end offsets of the Kafka log\u0026#39;s topic partitions, optionally retrying * if the {@code listOffsets()} method of the admin client throws a {@link RetriableException}. */ private void readToLogEnd(boolean shouldRetry) { Set\u0026lt;TopicPartition\u0026gt; assignment = consumer.assignment(); Map\u0026lt;TopicPartition, Long\u0026gt; endOffsets = readEndOffsets(assignment, shouldRetry); log.trace(\u0026#34;Reading to end of log offsets {}\u0026#34;, endOffsets); while (!endOffsets.isEmpty()) { Iterator\u0026lt;Map.Entry\u0026lt;TopicPartition, Long\u0026gt;\u0026gt; it = endOffsets.entrySet().iterator(); while (it.hasNext()) { Map.Entry\u0026lt;TopicPartition, Long\u0026gt; entry = it.next(); TopicPartition topicPartition = entry.getKey(); long endOffset = entry.getValue(); long lastConsumedOffset = consumer.position(topicPartition); if (lastConsumedOffset \u0026gt;= endOffset) { log.trace(\u0026#34;Read to end offset {} for {}\u0026#34;, endOffset, topicPartition); it.remove(); } else { log.trace(\u0026#34;Behind end offset {} for {}; last-read offset is {}\u0026#34;, endOffset, topicPartition, lastConsumedOffset); poll(Integer.MAX_VALUE); break; } } } } 简单总结一下，基于 kafka 的 offset 存储，利用了 kafka compact 机制来保存 offset 进度信息，这样可以避免消息量无限制增长。在需要从 kafka 中恢复数据时，则从头开始消费整个 topic 中的消息，保存最新的偏移量信息。\n迁移 # 经过前面的介绍，我们已经了解了 MirrorMaker2 的工作原理和偏移量管理机制。那么, 如果我们已经在使用 Dedicated 模式部署了 MirrorMaker2, 现在想要切换到 Connect 集群模式, 应该如何操作呢?\n这里考虑两个问题：\nKafka Connect 集群应该如何搭建？ 怎么迁移才能保证数据的一致，不会重复或丢失？ 搭建 Kafka Connect 集群 # Kafka Connect 集群的搭建可以参考官方文档或其他相关资料, 这里不做过多赘述。\nhttps://docs.confluent.io/platform/current/connect/userguide.html\n如何迁移进度 # MirrorMaker2 在 Dedicated 模式下运行一段时间后，会在 TARGET Kafka 集群 mm2-offsets.\u0026lt;source-cluster\u0026gt;.internal 主题中存储偏移量信息。而在 Connect 模式下, 偏移量信息则存储在 \u0026lt;offset.storage.topic\u0026gt; 主题中。如果我们不做任何处理，那么在切换到 Connect 模式后, 任务会从 \u0026lt;offset.storage.topic\u0026gt; 主题中读取偏移量, 由于该主题中没有任何数据, 任务会从头开始复制数据, 这会导致大量的重复数据。\n如果业务能够容忍重复数据, 那么可以直接切换, 但大多数场景下, 我们希望数据复制是连续且一致的。\n那么就需要我们把 mm2-offsets.\u0026lt;source-cluster\u0026gt;.internal 主题中的偏移量数据, 迁移到 \u0026lt;offset.storage.topic\u0026gt; 主题中, 那么在切换到 Connect 模式后, 任务就能从正确的位置继续复制数据, 避免重复或丢失。\n这个转换过程也很简单, 只需要编写一个脚本, 从 mm2-offsets.\u0026lt;source-cluster\u0026gt;.internal 主题中读取偏移量数据, 然后按照 \u0026lt;offset.storage.topic\u0026gt; 主题的格式写入即可。\n推荐将 offsets 数据保存到文件或者其他存储介质中, 方便人工验证或者调整。\n举例来说：\n假如 mm2-offsets.source.internal 主题中有如下消息：\nKey: [\u0026#34;MirrorSourceConnector\u0026#34;,{\u0026#34;cluster\u0026#34;:\u0026#34;source\u0026#34;,\u0026#34;partition\u0026#34;:0,\u0026#34;topic\u0026#34;:\u0026#34;test-topic\u0026#34;}] Value: {\u0026#34;offset\u0026#34;: 12345} 那么我们需要把它转换为 \u0026lt;offset.storage.topic\u0026gt; 主题中的消息：\nKey: [\u0026#34;mm2-source-connector\u0026#34;,{\u0026#34;cluster\u0026#34;:\u0026#34;source\u0026#34;,\u0026#34;partition\u0026#34;:0,\u0026#34;topic\u0026#34;:\u0026#34;test-topic\u0026#34;}] Value: {\u0026#34;offset\u0026#34;: 12345} 两者唯一的区别在于 Key 中的 connector 名称，如果 source.cluster.alias(cluster) 不同, 也需要做相应的调整。\n脚本举例如下:\noffset 迁移示例 def extract_dedicated_offsets(brokers, dedicated_offset_topic, output_file): \u0026#34;\u0026#34;\u0026#34;从专用模式的 offset topic 中提取 offset 信息并保存到文件\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;从 {dedicated_offset_topic} 提取 offsets, 创建 consumer 连接到 {brokers}\u0026#34;) consumer = KafkaConsumer( dedicated_offset_topic, bootstrap_servers=brokers, auto_offset_reset=\u0026#39;earliest\u0026#39;, enable_auto_commit=False, consumer_timeout_ms=10000, # 10秒内没有新消息就退出 key_deserializer=lambda x: json.loads(x.decode(\u0026#39;utf-8\u0026#39;)) if x else None, value_deserializer=lambda x: json.loads(x.decode(\u0026#39;utf-8\u0026#39;)) if x else None ) print(\u0026#34;开始消费消息...\u0026#34;) # 只保留每个 topic-partition 的最新 offset offsets = {} for message in consumer: if message.key and message.value: if message.key[0] != \u0026#34;MirrorSourceConnector\u0026#34;: # 只处理 MirrorSourceConnector 的消息 continue # 专用模式格式: Key[1] = {\u0026#34;cluster\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;partition\u0026#34;: ..., \u0026#34;topic\u0026#34;: \u0026#34;...\u0026#34;} partition_info = message.key[1] cluster = partition_info[\u0026#34;cluster\u0026#34;] topic = partition_info[\u0026#34;topic\u0026#34;] partition = partition_info[\u0026#34;partition\u0026#34;] offset = message.value[\u0026#34;offset\u0026#34;] # 过滤掉内部 topic (heartbeats, checkpoints 等) if topic.endswith(\u0026#39;heartbeats\u0026#39;) or \u0026#39;checkpoints\u0026#39; in topic or \u0026#39;internal\u0026#39; in topic: print(f\u0026#34;跳过内部 topic: {topic}\u0026#34;) continue # 使用 topic-partition 作为唯一标识，保留最新的 offset key = f\u0026#34;{cluster}-{topic}-{partition}\u0026#34; offsets[key] = { \u0026#34;cluster\u0026#34;: cluster, \u0026#34;topic\u0026#34;: topic, \u0026#34;partition\u0026#34;: partition, \u0026#34;offset\u0026#34;: offset } print(f\u0026#34;提取 offset: {key} -\u0026gt; {offset}\u0026#34;) else: print(f\u0026#34;跳过其他消息: {message.key[0] if message.key else \u0026#39;None\u0026#39;}\u0026#34;) consumer.close() # 保存到文件 with open(output_file, \u0026#39;w\u0026#39;) as f: json.dump(offsets, f, indent=2) print(f\u0026#34;提取了 {len(offsets)} 个 offset 记录到 {output_file}\u0026#34;) def publish_connect_offsets(brokers, connect_offset_topic, connector_name, input_file): \u0026#34;\u0026#34;\u0026#34;从文件读取 offset 信息，转换为 Connect 格式并发布到 Connect offset topic\u0026#34;\u0026#34;\u0026#34; # 从文件读取 offset 信息 with open(input_file, \u0026#39;r\u0026#39;) as f: offsets = json.load(f) producer = KafkaProducer( bootstrap_servers=brokers, key_serializer=lambda x: json.dumps(x, separators=(\u0026#39;,\u0026#39;, \u0026#39;:\u0026#39;)).encode(\u0026#39;utf-8\u0026#39;), value_serializer=lambda x: json.dumps(x, separators=(\u0026#39;,\u0026#39;, \u0026#39;:\u0026#39;)).encode(\u0026#39;utf-8\u0026#39;) ) success_count = 0 for offset_data in offsets.values(): # 转换为 Connect 格式 connect_key = [ connector_name, { \u0026#34;cluster\u0026#34;: offset_data[\u0026#34;cluster\u0026#34;], \u0026#34;partition\u0026#34;: offset_data[\u0026#34;partition\u0026#34;], \u0026#34;topic\u0026#34;: offset_data[\u0026#34;topic\u0026#34;] } ] connect_value = { \u0026#34;offset\u0026#34;: offset_data[\u0026#34;offset\u0026#34;] } try: # print(f\u0026#34;发布 offset: {connect_key} -\u0026gt; {connect_value} 到 {connect_offset_topic}\u0026#34;) future = producer.send(connect_offset_topic, key=connect_key, value=connect_value) future.get(timeout=10) success_count += 1 except KafkaError as e: print(f\u0026#34;发送 offset 失败: {e}\u0026#34;, file=sys.stderr) producer.flush() producer.close() print(f\u0026#34;成功发布了 {success_count}/{len(offsets)} 个 offset 记录\u0026#34;) 操作步骤 # 准备好 Kafka Connect 集群环境（需要搭建在 Target 集群中） 选择合适的时机停止 dedicated 模式的 MirrorMaker2 服务 运行 extract_dedicated_offsets 提取出 offset (从 mm2-offsets.\u0026lt;source-cluster-alias\u0026gt;.internal 中提取) 运行 publish_connect_offsets 写入 offset (写入 \u0026lt;offset.storage.topic\u0026gt;) 在 Kafka Connect 中创建 Connector 任务 curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; --data @mm2-connector.json http://localhost:8083/connectors connector 配置文件简单示例 更多配置请参考 MirrorSourceConnector 配置\n{ \u0026#34;name\u0026#34;: \u0026#34;mm2-source-connector\u0026#34;, // 连接名称, 可以自定义 \u0026#34;config\u0026#34;: { \u0026#34;connector.class\u0026#34;: \u0026#34;org.apache.kafka.connect.mirror.MirrorSourceConnector\u0026#34;, // 连接类, 固定值 \u0026#34;source.cluster.alias\u0026#34;: \u0026#34;source\u0026#34;, // 源集群别名, 可以自定义，迁移时写入 \u0026lt;offset.storage.topic\u0026gt; 的值 \u0026#34;target.cluster.alias\u0026#34;: \u0026#34;target\u0026#34;, // 目标集群别名, 可以自定义 \u0026#34;source.cluster.bootstrap.servers\u0026#34;: \u0026#34;kafka-source:9092\u0026#34;, // 源集群 bootstrap.servers, 迁移时需要指向源集群 \u0026#34;target.cluster.bootstrap.servers\u0026#34;: \u0026#34;kafka-target:9092\u0026#34;, // 目标集群 bootstrap.servers, 迁移时需要指向目标集群 \u0026#34;topics\u0026#34;: \u0026#34;test-topic,test-topic-second\u0026#34;, // 要迁移的 topic 列表, 可以自定义 \u0026#34;tasks.max\u0026#34;: 2 // 任务数量, 可以根据集群资源调整 // ... 更多配置 } } 总结 # 本文深入探讨了 Apache Kafka MirrorMaker2 的设计原理和实现机制，并详细介绍了从 Dedicated 模式迁移到 Connect 集群模式的完整方案。\n核心要点回顾 # 架构设计：\nMirrorMaker2 基于 Kafka Connect 框架构建，通过 MirrorSourceConnector 实现跨集群数据复制 支持 Dedicated 模式（独立运行）和 Connect 集群模式（分布式部署）两种部署方式 采用 round-robin 策略将 topic-partition 分配给多个 Task，实现负载均衡 偏移量管理：\n使用 KafkaOffsetBackingStore 基于 Kafka topic 存储偏移量信息 利用 Kafka 的 compact 机制避免偏移量数据无限增长 Dedicated 模式存储在 mm2-offsets.\u0026lt;source-cluster\u0026gt;.internal，Connect 模式存储在 \u0026lt;offset.storage.topic\u0026gt; 迁移策略：\n通过提取和转换偏移量数据，确保迁移过程中数据复制的连续性 关键在于正确转换 connector 名称和集群别名，保持偏移量的一致性 迁移过程需要短暂停机，但可以避免数据重复或丢失 实践建议 # 选择合适的部署模式：小规模场景可选择 Dedicated 模式，大规模生产环境建议使用 Connect 集群模式以获得更好的可扩展性和高可用性\n合理配置 Task 数量：根据 topic-partition 数量和集群资源合理设置 tasks.max，避免资源浪费或性能瓶颈\n做好迁移规划：迁移前充分测试偏移量提取和转换脚本，选择业务低峰期进行迁移操作\n监控和验证：迁移后密切监控数据复制状态，验证偏移量恢复的正确性\n注意事项 # 本文基于 Kafka v3.3.2，不同版本的实现细节可能有所差异 未涉及 MirrorCheckpointConnector 和 MirrorHeartbeatConnector 的详细介绍，实际使用中需要根据场景考虑是否需要启用这些组件 对于消费者组偏移量同步的场景，需要额外的迁移策略 通过理解 MirrorMaker2 的内部机制，我们能够更好地运维和优化跨集群数据复制方案，确保数据的一致性和系统的稳定性。\n参考 # Kafka Connect API Kafka Connect Configs Confluent - Kafka Connect Architecture Confluent - Kafka Connect Developer Guide 附 # Connect 配置 # Kafka Connect 的配置分为两个层面：\nWorker 配置：控制 Connect 集群的基础行为\nbootstrap.servers: Kafka 集群地址 group.id: Connect 集群的唯一标识 config.storage.topic: 存储 Connector 配置的内部主题，默认值为 connect-configs offset.storage.topic: 存储偏移量信息的内部主题，默认值为 connect-offsets status.storage.topic: 存储任务状态的内部主题，默认值为 connect-status 更多参见 Kafka Connect Configs\nConnector 配置：定义具体的数据复制任务\nconnector.class: 指定使用的 Connector 类 tasks.max: 最大任务数量 topics 或 topics.regex: 指定要处理的主题 其他特定于 Connector 的配置参数, 要参见相应的 Connector 源码或者文档。\nMirrorConnector 配置 # 表格根据 Kafka@3.3.2/MirrorConnectorConfig.java 整理而来。\n配置项 描述 默认值 enabled 是否启用源集群到目标集群的复制 true topics 要复制的主题。支持逗号分隔的主题名称和正则表达式 .* (所有主题) topics.exclude 排除的主题。支持逗号分隔的主题名称和正则表达式。排除规则优先于包含规则 \u0026quot;\u0026quot; (无排除主题) groups 要复制的消费者组。支持逗号分隔的组 ID 和正则表达式 .* (所有组) groups.exclude 排除的消费者组。支持逗号分隔的组 ID 和正则表达式。排除规则优先于包含规则 \u0026quot;\u0026quot; (无排除组) config.properties.exclude 不应复制的主题配置属性。支持逗号分隔的属性名称和正则表达式 \u0026quot;\u0026quot; (无排除属性) topic.filter.class 使用的 TopicFilter 类。选择要复制的主题 org.apache.kafka.connect.mirror.DefaultTopicFilter group.filter.class 使用的 GroupFilter 类。选择要复制的消费者组 org.apache.kafka.connect.mirror.DefaultGroupFilter config.property.filter.class 使用的 ConfigPropertyFilter 类。选择要复制的主题配置属性 org.apache.kafka.connect.mirror.DefaultConfigPropertyFilter source.cluster.alias 源集群别名 source target.cluster.alias 目标集群别名。用于指标报告 target consumer.poll.timeout.ms 轮询源集群时的超时时间 1000 (1 秒) admin.timeout.ms 管理任务的超时时间（例如检测新主题） 60000 (1 分钟) refresh.topics.enabled 是否定期检查新主题和分区 true refresh.topics.interval.seconds 主题刷新频率 600 (10 分钟) refresh.groups.enabled 是否定期检查新消费者组 true refresh.groups.interval.seconds 消费者组刷新频率 600 (10 分钟) sync.topic.configs.enabled 是否定期配置远程主题以匹配其对应的上游主题 true sync.topic.configs.interval.seconds 主题配置同步频率 600 (10 分钟) sync.topic.acls.enabled 是否定期配置远程主题 ACL 以匹配其对应的上游主题 true sync.topic.acls.interval.seconds 主题 ACL 同步频率 600 (10 分钟) emit.heartbeats.enabled 是否向目标集群发送心跳 true emit.heartbeats.interval.seconds 心跳频率 1 (1 秒) emit.checkpoints.enabled 是否将消费者偏移量复制到目标集群 true emit.checkpoints.interval.seconds 检查点频率 60 (1 分钟) sync.group.offsets.enabled 是否将转换后的偏移量同步到目标集群的 __consumer_offsets（如果没有活跃消费者） false sync.group.offsets.interval.seconds 消费者组偏移量同步频率 60 (1 分钟) replication.policy.class 定义远程主题命名约定的类 org.apache.kafka.connect.mirror.DefaultReplicationPolicy replication.policy.separator 远程主题命名约定中使用的分隔符 . (点) replication.factor 新创建的远程主题的复制因子 2 heartbeats.topic.replication.factor 心跳主题的复制因子 3 checkpoints.topic.replication.factor 检查点主题的复制因子 3 offset-syncs.topic.replication.factor 偏移量同步主题的复制因子 3 offset.lag.max 远程分区在重新同步之前可以落后的最大偏移量 100 (偏移量) offset-syncs.topic.location 偏移量同步主题的位置（源集群/目标集群） source metric.reporters 用作指标报告器的类列表 null (无额外报告器，默认启用 JMX) security.protocol 与 broker 通信时使用的安全协议 PLAINTEXT 除此之外还有一些通用的 Kafka 配置，采用前缀匹配再合并的方式，处理先后顺序是：\n\u0026lt;source/target\u0026gt;.cluster.* -\u0026gt; consumer.* -\u0026gt; \u0026lt;source/target\u0026gt;.producer.* -\u0026gt; 固定配置，参见下方 sourceConsumerConfig 代码处理顺序。\n配置会被剪切掉前缀再合并到最终的 props 中。如：source.cluster.bootstrap.servers 其实对应的是 bootstrap.servers 配置。\nMap\u0026lt;String, Object\u0026gt; sourceConsumerConfig() { Map\u0026lt;String, Object\u0026gt; props = new HashMap\u0026lt;\u0026gt;(); // 1. 基础集群配置（如 source.cluster.bootstrap.servers） props.putAll(originalsWithPrefix(SOURCE_CLUSTER_PREFIX)); // 2. 保留 Kafka 客户端配置（过滤非客户端属性） props.keySet().retainAll(MirrorClientConfig.CLIENT_CONFIG_DEF.names()); // 3. 通用 consumer. 配置（如 consumer.fetch.min.bytes） props.putAll(originalsWithPrefix(CONSUMER_CLIENT_PREFIX)); // 4. 源集群专用 consumer. 配置（如 source.consumer.max.poll.records，优先级最高） props.putAll(originalsWithPrefix(SOURCE_PREFIX + CONSUMER_CLIENT_PREFIX)); // 5. 强制覆盖关键配置（如禁用自动提交、默认 earliest 偏移量） props.put(ENABLE_AUTO_COMMIT_CONFIG, \u0026#34;false\u0026#34;); props.putIfAbsent(AUTO_OFFSET_RESET_CONFIG, \u0026#34;earliest\u0026#34;); return props; } Key 前缀 描述 生效范围 producer.* 通用生产者配置（如 producer.bootstrap.servers、producer.acks 等）。 同时作用于源集群和目标集群的生产者，除非被 source.producer. 或 target.producer. 覆盖。 consumer.* 通用消费者配置（如 consumer.bootstrap.servers、consumer.group.id 等）。 同时作用于源集群和目标集群的消费者，除非被 source.consumer. 或 target.consumer. 覆盖。 source.producer.* 源集群生产者的专用配置（优先级高于 producer.）。 仅用于源集群的生产者（如发送偏移量同步记录到 offsetSyncsTopic）。 source.consumer.* 源集群消费者的专用配置（优先级高于 consumer.）。 仅用于源集群的数据拉取消费者（如从源集群主题拉取待复制数据）。 target.producer.* 目标集群生产者的专用配置（优先级高于 producer.）。 仅用于目标集群的生产者（如复制数据到目标集群主题）。 target.consumer.* 目标集群消费者的专用配置（优先级高于 consumer.）。 仅用于目标集群的消费者（如读取目标集群元数据或偏移量同步记录）。 producer. 和 consumer. 前缀支持所有 Kafka 标准生产者/消费者配置（如 acks、retries、fetch.min.bytes 等），具体可参考 Kafka 官方文档配置： Consumer Config / Producer Config\n"},{"id":4,"href":"/2025/07/02/WebAssemblySpec%E6%B5%85%E6%9E%90%E5%92%8C%E8%B7%A8%E8%AF%AD%E8%A8%80%E5%AE%9E%E8%B7%B5/","title":"WebAssembly Spec 浅析和跨语言实践","section":"Posts","content":" 本文主要聚焦于 WebAssembly 的核心规范（Core Spec）部分。WASI 和 Embedder Spec 等其他部分并非本文的重点，感兴趣的读者可自行查阅相关资料。\n目的 # 随着 Web 技术的普及，越来越多的应用场景（如游戏、音视频处理、AI 等）需要在浏览器中运行。这些场景通常涉及 CPU 密集型任务，而现有 Web 引擎在处理此类任务时，性能仍不及原生语言。此外，C/C++ 等语言已积累了大量成熟的库，为了高效复用这些库以扩展 Web 的能力，急需一种新方式，使 C++/Rust 等语言也能在浏览器环境中运行。\n为解决上述问题，W3C 提出了 WebAssembly 规范。该规范设计了一种全新的、与机器无关的汇编指令集、运行时（可理解为虚拟机）以及内存模型等。\nWebAssembly 规范发布后，各大浏览器厂商迅速跟进支持，使其成为一种跨平台的二进制格式，能够在不同操作系统和硬件平台上运行。WASM 的应用范围也因此不再局限于 Web 场景，而是扩展到移动端、服务器端等领域。在云原生领域，Envoy、Kong 和 Apisix 等项目已支持 WASM 作为其扩展插件。WasmEdge 更进一步，直接提出将 WASM 应用于边缘计算场景，例如无服务器应用和函数即服务等。\n核心概念 # 在最新规范中，WebAssembly 定义了以下核心概念：\n概念 解释说明 Values 提供四种基础数值类型：32位和64位的整型及浮点型。32位整型可用于表示布尔值或内存地址。另有128位扩展整型用于高精度计算。 Instructions 基于栈式虚拟机执行的指令，分为简单指令和控制指令两类。 Traps 类似异常机制，当发生非法操作（如越界访问）时立即中止执行并报告宿主环境。功能类似于Go/Rust中的panic。 Functions 与其他编程语言一致，用于组织特定功能的代码，接收参数并返回结果。 Tables 数据结构上是一个数组，用于存储特定类型（funcref和externref），可通过索引模拟函数指针。 Linear Memory 一段可动态增长的连续字节数组，程序可存储和加载其中任意位置的数据，越界访问会触发Trap。 Modules 包含类型、函数、表、内存和全局变量的定义，作为部署、加载和编译的基本单位。可声明导入导出项，并支持定义自动执行的启动函数。 Embedder 指将WebAssembly程序嵌入宿主环境的实现方式，如wasmtime或Web环境中的WebAssembly运行时。 举例分析 # 接下来，我们将结合一个简单的 WASM 程序来辅助理解 WebAssembly 规范的内容。这里使用 Rust 语言编写一个简单的加法程序，并将其编译为 WASM 模块：\n#![no_std] #[unsafe(no_mangle)] pub extern \u0026#34;C\u0026#34; fn add(left: u64, right: u64) -\u0026gt; u64 { left + right } #[unsafe(no_mangle)] pub extern \u0026#34;C\u0026#34; fn subtract(left: u32, right: u32) -\u0026gt; u32 { left - right } 通过 wasmtime 这样的工具可以方便的调用 wasm 模块，如下：\n\u0026gt;\u0026gt;\u0026gt; wasmtime --invoke add ./wasm_demo.wasm 1 2 3 编译生成的 wasm 可以使用 wasm2wat 将二进制格式转为 WAT 文本格式，方便查看和分析。 wasm2wat wasm_demo.wasm -o wasm_demo.wat\n(module $wasm_demo.wasm (type (;0;) (func (param i64 i64) (result i64))) (type (;1;) (func (param i32 i32) (result i32))) (func $add (type 0) (param i64 i64) (result i64) local.get 1 local.get 0 i64.add) (func $subtract (type 1) (param i32 i32) (result i32) local.get 0 local.get 1 i32.sub) (memory (;0;) 16) (global $__stack_pointer (mut i32) (i32.const 1048576)) (global (;1;) i32 (i32.const 1048576)) (global (;2;) i32 (i32.const 1048576)) (export \u0026#34;memory\u0026#34; (memory 0)) (export \u0026#34;add\u0026#34; (func $add)) (export \u0026#34;subtract\u0026#34; (func $subtract)) (export \u0026#34;__data_end\u0026#34; (global 1)) (export \u0026#34;__heap_base\u0026#34; (global 2))) 整体结构 # WAT 文件以 (module \u0026hellip;) 开头，它定义了一个 WebAssembly 模块。模块是 WebAssembly 的基本封装单元，包含类型、函数、内存、全局变量等元素。\n(module $wasm_demo.wasm ... ) $wasm_demo.wasm 是模块的可选名称，用于在调试或引用时标识该模块。\n类型定义 # (type (;0;) (func (param i64 i64) (result i64))) (type ...) 用于定义函数类型。\n(;0;) 是类型的索引，这里表示该类型的索引为 0。\n(func (param i64 i64) (result i64)) 定义了一个函数类型，该函数接受两个 64 位整数（i64）作为参数，并返回一个 64 位整数。\n可以看到这里有两个函数类型定义，但是如果 add 和 subtract 的函数签名改成一致的，这里就可以只定义一个函数类型。\n函数定义 # (func $add (type 0) (param i64 i64) (result i64) local.get 1 local.get 0 i64.add) (func ...) 用于定义函数。\n$add 是函数的名称，用于在其他地方引用该函数。\n(type 0) 表示该函数的类型为索引为 0 的类型。\n(param i64 i64) 定义了两个 64 位整数参数。\n(result i64) 表示函数返回一个 64 位整数。\n函数体由指令序列组成，这里使用了 local.get 和 i64.add 指令。\n内存定义 # (memory (;0;) 16) (memory ...) 用于定义内存。\n(;0;) 是内存的索引，这里表示该内存的索引为 0。\n16 表示内存的初始页数为 16 页，每页大小为 64KB, 所以初始内存大小为 16 * 64KB = 1024KB。\n全局变量定义 # (global $__stack_pointer (mut i32) (i32.const 1048576)) (global ...) 用于定义全局变量。\n$__stack_pointer 是全局变量的名称。\n(mut i32) 表示该全局变量为可变的 32 位整数类型。\n(i32.const 1048576) 表示初始值为 1048576 的 32 位整数。\n导出定义 # (export \u0026#34;memory\u0026#34; (memory 0)) (export \u0026#34;add\u0026#34; (func $add)) (export \u0026#34;subtract\u0026#34; (func $subtract)) (export \u0026#34;__data_end\u0026#34; (global 1)) (export \u0026#34;__heap_base\u0026#34; (global 2)) (export ...) 用于定义导出项。\n\u0026quot;memory\u0026quot; 是导出项的名称，用于在其他地方引用该内存。\n(memory 0) 表示导出索引为 0 的内存区域。\n至于函数和全局变量的导出，也是类似的定义方式，这里就不一一列出了。\n小结 # WebAssembly，顾名思义，是一种汇编语言，它定义了非常底层的指令。但与传统汇编不同的是，其指令不依赖于具体的 CPU（宿主环境）。同时，它不仅是语言规范，还提供了一些与宿主环境（尤其是 Web 和 JavaScript 宿主环境）交互的约定。\n由此可见，WebAssembly 需要的是一个运行时支持，而这个运行时支持可以是一个嵌入某种语言的虚拟机，也可以是一个 docker 容器，也可以在裸机上运行。就像解释型语言一样，需要一个 “解释器” 来执行 WASM 模块中的指令。\n目前，WebAssembly 还在不断发展，例如 WASI (WebAssembly System Interface) 和 WASM Component Model 等更丰富的特性也已相继推出。\n这一小节通过从打包的 Module 出发，转换为 WAT 格式并逐个理解，虽然没有列举 Spec 中全部内容，但足以对 WebAssembly 的核心概念形成基本的认识。\n线性内存 # 通过前面的示例，我们得以窥见 WebAssembly 的大致结构和概念。至于其详细指令，作为使用者我们通常无需深究，我们更关注如何编写 WASM 模块、如何在不同语言的宿主环境中运行，以及如何在宿主环境与 WASM 模块之间进行数据传输。\n由于 WebAssembly 没有提供复杂的数据类型，当宿主环境和 WASM 模块需要传输复杂数据时，我们就需要利用 WASM 提供的线性内存进行数据交互。\n复杂数据结构包括：数组、字符串、结构体、联合体、枚举等。\n这里我们将以最简单的字符串为例，演示宿主环境和 WASM 模块如何通过线性内存进行字符串交互。\n需求 # 假设我们使用 Rust 实现了一个为 JSON 字符串动态添加字段的 WASM 模块，现在想要在 python 和 javascript 中调用该功能。\nRust 到 WASM # 在 Rust 中，我们需要实现一个 allocate 函数，用于在 WASM 内存中分配一段空间。同时，我们还需要实现一个 deallocate 函数，用于释放之前分配的内存，并将这两个函数导出，以便在其他语言中调用。\n至于怎么为 JSON 字符串动态添加字段，我们可以使用 serde_json 库来进行 JSON 字符串的解析和序列化。\n#[no_mangle] pub extern \u0026#34;C\u0026#34; fn allocate(size: usize) -\u0026gt; *mut u8 { let mut buffer = Vec::with_capacity(size); let ptr = buffer.as_mut_ptr(); core::mem::forget(buffer); // 阻止 Vec 在这里被 Drop ptr } #[no_mangle] pub extern \u0026#34;C\u0026#34; fn deallocate(ptr: *mut u8, capacity: usize) { unsafe { let _ = Vec::from_raw_parts(ptr, 0, capacity); } } #[no_mangle] pub extern \u0026#34;C\u0026#34; fn json_add_field( json_ptr: *const u8, json_len: usize, field_ptr: *const u8, field_len: usize, ) -\u0026gt; u64 { // 加载 host 传入的字符串，并解析 let json_bytes = unsafe { slice::from_raw_parts(json_ptr, json_len) }; let field_bytes = unsafe { slice::from_raw_parts(field_ptr, field_len) }; let mut json_value: serde_json::Value = match serde_json::from_slice(json_bytes) { Ok(v) =\u0026gt; v, Err(_) =\u0026gt; return 0, // 错误处理 }; let field_value: serde_json::Value = match serde_json::from_slice(field_bytes) { Ok(v) =\u0026gt; v, Err(_) =\u0026gt; return 0, // 错误处理 }; // 执行添加字段的逻辑 if let (Some(obj), Some(field_obj)) = (json_value.as_object_mut(), field_value.as_object()) { for (k, v) in field_obj { obj.insert(k.clone(), v.clone()); } } else { return 0; } // 将修改后的 JSON 序列化回字符串 let result_json = match serde_json::to_string(\u0026amp;json_value) { Ok(s) =\u0026gt; s, Err(_) =\u0026gt; return 0, // 错误处理 }; // 将结果字符串转换为字节向量并获取指针和长度 let mut result_bytes = result_json.into_bytes(); let len = result_bytes.len(); let ptr = result_bytes.as_mut_ptr() as u64; // 泄漏内存，以便 Host 可以访问它 core::mem::forget(result_bytes); // 将长度和指针打包成一个 u64 返回 // 长度放在高 32 位，指针放在低 32 位 // Q: WASM 支持 multi-value 返回，RUST 如何使用这个特性呢？ (len as u64) \u0026lt;\u0026lt; 32 | ptr } 在 python 中使用 # 在 python 中，我们可以使用 wasmtime 库来加载和运行 WASM 模块。运行效果如下：\n$ python run.py Initial JSON: {\u0026#34;name\u0026#34;: \u0026#34;Alice\u0026#34;, \u0026#34;age\u0026#34;: 30} Adding Fields: {\u0026#34;city\u0026#34;: \u0026#34;New York\u0026#34;, \u0026#34;occupation\u0026#34;: \u0026#34;Engineer\u0026#34;} Modified JSON: {\u0026#34;age\u0026#34;:30,\u0026#34;city\u0026#34;:\u0026#34;New York\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Alice\u0026#34;,\u0026#34;occupation\u0026#34;:\u0026#34;Engineer\u0026#34;} store = wasmtime.Store() module = wasmtime.Module.from_file(store.engine, \u0026#34;wasms/rust.wasm\u0026#34;) instance = wasmtime.Instance(store, module, []) # 获取 WASM 中的导出 exports = instance.exports(store) wasm_memory = exports[\u0026#34;memory\u0026#34;] wasm_allocate = exports[\u0026#34;allocate\u0026#34;] wasm_deallocate = exports[\u0026#34;deallocate\u0026#34;] wasm_json_add_field = exports[\u0026#34;json_add_field\u0026#34;] def rust_string_to_python(ptr, length): data_bytes = wasm_memory.read(store, ptr, ptr + length) return data_bytes.decode(\u0026#39;utf-8\u0026#39;) def python_string_to_wasm(s: str) -\u0026gt; tuple[int, int]: s_bytes = s.encode(\u0026#39;utf-8\u0026#39;) s_len = len(s_bytes) s_ptr = wasm_allocate(store, s_len) wasm_memory.write(store, s_bytes, s_ptr) return s_ptr, s_len def process_json_with_wasm(original_json: str, field_to_add: str) -\u0026gt; str: json_ptr, json_len = python_string_to_wasm(original_json) field_ptr, field_len = python_string_to_wasm(field_to_add) # 调用函数 result_u64 = wasm_json_add_field(store, json_ptr, json_len, field_ptr, field_len) # 释放 WASM 中的内存 wasm_deallocate(store, json_ptr, json_len) wasm_deallocate(store, field_ptr, field_len) # 解析 WASM 函数返回的指针和长度 if result_u64 == 0: raise ValueError(\u0026#34;Wasm function returned an error (0)\u0026#34;) result_len = result_u64 \u0026gt;\u0026gt; 32 result_ptr = result_u64 \u0026amp; 0xFFFFFFFF result_json_str = rust_string_to_python(result_ptr, result_len) # 在 Rust 这部分内存是没有被释放的，需要手动释放 wasm_deallocate(store, result_ptr, result_len) return result_json_str if __name__ == \u0026#34;__main__\u0026#34;: initial_json = \u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;Alice\u0026#34;, \u0026#34;age\u0026#34;: 30}\u0026#39; new_field = \u0026#39;{\u0026#34;city\u0026#34;: \u0026#34;New York\u0026#34;, \u0026#34;occupation\u0026#34;: \u0026#34;Engineer\u0026#34;}\u0026#39; modified_json = process_json_with_wasm(initial_json, new_field) print(f\u0026#34;Modified JSON: {modified_json}\u0026#34;) 在 NodeJS 中使用 # 在 NodeJS 中已经自带了 WebAssembly 模块，我们可以直接使用。\nasync function runWasm() { // 1. 读取 Wasm 模块的二进制数据 const wasmPath = path.join(__dirname, \u0026#39;wasms\u0026#39;, \u0026#39;rust.wasm\u0026#39;); const wasmBytes = fs.readFileSync(wasmPath); // 2. 创建 WebAssembly 内存实例 // initial: 初始内存页数 (1页 = 64KB) // maximum: 最大内存页数 (可选，但推荐设置，防止内存无限增长) const memory = new WebAssembly.Memory({ initial: 1, maximum: 16 }); // 1页 = 64KB // 3. 自定义定义导入对象 (imports object) const importObject = { env: { memory: memory, } }; // 4. 实例化 Wasm 模块 const wasm = await WebAssembly.instantiate(wasmBytes, importObject); const exports = wasm.instance.exports; // 5. 获取导出的函数和内存 const wasmAllocate = exports.allocate; const wasmDeallocate = exports.deallocate; const wasmJsonAddField = exports.json_add_field; const wasmMemory = exports.memory; // 直接获取导出的内存对象 function writeStringToWasm(jsString) { const encoder = new TextEncoder(\u0026#39;utf-8\u0026#39;); const encodedBytes = encoder.encode(jsString); const byteLength = encodedBytes.length; const ptr = wasmAllocate(byteLength); const wasmByteView = new Uint8Array(wasmMemory.buffer); wasmByteView.set(encodedBytes, ptr); return { ptr, len: byteLength }; } function readStringFromWasm(ptr, len) { const wasmByteView = new Uint8Array(wasmMemory.buffer, ptr, len); const decoder = new TextDecoder(\u0026#39;utf-8\u0026#39;); return decoder.decode(wasmByteView); } const initialJson = \u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;Alice\u0026#34;, \u0026#34;age\u0026#34;: 30}\u0026#39;; const newField = \u0026#39;{\u0026#34;city\u0026#34;: \u0026#34;New York\u0026#34;, \u0026#34;occupation\u0026#34;: \u0026#34;Engineer\u0026#34;}\u0026#39;; console.log(`Initial JSON: ${initialJson}`); console.log(`Field to add: ${newField}`); try { // 写入原始 JSON 字符串到 Wasm 内存 const { ptr: jsonPtr, len: jsonLen } = writeStringToWasm(initialJson); // 写入要添加的字段 JSON 字符串到 Wasm 内存 const { ptr: fieldPtr, len: fieldLen } = writeStringToWasm(newField); // 调用 Wasm 函数 // Rust 函数返回一个 u64，高 32 位是长度，低 32 位是指针 const resultU64 = wasmJsonAddField(jsonPtr, jsonLen, fieldPtr, fieldLen); // 释放输入字符串的 Wasm 内存 wasmDeallocate(jsonPtr, jsonLen); wasmDeallocate(fieldPtr, fieldLen); if (resultU64 === 0) { throw new Error(\u0026#34;Wasm function returned an error (0)\u0026#34;); } // 从 u64 中解析指针和长度 const resultLen = Number(resultU64 \u0026gt;\u0026gt; 32n); // 使用 BigInt 操作 u64 const resultPtr = Number(resultU64 \u0026amp; 0xFFFFFFFFn); // 使用 BigInt 操作 u64 // 从 Wasm 内存中读取结果字符串 const modifiedJson = readStringFromWasm(resultPtr, resultLen); console.log(`Modified JSON: ${modifiedJson}`); // 释放 Wasm 模块分配的内存 wasmDeallocate(resultPtr, resultLen); console.log(\u0026#34;Got JSON:\u0026#34;, expectedJsonDict); } catch (e) { console.error(`An error occurred: ${e.message}`); } } runWasm(); 整体交互流程上与 python 大同小异。\n小结 # 全部代码可以在 https://github.com/yeqown/wasm-demo 找到。\n简而言之，WASM 内存允许开发者自行管理内存，而不依赖于宿主环境的内存管理机制。虽然诸如 wasm-bindgen 这样的工具可以实现 Rust 到 JavaScript 的绑定，但若需支持其他语言，其能力则有所局限。\n然而，一旦理解了其底层原理，即使是更高级的数据结构，也能够自行封装实现。\n🤣 这仿佛又回到了 C 语言手动内存管理的时代。\n参考 # WebAssembly 规范 WebAssembly 指令索引 WebAssembly 类型索引 WebAssembly 文本格式 "},{"id":5,"href":"/2025/05/08/%E5%9C%A8-Kubernetes%E4%B8%AD%E5%AE%9E%E7%8E%B0gRPC%E6%B5%81%E9%87%8F%E9%95%9C%E5%83%8F/","title":"在 Kubernetes 中实现 gRPC 流量的镜像和对比","section":"Posts","content":"本文主要解决在服务重构过程中如何保证新旧服务行为一致性的问题。\n场景描述 # 现有一个 python 开发的 gRPC 微服务提供了一些 数据查询 接口 供 上层应用使用，随着业务流量的增加运维这个服务的成本也逐渐增加，为了降低运维成本和提高性能 (木有擅长 python 高性能的开发)，因此选择了使用 go 语言对这个服务进行重写。在开发完成之后，需要对新服务的 gRPC 接口进行验证。\n这种场景对测试开发人员来说，实在是太熟悉了吧？典型的 重放验证，马上能想到的验证手段就是：\n如果有存量的单元测试，那么直接重新跑一遍单元测试就能快速的完成验证。 没有单元测试的情况，那么可以将新服务部署起来，通过流量复制的方式将旧服务的流量复制到新服务上，然后对比两个服务的返回结果是否一致。 flowchart LR %% 定义布局方向和间距 subgraph s1[\"方案一: 单元测试验证\"] direction TB UT[单元测试] --\u003e|执行| NS1[新服务] end subgraph s2[\"方案二: 流量复制验证\"] direction TB C[客户端] --\u003e|请求| OS[旧服务] OS --\u003e|响应| C OS --\u003e|复制流量| NS2[新服务] NS2 --\u003e|对比响应| OS end %% 设置布局方向和对齐方式 s1 ~~~ s2 但是很遗憾 😭，并没有成熟的单元测试；测试人员也都是人肉测试，对于内部服务的接口验证帮助不大，因此这里采用第二种方式进行验证。\n服务均采用 Kubernetes 部署。\n方案介绍 # HTTP 流量的复制重放工具就很多很成熟了，而且往往在 网关/代理 一侧就能实现流量复制甚至对比。\n工具 分类 文档链接 Nginx mirror 网关/代理 https://nginx.org/en/docs/http/ngx_http_mirror_module.html APISIX 网关/代理 https://apisix.apache.org/docs/apisix/plugins/proxy-mirror/ Istio: Virtual Service mirror Service Mesh https://istio.io/latest/docs/tasks/traffic-management/mirroring/ GoReplay 流量镜像 https://github.com/buger/goreplay tcpcopy 流量镜像 https://github.com/session-replay-tools/tcpcopy tcpcopy 相比于其他工具方案，虽然不能直接使用，但是其作用于 TCP 传输层，功能会更丰富。相比之下，gRPC 流量复制工具就没有那么成熟了。\n由于这里是一个 Kubernetes gRPC 微服务，可以想到的就是使用 sidecar 容器的方式来实现流量镜像，并记录到本地文件中，然后通过对比脚本来对比新旧服务的响应内容。\nMirror and Compare flow Kubernetes 中的 sidecar # Kubenetes 中的 sidecar 是指在应用容器之外，部署一个独立的容器，用于处理应用容器的请求和响应。sidecar 容器和应用容器共享同一个网络命名空间，因此可以通过 localhost 来进行通信。\n因此不管是使用 Istio 还是 grpcreplay，都需要在应用容器之外部署一个 sidecar 容器。只不过两个容器的作用原理不相同，grpcreplay 一个是通过嗅探网络协议栈上的 gRPC 流量，将这部分数据报文解析再使用；而 Istio 则是接管了应用容器的请求和响应，然后将流量复制到新的服务上。\ngrpcreplay 简介 # grpcreplay 是一款开源的 gRPC 流量解析和重放工具，它类似于 goreplay 都是通过抓包实现流量的复制，而不是代理的方式。另外它还支持多种输入输出，包括文件、网卡、消息队列、标准输入输出等。当然也有一些限制:\n目前支持到 h2c 暂不支持 h2 序列化器支持 protobuf 不支持 Streaming h2c 指的是 HTTP/2 over TCP，h2 指的是 HTTP/2 over TLS/SSL。\ngrpcr help message $ grpcreplay -h ______ ____ ____ ______ ____ / ____// __ \\ / __ \\ / ____// __ \\ / / __ / /_/ // /_/ // / / /_/ / / /_/ // _, _// ____// /___ / _, _/ \\____//_/ |_|/_/ \\____//_/ |_| Usage of grpcreplay: -codec string (default \u0026#34;simple\u0026#34;) -exit-after duration exit after specified duration -include-filter-method-match string filter requests when the method matches the specified regular expression -input-file-directory value grpcr --input-file-directory=\u0026#34;/tmp/mycapture\u0026#34; --output-grpc=\u0026#34;grpc://xx.xx.xx.xx:35001“ (default []) -input-file-read-depth int (default 100) -input-file-replay-speed float (default 1) -input-raw value Capture traffic from given port (use RAW sockets and require *sudo* access): # Capture traffic from 80 port grpcr --input-raw=\u0026#34;0.0.0.0:80\u0026#34; --output-grpc=\u0026#34;grpc://xx.xx.xx.xx:35001\u0026#34; (default []) -input-rocketmq-access-key string -input-rocketmq-group-name string (default \u0026#34;fakeGroupName\u0026#34;) -input-rocketmq-name-server value grpcr --input-rocketmq-name-server=\u0026#34;192.168.2.100:9876\u0026#34; --output-grpc=\u0026#34;grpc://xx.xx.xx.xx:35001\u0026#34; (default []) -input-rocketmq-secret-key string -input-rocketmq-topic string (default \u0026#34;test\u0026#34;) -output-file-directory value Write incoming requests to file: grpcr --input-raw=\u0026#34;0.0.0.0:80\u0026#34; --output-file-directory=\u0026#34;/tmp/mycapture\u0026#34; (default []) -output-file-max-age int MaxAge is the maximum number of days to retain old log files based on the timestamp encoded in their filename (default 30) -output-file-max-backups int MaxBackups is the maximum number of old log files to retain. (default 10) -output-file-max-size int MaxSize is the maximum size in megabytes of the log file before it gets rotated. (default 500) -output-grpc value Forwards incoming requests to given grpc address. # Redirect all incoming requests to xxx.com address grpcr --input-raw=\u0026#34;0.0.0.0:80\u0026#34; --output-grpc=\u0026#34;grpc://xx.xx.xx.xx:35001\u0026#34;) (default []) -output-grpc-worker-number int multiple workers call services concurrently (default 5) -output-rocketmq-access-key string -output-rocketmq-name-server value grpcr --input-raw=\u0026#34;0.0.0.0:80\u0026#34; --output-rocketmq-name-server=\u0026#34;192.168.2.100:9876\u0026#34; (default []) -output-rocketmq-secret-key string -output-rocketmq-topic string (default \u0026#34;test\u0026#34;) -output-stdout Just prints data to console -rate-limit-qps int the capture rate per second limit for Query (default -1) -record-response record response -version print version 实践1: grpcreplay sidecar # 想要把 grpcreplay 想要作为 sidecar 容器部署在 Kubernetes 中，这里需要做的工作有：\ngrpcreplay 镜像打包并推送到镜像仓库 添加 PVC 以存储 grpcreplay 的流量数据 修改新旧服务的 Deployment 以配置 sidecar 容器。 架构设计 # 图以蔽之：\n架构描述 镜像打包 # 提前将 grpcreplay 源码下载到本地，然后编写 Dockerfile 进行构建。\nFROM golang:1.23.6 AS builder WORKDIR /go/src/github.com/vearne/grpcreplay/ COPY . /go/src/github.com/vearne/grpcreplay/ # 安装依赖 RUN apt-get update \u0026amp;\u0026amp; apt-get install -y libpcap-dev RUN go get # 构建应用 RUN CGO_ENABLED=1 go build -ldflags \u0026#34;-s -w\u0026#34; -o grpcr # 使用多阶段构建减小镜像大小 FROM debian:bookworm-slim # 安装运行时依赖 RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y libpcap0.8 \u0026amp;\u0026amp; \\ apt-get clean \u0026amp;\u0026amp; \\ rm -rf /var/lib/apt/lists/* WORKDIR /app # 从构建阶段复制二进制文件 COPY --from=builder /go/src/github.com/vearne/grpcreplay/grpcr /app/ # 设置容器入口点 ENTRYPOINT [\u0026#34;/app/grpcr\u0026#34;] # 构建镜像 podman build --platform linux/amd64 --rm -t grpcr -f Dockerfile . # 修改镜像标签 podman tag grpcr yeqown/grpcr:v0.2.6 # 推送到 hub.docker.com 镜像仓库 https://hub.docker.com/repository/docker/yeqown/grpcr podman push yeqown/grpcr:v0.2.6 添加 PVC # 这里仅供参考，也不是必要的步骤，只要能给 POD 挂载一个 PV 即可。\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: grpc-capture-pvc spec accessModes: - ReadWriteMany # 多个Pod需要同时访问 resources: requests: storage: 10Gi storageClassName: YOUR_STORAGE_CLASS_NAME # 替换为你的存储类名称 配置 Sidecar: grpcreplay # 配置新旧服务的 Deployment 文件以配置 sidecar 容器。同样的这里以实际情况为准\n旧服务：\napiVersion: apps/v1 kind: Deployment metadata: name: old-grpc labels: app: old-grpc spec: replicas: 1 selector: matchLabels: app: old-grpc template: metadata: labels: app: old-grpc spec: containers: - name: old-grpc image: your-registry/old-grpc:latest ports: - containerPort: 50051 # gRPC服务端口 + # grpcreplay sidecar容器 + - name: grpcreplay + image: docker.io/yeqown/grpcr:v0.2.6 + securityContext: + capabilities: + add: [\u0026#34;NET_ADMIN\u0026#34;, \u0026#34;NET_RAW\u0026#34;] # 需要这些权限来捕获网络流量 + args: + - \u0026#34;--input-raw=0.0.0.0:50051\u0026#34; + - \u0026#34;--output-grpc=grpc://new-grpc:50051\u0026#34; + - \u0026#34;--output-file-directory=/capture/server\u0026#34; + - \u0026#34;--record-response\u0026#34; + - \u0026#34;--codec=json\u0026#34; + volumeMounts: + - name: capture-volume + mountPath: /capture/server + volumes: + - name: capture-volume + persistentVolumeClaim: + claimName: grpc-capture-pvc 新服务：\napiVersion: apps/v1 kind: Deployment metadata: name: new-grpc labels: app: new-grpc spec: replicas: 1 selector: matchLabels: app: new-grpc template: metadata: labels: app: new-grpc spec: containers: - name: new-grpc image: your-registry/new-grpc:latest ports: - containerPort: 50051 # gRPC服务端口 + # grpcreplay sidecar容器（可选，用于记录请求和响应） + - name: grpcreplay + image: docker.io/yeqown/grpcr:v0.2.6 + securityContext: + capabilities: + add: [\u0026#34;NET_ADMIN\u0026#34;, \u0026#34;NET_RAW\u0026#34;] + args: + - \u0026#34;--input-raw=0.0.0.0:50051\u0026#34; + - \u0026#34;--output-stdout\u0026#34; + - \u0026#34;--output-file-directory=/capture/mirror\u0026#34; + - \u0026#34;--record-response\u0026#34; + - \u0026#34;--codec=json\u0026#34; + volumeMounts: + - name: capture-volume + mountPath: /capture/mirror + + volumes: + - name: capture-volume + persistentVolumeClaim: + claimName: grpc-capture-pvc 服务启动后，会在 /capture/server 和 /capture/mirror 目录下生成 grpcreplay 的流量数据。\ngrpcreplay 流量数据 踩坑记录 # 使用 grpcreplay 作为 sidecar 容器部署在 Kubernetes 中后，经过一段时间后，发现记录的文件会出现如下的情况：\n单行数据不完整，JSON 无法解析 Response 记录和方法不匹配，如：请求 Method1 但是记录的 Response 确实 gRPC 健康检查的响应 某些记录 Request 为空，实际上该请求并不为空。 导致对比无法进行，会出现误判的情况。\n大概率是 grpcreplay 本身采集写入的问题，这里再没有进一步深入追查。\n实践2: Istio VirtualService mirror # Istio 是一个非常成熟的 Service Mesh 解决方案，它可以在不修改应用代码的情况下，对 gRPC 流量进行镜像和重放。只需要为服务配置好 VirtualService mirror 规则即可。\n字段 路径层级 描述 mirror spec.http[].mirror 指定要镜像的目标服务。 mirrorPercentage spec.http[].mirrorPercentage 指定要镜像的流量百分比。 复制流量之后，这个方案还需要获取 gRPC 服务的响应内容，记录到本地文件中，然后通过对比脚本来对比新旧服务的响应内容。这里可以考虑实现 Istio 的插件去记录，也可以选择在服务中添加 gRPC 拦截器来实现记录。\n配置源服务 VirtualService # 这里为 old-grpc 服务配置 VirtualService。\napiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: istio-gateway namespace: xxx spec: selector: istio: ingressgateway servers: - hosts: - old-grpc.xxx.com port: name: grpc number: 443 protocol: GRPC --- apiVersion: networking.istio.io/v1 kind: VirtualService metadata: name: old-grpc spec: hosts: - old-grpc.default.svc.cluster.local - old-grpc.xxx.com gateways: - xxx/istio-gateway http: - route: - destination: host: old-grpc.default.svc.cluster.local port: number: 50051 weight: 100 mirror: host: new-grpc.default.svc.cluster.local mirrorPercentage: value: 100 踩坑记录 # ‼️ 这里在实践过程中遇到的问题是：使用了 AWS 的 ALB 作为 Ingress Load Balancer，old-grpc 原始暴露了 old-grpc.xxx.com 的一个对外服务的域名，在将该 ingress 的 backend 切换到 istio-ingressgateway 之后，请求异常 (请求响应状态码： UNIMPLEMENTED)。\nIstio ingressgateway Issue 从上图可以发现，ingressgateway 未能正确的将请求转发到对应的后端服务（Service = Unkonwn） 所有的请求全都失败了，这里的问题是：\n原本 ALB 处的配置是按照 HTTP(s) 配置的，转发到了 ingressgateway 的 80 端口，而 ingressgateway 的 80 端口配置为处理 HTTP 流量。 istio ingressgateway 可能存在无法自动识别流量的情况，需要显示的配置 gRPC 协议。 第一个问题的解决方案是：将 ingress 转发端口从 80 改为 443。同时需要注意 AWS 转发过来的流量也需要在 ingress 中添加注解 alb.ingress.kubernetes.io/backend-protocol-version: \u0026quot;GRPC\u0026quot;\nAWS ALB Ingress 代理 gPRC 流量\n第二个问题的解决方案是：service 显式的配置为 gRPC 协议。port name 改为：protocol[-suffix] 格式，或者在 kubernetes 1.18+ 版本中直接使用 appProtocol 字段。如下：\napiVersion: v1 kind: Service metadata: name: old-grpc namespace: xxx spec: selector: app: old-grpc ports: - name: grpc-xxx # 显示的配置为 gRPC 协议 appProtocol: grpc # 显式的配置为 gRPC 协议，这个配置的优先级更高 port: 50051 protocol: TCP Istio 如何识别流量协议\n编写 gPRC interceptor # 各个语言要实现 gRPC 拦截器的方式都不尽相同，但都大差不差，这里以 python 语言为例：\nPS: 为啥用 python 举例？对我来说，它是真抽象～\nfrom grpc_interceptor import ServerInterceptor class RecordReqRespInterceptor(ServerInterceptor): \u0026#34;\u0026#34;\u0026#34; Interceptor to record request and response information. Usage: server = grpc.server( ThreadPoolExecutor(max_workers=5), interceptors=[RecordReqRespInterceptor(store_directory)]) \u0026#34;\u0026#34;\u0026#34; fd = None store_directory = None # /capture/server/capture.log _lock = threading.Lock() # 添加线程锁 def __init__(self, store_directory=None): if not store_directory: __LOGGER__.warning( \u0026#34;Warning: store_directory is not specified, request and response information will not be recorded.\u0026#34;) return filename = store_directory + \u0026#34;capture.log\u0026#34; try: self.store_directory = filename self.fd = open(filename, \u0026#39;a\u0026#39;) except Exception as e: __LOGGER__.error(f\u0026#34;Failed to open file {store_directory}: {e}\u0026#34;) def record(self, call_detail: CallDetails, error: str = None): if self.fd is None or self.store_directory is None: return # 从handler_call_details中提取基本信息 method = call_detail.method if method in [ \u0026#34;/grpc.health.v1.Health/Check\u0026#34;, \u0026#34;/grpc.reflection.v1.ServerReflection/ServerReflectionInfo\u0026#34;, \u0026#34;/grpc.reflection.v1alpha.ServerReflection/ServerReflectionInfo\u0026#34;, ]: return metadata = dict(call_detail.invocation_metadata) timestamp_nano = time.time_ns() timestamp_seconds = int(timestamp_nano / 1000000000) req_body = json.dumps(MessageToDict(call_detail.request), separators=(\u0026#39;,\u0026#39;, \u0026#39;:\u0026#39;)) resp_body = json.dumps(MessageToDict(call_detail.response), separators=(\u0026#39;,\u0026#39;, \u0026#39;:\u0026#39;)) # 构建记录 record = { \u0026#34;method\u0026#34;: method, \u0026#34;request\u0026#34;: { \u0026#34;headers\u0026#34;: metadata, \u0026#34;body\u0026#34;: req_body, }, \u0026#34;response\u0026#34;: { \u0026#34;headers\u0026#34;: { \u0026#34;failed\u0026#34;: error is not None, \u0026#34;error\u0026#34;: error if error else \u0026#34;\u0026#34;, }, \u0026#34;body\u0026#34;: resp_body, } } try: with self._lock: # 使用锁保证写入原子性 self.fd.write(json.dumps(record) + \u0026#39;\\n\u0026#39;) self.fd.flush() except Exception as e: __LOGGER__.error(f\u0026#34;Failed to write to file {self.store_directory}: {e}\u0026#34;) # 实现 ServerInterceptor 抽象方法 def intercept( self, method: Callable, request_or_iterator: Any, context: grpc.ServicerContext, method_name: str, ): response = method(request_or_iterator, context) self.record(CallDetails(method_name, context, request_or_iterator, response)) return response 对比脚本 # capture.log 中的保存每一条记录 JSON 结构如下：\n{ \u0026#34;method\u0026#34;: \u0026#34;/pbpkg.Service/Method\u0026#34;, \u0026#34;request\u0026#34;: { \u0026#34;headers\u0026#34;: { \u0026#34;:authority\u0026#34;: \u0026#34;10.90.74.23:50051\u0026#34;, \u0026#34;:method\u0026#34;: \u0026#34;POST\u0026#34;, \u0026#34;:path\u0026#34;: \u0026#34;/pbpkg.Service/Method\u0026#34;, \u0026#34;:scheme\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;content-type\u0026#34;: \u0026#34;application/grpc\u0026#34;, \u0026#34;grpc-accept-encoding\u0026#34;: \u0026#34;gzip\u0026#34;, \u0026#34;te\u0026#34;: \u0026#34;trailers\u0026#34;, \u0026#34;user-agent\u0026#34;: \u0026#34;grpc-go/1.61.0\u0026#34; }, \u0026#34;body\u0026#34;: \u0026#34;\u0026lt;ignored data string\u0026gt;\u0026#34; }, \u0026#34;response\u0026#34;: { \u0026#34;headers\u0026#34;: { \u0026#34;:status\u0026#34;: \u0026#34;200\u0026#34;, \u0026#34;content-type\u0026#34;: \u0026#34;application/grpc\u0026#34;, \u0026#34;grpc-message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;grpc-status\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;body\u0026#34;: \u0026#34;\u0026lt;ignored data string\u0026gt;\u0026#34; } } JSON 中已经详细的记录了请求和响应的内容，注意一定需要在请求头中携带一条唯一表示，比如 trace_id 或者 request_id 方便将源服务器和 镜像服务器的请求响应串联起来。脚本的对比逻辑伪代码如下：\ndef parse_capture_log(log_file) -\u0026gt; Record: \u0026#34;\u0026#34;\u0026#34; 解析 grpcreplay 生成的 capture.log 文件，返回一个 Record 对象 \u0026#34;\u0026#34;\u0026#34; records = {} with open(log_file, \u0026#34;r\u0026#34;) as f: for line in f: record = Record.parse_raw(line) records[record.uuid] = record def main(): source_records = parse_capture_log(\u0026#34;capture.source.log\u0026#34;) mirror_records = parse_capture_log(\u0026#34;capture.mirror.log\u0026#34;) for source_record in source_records: mirror_record = mirror_records.get(source_record.uuid) diff = source_record.diff(mirror_record) if diff: # 展示差异 print(f\u0026#34;Request {source_record.uuid} is different:\u0026#34;) print(diff) # 统计对比结果：总共有多少条记录，有多少条记录不同；多少个 method 不同；method 各有多少条记录不同； # 统计 method 不同的原因：请求头不同、请求体不同、响应头不同、响应体不同； print_summary() 这里需要注意的是原始服务器和镜像服务器保存的JSON结构可能存在差异, 比如：\n字段的序列化顺序不同，如：{\u0026quot;a\u0026quot;: 1, \u0026quot;b\u0026quot;: 2} 和 {\u0026quot;b\u0026quot;: 2, \u0026quot;a\u0026quot;: 1} 对于空值的（序列化）处理不同，如：{\u0026quot;a\u0026quot;: 1, b: []} 和 {\u0026quot;a\u0026quot;: 1} 列表类型的顺序不同, 如：[1, 2, 3] 和 [3, 2, 1] 这些差异需要视情况而定，有些场景下列表的顺序差异就代表两者不同，但是在这里列表的顺序差异是可以忽略不计的。\n因此，在对比的时候需要将两个结构体进行泛化，才能准确地对比两个结构体是否相同，下面就用一段 python 代码来实现这个功能：\ndef normalize_json(obj: Any) -\u0026gt; Any: \u0026#34;\u0026#34;\u0026#34;递归地规范化 JSON 对象，使其可以进行顺序无关的比较 Args: obj: 要规范化的对象，可以是字典、列表或其他 JSON 支持的类型 Returns: 规范化后的对象 \u0026#34;\u0026#34;\u0026#34; if isinstance(obj, dict): # 如果字典为空，返回 None if not obj: return None normalized = [(k, normalize_json(v)) for k, v in obj.items()] normalized = [(k, v) for k, v in normalized if v is not None] return sorted(normalized) if normalized else None elif isinstance(obj, list): if not obj: return None normalized = [normalize_json(x) for x in obj] normalized = [x for x in normalized if x is not None] return sorted(normalized) if normalized else None else: return obj def stringify_json(obj: Any) -\u0026gt; Any: \u0026#34;\u0026#34;\u0026#34;将规范化的对象转换回 JSON 对象 Args: obj: 规范化后的对象，可能是元组列表（字典）或排序后的列表 Returns: 转换回原始 JSON 结构的对象，如果输入为 None，返回空字典或空列表 \u0026#34;\u0026#34;\u0026#34; if obj is None: return {} elif isinstance(obj, list): if obj and isinstance(obj[0], tuple): return {k: stringify_json(v) for k, v in obj} return [stringify_json(x) for x in obj] elif isinstance(obj, tuple): return {obj[0]: stringify_json(obj[1])} else: return obj # 测试 json1 = \u0026#39;{\u0026#34;a\u0026#34;: [2, 1], \u0026#34;b\u0026#34;: {\u0026#34;x\u0026#34;: 1, \u0026#34;y\u0026#34;: 2}}\u0026#39; json2 = \u0026#39;{\u0026#34;a\u0026#34;: [1, 2], \u0026#34;b\u0026#34;: {\u0026#34;y\u0026#34;: 2, \u0026#34;x\u0026#34;: 1}, \u0026#34;c\u0026#34;: []}\u0026#39; normalized1 = normalize_json(json.loads(json1)) normalized2 = normalize_json(json.loads(json2)) print(normalized1) # [(\u0026#39;a\u0026#39;, [1, 2]), (\u0026#39;b\u0026#39;, [(\u0026#39;x\u0026#39;, 1), (\u0026#39;y\u0026#39;, 2)])] print(normalized2) # [(\u0026#39;a\u0026#39;, [1, 2]), (\u0026#39;b\u0026#39;, [(\u0026#39;x\u0026#39;, 1), (\u0026#39;y\u0026#39;, 2)])] restored1 = stringify_json(normalized1) restored2 = stringify_json(normalized2) print(json.dumps(restored1)) # {\u0026#34;a\u0026#34;: [1, 2], \u0026#34;b\u0026#34;: {\u0026#34;x\u0026#34;: 1, \u0026#34;y\u0026#34;: 2}} print(json.dumps(restored2)) # {\u0026#34;a\u0026#34;: [1, 2], \u0026#34;b\u0026#34;: {\u0026#34;x\u0026#34;: 1, \u0026#34;y\u0026#34;: 2}} 可以看到原本两个不相同的JSON字符串，json1 和 json2，经过 normalize_json 函数处理后，得到的结果是相同的，因此可以认为这两个JSON字符串是相同的。\nnormalize_json 函数的实现思路是：\n对 key 进行排序，达到让 JSON 归一化的目的。想要排序那么格式一定是需要数组类型，因此这里对所有的 kv 递归处理为（key, value）的元组格式，存放为一个数组，再使用 key 进行排序。 过滤掉其中的空值: 空值对比较没有意义。 总结 # 本文介绍了如何在 Kubernetes 中使用 grpcreplay 实现 gRPC 流量镜像。通过 sidecar 容器的方式，我们可以将 grpcreplay 部署在旧服务和新服务的旁边，从而实现流量的复制和对比。这种方式的好处在于：\n不需要修改应用代码； 相比于代理的方式，不会影响应用的性能； 可以方便的对比新旧服务的响应内容的差异； 如果当前环境中已经有 Istio 这种 service mesh 的话，那么可以考虑使用 Virtual Service 的 mirror 配置来快速实现流量复制，另外再加上一个请求和响应的记录器，就可以实现流量的复制和对比。\n"},{"id":6,"href":"/2025/04/24/kafka-mongodb%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE%E7%BA%AA%E8%A6%81/","title":"Kafka + Mongodb 通信协议纪要","section":"Posts","content":"Kafka 和 MongoDB 是目前使用比较广泛的消息队列和数据库，在之前的很长时间里对这两个软件系统的理解都停留在概念和使用上，直到最近遇到一个“诡异”的问题，已有的经验和调试方法无法定位时，最终尝试了下抓包分析才最终定位到问题的根源。\n问题描述： 使用 sarama 编写了一个 kafka 消费者组，这里不同寻常的地方在于：手动提交 + 批量消费。遇到的问题：某些分区消费进度无法成功提交，但是消息是消费成功的。出现这种情况的分区没有规律，触发 rebalance 后 “故障分区” 有概率会发生变化。\n分析/定位：这里很明显的问题在于手动提交 offset 为什么不成功？从实现来说，提交 offset 的逻辑跟分区没有关系是一致，那这种不确定性故障时从哪儿来的？而且还和 rebalance 相关。 梳理下 kafka 客户端消费提交涉及到的操作：Fetch, OffsetCommit, 但是消费是正常的，那么只需要抓包分析 OffsetCommit 就可以知道 offset 提交存在什么问题。\n结果： 通过抓包一切都明朗了：出现问题的分区同时有多个 OffsetCommit 请求，且其中有的请求提交的 offset 一致停留在一个 “旧的” 位置，不会更新，这样就缩小了范围：程序提交 offset 逻辑异常。\nKAFKA 协议 # Kafka 协议是基于 TCP/IP 协议的二进制协议。其结构组成如下：\nstruct RequestOrResponse { RequestResponseHeader requestResponseHeader; // uint32 messageLength; SpecificRequestOrResponseHeader body; // 格式取决于具体的请求和响应，比如：RequestV1Header } struct RequestV1Header { int16 apiKey; int16 apiVersion; int32 correlationId; string clientId; } 协议结构 # https://kafka.apache.org/protocol.html#protocol_messages\n二进制协议的结构如下：\nRequest V1 Header:\n0 1 2 3 4 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ | message length (total message size) | +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ | request api key | request api version | +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ | correlation_id | +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ | client id length N | | | client_id 占用 N 空间的，这里不确定具体的长度 | +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ | request extend header | | .................................... | | .................................... | | request extend header | | .................................... | +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 字段 描述 message length INT32, 消息长度，包括所有字段，不包括消息长度本身。 request api key INT16 请求的 API 键，用于标识请求的类型。 request api version INT16 请求的 API 版本，用于标识请求的版本。 correlation_id INT32 关联 ID，用于标识请求和响应之间的关系。 client id NULLABLE_STRING, 客户端 ID, 是一个 NULLABLE_STRING NULLABLE_STRING Represents a sequence of characters or null.\nFor non-null strings, first the length N is given as an INT16. Then N bytes follow which are the UTF-8 encoding of the character sequence.\nA null value is encoded with length of -1 and there are no following bytes.\nRequest V2 Header:\n0 1 2 3 4 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ | message length (total message size) | +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ | request api key | request api version | +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ | correlation_id | +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ | client id length N | | | client_id 占用 N 空间的，这里不确定具体的长度 | +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ | _tagged_fields (仅为说明该值传输，占用空间不确定) | +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ | request extend header | | .................................... | | .................................... | | request extend header | | .................................... | +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ Response V0 Header:\n0 1 2 3 4 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ | message length (total message size) | +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ | correlation_id | +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 字段 描述 correlation_id UINT32 关联 ID，用于标识请求和响应之间的关系。 Response V1 Header:\n0 1 2 3 4 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ | message length (total message size) | +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ | correlation_id | +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ | _tagged_fields (仅为说明该值传输，占用空间不确定) | +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 这里 request 和 reponse header 分别有两个版本，且相当于前一个版本，后者增加了一个 _tagged_fields 字段，其格式为：\nhttps://cwiki.apache.org/confluence/display/KAFKA/KIP-482%3A+The+Kafka+Protocol+should+Support+Optional+Tagged+Fields\nThe number of tagged fields Field 1 Tag Field 1 Length Field 1 Data Field 2 Tag Field 2 Length Field 2 Data \u0026hellip; UNSIGNED_VARINT UNSIGNED_VARINT UNSIGNED_VARINT \u0026lt;field 1 type\u0026gt; UNSIGNED_VARINT UNSIGNED_VARINT \u0026lt;field 2 type\u0026gt; \u0026hellip; UNSIGNED_VARINT 是一种变长的无符号整数编码方式，参考 google protobuf varint 和 zig-zag 编码。如果是 tagCount = 0，那么这里只需要使用一个字节表示 0x00。\nAPI 请求和响应举例 # 其中 request 和 response header 格式取决于请求和响应的类型。比如 ApiVersion(18) 请求有 5 个版本，如下：\nAPI Version Based Request Header Version fields 0 1 - 1 1 - 2 1 - 3 2 client_software_name(COMPACT_STRING); client_software_version(COMPACT_STRING); _tagged_fileds 4 2 client_software_name(COMPACT_STRING); client_software_version(COMPACT_STRING); _tagged_fileds 响应有 4 个版本，如下：\nAPI Version Based Response Header Version fields 0 0 {error_code, api_keys[{api_key,min_version,max_version}]} 1 0 {error_code, api_keys[{api_key,min_version,max_version}], throttle_time_ms} 2 0 {error_code, api_keys[{api_key,min_version,max_version}], throttle_time_ms} 3 0 {error_code, api_keys[{api_key,min_version,max_version,_tagged_fields}], throttle_time_ms, _tagged_fields} 更多的请求和响应 header 格式参考：API Keys\n协议抓包分析 # 下面使用 wireshark 抓包并结合前面提到的协议实际分析下 Kafka 协议的交互过程。启动一个 kafka 客户端：\nKafka 抓包：ApiVersions Request Kafka 抓包：ApiVersions Response 从图中能看到由于 wireshark 解析已经足够好了，所以看到的 kafka 协议字段已经可读性非常高了，直接就能和协议文档一一对应起来。唯一需要注意的就是 kafka 协议的编码需要根据文档和字段类型进行解析，比如 NullableString 类型的字段需要先解析长度，然后再解析字符串; VARINT 遵循了变长的 zig-zag 编码，参考 Google Protocol Buffers\nZig-zag 编码解释：https://gist.github.com/mfuerstenau/ba870a29e16536fdbaba\n小结 # 为了支持协议的向后兼容性，Kafka API 提供了版本化语义 和 ApiVersions API 来动态协商客户端和服务器通信协议（版本）。客户端连接到服务器之后需要请求服务端支持的 API 版本范围，从中选一个两边都支持的版本。在发送请求时，客户端携带 请求API 的版本，服务器默认相同版本的响应格式返回，如果客户端携带了不支持的版本，服务器会响应 UNSUPPORTED_VERSION(35) 以提示客户端调整版本。\n协议中定义了 API Keys 枚举，用来说明服务器支持的 API 清单，每个 API 都有唯一的标识ID，客户端和服务器通过这个 ID 对应特定的功能，也用来确定具体的请求和响应格式。\nKafka 协议采用了二进制编码，为此协议中约定了一些基础类型，如：BOOLEAN，INT8, INT16, INT32, INT64, UINT, VARINT, VARLONG, UUID, STRING, COMPACT_STRING 等等。\nMongoDB 协议 # MongoDB 协议是一个简单的基于 socket 的请求响应协议。客户端通过一个常规的 TCP/IP 套接字与数据库服务器进行通信，服务器默认端口为 27017，协议采用了小端字节序。\n标准消息头 # 通常而言，每条消息都是有一个标准消息头和紧跟着的数据构成，标准消息头格式如下：\n0 1 2 3 4 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ | message length (total message size) | +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ | requestID (identifier for this message) | +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ | responseTo (requestID from the origin request) | +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ | opcode (message type) | +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 其中 OpCode 的取值有如下几种：\n注意随着 MongoDB 版本的升级，MongoDB 已经使用 OP_MSG 操作码来统一请求和响应格式。\nOpCode 值 描述 OP_COMPRESSED 2011 使用压缩来来包装其他操作码 OP_MSG 2013 使用标准格式发送消息。用于客户端请求和数据库回复。 OP_REPLY 1 响应消息。自 MongoDB 5.0 弃用，5.1 删除 OP_UPDATE 2001 更新消息。自 MongoDB 5.0 弃用，5.1 删除 OP_INSERT 2002 插入消息。自 MongoDB 5.0 弃用，5.1 删除 RESERVED 2003 以前用于 OP_GET_BY_OID OP_QUERY 2004 查询消息。自 MongoDB 5.0 弃用，5.1 删除 OP_GET_MORE 2005 获取更多消息。自 MongoDB 5.0 弃用，5.1 删除 OP_DELETE 2006 删除消息。自 MongoDB 5.0 弃用，5.1 删除 OP_KILL_CURSORS 2007 杀死游标消息。自 MongoDB 5.0 弃用，5.1 删除 OP_COMPRESSED 消息格式 # 任何操作码都可以使用 OP_COMPRESSED 来包装，OP_COMPRESSED 消息格式如下：\nstruct OP_COMPRESSED { MsgHeader header; // 标准消息头 int32 originalOpcode; // 原始 opcode int32 uncompressedSize; // 未压缩消息大小 uint8 compressorId; // 压缩器（算法）的标识 char *compressedMessage; // 压缩后的消息，不包括标准消息头 } compressorId 的取值有如下几种：\ncompressorId 值 描述 0 noop 不压缩 1 snappy snappy 压缩算法 2 zlib zlib 压缩算法 3 zstd zstd 压缩算法 4-255 保留 保留 OP_MSG 消息格式 # OP_MSG 是一种可以扩展的消息格式，用于客户端请求和服务器响应。其格式如下：\nstruct OP_MSG { MsgHeader header; // 标准消息头 uint32 flagBits; // 标志位 Sections[] sections; // 数据段 optional\u0026lt;uint32\u0026gt; checksum; // 校验和 CRC-32C } FlagBits\nFlagsBits 是一个 32 位的无符号整数，用来说明 OP_MSG 的格式和行为。前 16 位是必要的，并且如果有未知位设置，解析器必须要抛出错误。后16位是可选的，解析器必须忽略任何未知位设置，代理和消息转发器必须清除任何未知位设置。\nbit位 字段 请求 响应 描述 0 checksumPresent ✅ ✅ 表示消息是否包含校验和 1 moreToCome ✅ ✅ 表示是否有更多数据段。另一条消息将紧随此消息之后，无需接收方采取进一步行动。接收方在收到 moreToCome 设置为 0 的消息之前， 不得发送其他消息，否则发送可能会阻塞，从而导致死锁。带有 moreToCome 的请求 位设置不会收到回复。回复只会包含此 响应设置了 exhaustAllowed 位的请求而设置。 16 exhaustAllowed ✅ - 客户端已准备好使用 moreToCome 位对此请求进行多次回复。除非请求中设置了此位，否则服务器永远不会生成设置了 moreToCome 位的回复。 Sections\nOP_MSG 消息包含一个或者多个部分（section），每个部分都以一个kind 字节开头，kind 用来指示类型。kind 之后的字节构成该部分的载荷。\nkind 描述 0 Body, body section 被编码为单个 BSON 对象；BSON 对象的字节大小在 section 开始位置（64位） 1 Document Sequence, 支持传输多个文档，优化批量操作。 其基本结构是 2 仅内部使用 Body 类型常用于标准的请求和响应体。\nCheckSum\n每条消息都可以附带 CRC-32C 校验和，用于验证消息的完整性（除 checksum 自身）。\n协议抓包分析 # 下面用 wireshark 抓包并结合前面提到的协议实际分析下 MongoDB 协议的交互过程。使用 mongosh 连接到 MongoDB 数据库，执行一条查询所有db的命令：\nUsing MongoDB:\t7.0.2 Using Mongosh:\t2.3.2\n$ mongosh \u0026gt; show databases 抓包结果如下：\nMongoDB 抓包：Request MongoDB 抓包：Response wireshark 可以清晰的看到 show databases 命令的请求和响应体。都包含了标准消息头（message length, requestID, responseTo, opcode）。响应中的 reponseTo 值等于对应请求的 requestID，而请求中的 reponseTo 值为 0，这也是区分请求和响应的方法。\n在 OP_MSG（opcode = 2013, 也就是 Extensible Message Format） 消息体中包含了 flagBits, sections 等字段，而 checksum 由于 flagBits 中没有设置，所以没有包含。\n掌握了这些基本概念后，我们可以简单的手搓一个脚本用来解析 MongoDB 协议（ wireshark 毕竟看起来不那么直观）。核心解析代码如下：\ndef parse_op_msg(self, payload): \u0026#34;\u0026#34;\u0026#34;解析 OP_MSG 消息格式 OP_MSG 格式 (MongoDB 3.6+): struct { MsgHeader header; // 标准消息头, 16 字节 uint32 flagBits; // 标志位 // 接下来是一个或多个部分 (sections) // 每个部分以一个字节的类型标识开始 // 类型 0: 正文部分 (body section) // 类型 1: 文档序列部分 (document sequence) } \u0026#34;\u0026#34;\u0026#34; if len(payload) \u0026lt; 20: # 头部 16 字节 + 标志位 4 字节 return None try: # 解析标志位 flag_bits = struct.unpack(\u0026#34;\u0026lt;I\u0026#34;, payload[16:20])[0] flags = [] for bit, name in OP_MSG_FLAGS.items(): if flag_bits \u0026amp; (1 \u0026lt;\u0026lt; bit): flags.append(name) result = { \u0026#34;flags\u0026#34;: flags, \u0026#34;sections\u0026#34;: [] } # 解析部分 (sections) offset = 20 # 跳过头部和标志位 # 读取部分类型 kind = payload[offset] offset += 1 if kind == 0: # Body kind section = { \u0026#34;kind\u0026#34;: \u0026#34;body\u0026#34;, \u0026#34;document\u0026#34;: None, \u0026#34;error\u0026#34;: None } doc = parse_body_kind(payload[offset:]) if doc: section[\u0026#34;document\u0026#34;] = doc result[\u0026#34;sections\u0026#34;].append(section) offset = len(payload) else: # documentSequence kind 和 其他，这里暂不考虑 section = { \u0026#34;kind\u0026#34;: f\u0026#34;unknown({kind})\u0026#34;, \u0026#34;error\u0026#34;: \u0026#34;未知的部分类型\u0026#34; } result[\u0026#34;sections\u0026#34;].append(section) return result except Exception as e: raise Exception(f\u0026#34;解析 OP_MSG 消息时出错: {e}\u0026#34;) return None def parse_body_kind(payload) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34; 解析 OP_MSG 消息体中的 body 类的 section: 8byte 的 sectionMessageLength 剩余都是 body 的 section \u0026#34;\u0026#34;\u0026#34; if len(payload) \u0026lt; 4: return None try: # 这里通过 bson.decode_document 整个 payload 就是 bson 序列化后的结构。 # length (4 bytes) + payload (length-4, bytes) doc = bson.decode_document(payload, 0) return doc except Exception as e: print(f\u0026#34;解析 OP_MSG 消息体中的 body 类的 section 时出错: {e}\u0026#34;) return None 展示结果如下：\nshow databases db.coll.find().limit(10) 小结 # MongoDB 协议采用了小端字节序，并且使用了标准的 TCP/IP 套接字进行通信；客户端和服务器通过一个常规的 TCP/IP 套接字进行通信，服务器默认端口为 27017。\n随着 MongoDB 版本的升级，MongoDB 已经使用 OP_MSG 操作码来统一请求和响应格式。在 OP_MSG 中 Section 采用 BSON 编码，这样非常灵活，如果 API 功能进行了调整（新增/调整了字段，那么是可以不需要调整协议，而只用调整功能即可）。\n要进一步深入的话可以再实现一个简单的 MongoDB 客户端，或者结合某个特定的客户端实现代码来深入了解 mongodb 协议的交互过程。当然这里还没有深究 show databases 这种命令的具体内容，有兴趣可以继续研究下 MongoDB specification。\n总结 # 任何 C/S 模式的软件系统都可以自定义自己的 “协议”，而这些协议的模式大同小异，简单的可以直接使用文本协议实现如 memcached, 复杂的则可以完全自定义自己的二进制协议，如 kafka 和 mongodb, 更进一步它们甚至有自己独一套的编码协议，但是不管怎么样一套协议也无非就是：确定传输协议 + 设计应用协议 + 采用什么编码。\n掌握这些协议的设计思路和理念之后，下一步就可以去深入了解软件整体的 API 功能设计，这样会帮助我们更深入服务侧的逻辑实现，加深对软件系统的理解。\n参考文档 # Kafka Protocol Guide MongoDB Wire Protocol MongoDB specification OP_MSG BSON serialization specification "},{"id":7,"href":"/2025/02/28/%E5%9C%A8k8s%E4%B8%AD%E6%90%AD%E5%BB%BA%E5%AD%98%E7%AE%97%E5%88%86%E7%A6%BB%E7%9A%84Doris%E9%9B%86%E7%BE%A4/","title":"在 K8S 中部署存算分离 Doris 集群","section":"Posts","content":" 本文记录了在 ubuntu 22.04 上配合 minikube 搭建的 k8s 集群，搭建 doris 存算分离集群的过程。\n0. 环境信息 # 软件 版本 OS Ubuntu 24.04.1 LTS x86_64 kernel 6.8.0-52-generic minikube v1.35.0 kubernetes v1.32.0 doris v3.0.3 这里默认已经准备好了基础的 kubernetes 集群，所以也不再阐述如何通过 minikube 或者其他方式搭建 kubernetes 集群。\n1. 安装 FoundationDB # 参考文档：https://doris.apache.org/zh-CN/docs/3.0/install/deploy-on-kubernetes/separating-storage-compute/install-fdb\n1.1 安装 FoundationDB CRD 资源 # kubectl apply -f https://raw.githubusercontent.com/FoundationDB/fdb-kubernetes-operator/main/config/crd/bases/apps.foundationdb.org_foundationdbclusters.yaml kubectl apply -f https://raw.githubusercontent.com/FoundationDB/fdb-kubernetes-operator/main/config/crd/bases/apps.foundationdb.org_foundationdbbackups.yaml kubectl apply -f https://raw.githubusercontent.com/FoundationDB/fdb-kubernetes-operator/main/config/crd/bases/apps.foundationdb.org_foundationdbrestores.yaml 1.2 部署 FoundationDB Operator # wget https://raw.githubusercontent.com/apache/doris-operator/master/config/operator/fdb-operator.yaml kubectl apply -f fdb-operator.yaml 1.3 部署 FoundationDB 集群 # wget https://raw.githubusercontent.com/foundationdb/fdb-kubernetes-operator/main/config/samples/cluster.yaml -O fdb-cluster.yaml kubectl apply -f fdb-cluster.yaml # 查看集群状态 kubectl get fdb # 预期输出（启动需要时间，需要等待几分钟） NAME GENERATION RECONCILED AVAILABLE FULLREPLICATION VERSION AGE test-cluster 1 1 true true 7.1.26 3m30s 2. 安装 Doris Operator # 2.1 安装 CRD 部署 Doris 相关资源定义 # kubectl create -f https://raw.githubusercontent.com/apache/doris-operator/master/config/crd/bases/crds.yaml 2.2 部署 Doris Operator # wget https://raw.githubusercontent.com/apache/doris-operator/master/config/operator/disaggregated-operator.yaml -O disaggregated-operator.yaml kubectl apply -f disaggregated-operator.yaml # 查看部署状态 kubectl get pod -n doris # 预期输出 NAME READY STATUS RESTARTS AGE doris-operator-5fd65d8d69-rgqlk 1/1 Running 0 79s 3. 部署存算分离集群 # 3.1 下载示例配置 # wget https://raw.githubusercontent.com/apache/doris-operator/master/doc/examples/disaggregated/cluster/ddc-sample.yaml -O ddc-sample.yaml 3.2 配置 ConfigMap # 对于 ddc-sample.yaml 配置进行调整配置。这三个都需要分别配置 ConfigMap 并修改集群中的配置挂载。\n配置元数据 https://doris.apache.org/zh-CN/docs/3.0/install/deploy-on-kubernetes/separating-storage-compute/config-ms 配置 FE 集群 https://doris.apache.org/zh-CN/docs/3.0/install/deploy-on-kubernetes/separating-storage-compute/config-fe 配置计算资源组 https://doris.apache.org/zh-CN/docs/3.0/install/deploy-on-kubernetes/separating-storage-compute/config-cg ConfigMap 配置 apiVersion: v1 data: doris_cloud.conf: | # // meta_service brpc_listen_port = 5000 brpc_num_threads = -1 brpc_idle_timeout_sec = 30 http_token = greedisgood9999 # // doris txn config label_keep_max_second = 259200 expired_txn_scan_key_nums = 1000 # // logging log_dir = ./log/ # info warn error log_level = info log_size_mb = 1024 log_filenum_quota = 10 log_immediate_flush = false log_verbose_modules = * # //max stage num max_num_stages = 40 kind: ConfigMap metadata: name: doris-metaservice namespace: default --- apiVersion: v1 kind: ConfigMap metadata: name: fe-configmap namespace: default labels: app.kubernetes.io/component: fe data: fe.conf: | CUR_DATE=`date +%Y%m%d-%H%M%S` # Log dir LOG_DIR = ${DORIS_HOME}/log # For jdk 17, this JAVA_OPTS will be used as default JVM options JAVA_OPTS_FOR_JDK_17=\u0026#34;-Djavax.security.auth.useSubjectCredsOnly=false -Xmx8192m -Xms8192m -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=$LOG_DIR -Xlog:gc*:$LOG_DIR/fe.gc.log.$CUR_DATE:time,uptime:filecount=10,filesize=50M --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens java.base/jdk.internal.ref=ALL-UNNAMED\u0026#34; # INFO, WARN, ERROR, FATAL sys_log_level = INFO # NORMAL, BRIEF, ASYNC sys_log_mode = NORMAL # Default dirs to put jdbc drivers,default value is ${DORIS_HOME}/jdbc_drivers # jdbc_drivers_dir = ${DORIS_HOME}/jdbc_drivers http_port = 8030 rpc_port = 9020 query_port = 9030 edit_log_port = 9010 enable_fqdn_mode=true --- apiVersion: v1 kind: ConfigMap metadata: name: be-configmap labels: app.kubernetes.io/component: be data: be.conf: | # For jdk 17, this JAVA_OPTS will be used as default JVM options JAVA_OPTS_FOR_JDK_17=\u0026#34;-Xmx1024m -DlogPath=$LOG_DIR/jni.log -Xlog:gc*:$LOG_DIR/be.gc.log.$CUR_DATE:time,uptime:filecount=10,filesize=50M -Djavax.security.auth.useSubjectCredsOnly=false -Dsun.security.krb5.debug=true -Dsun.java.command=DorisBE -XX:-CriticalJNINatives -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=java.management/sun.management=ALL-UNNAMED\u0026#34; file_cache_path = [{\u0026#34;path\u0026#34;:\u0026#34;/mnt/disk1/doris_cloud/file_cache\u0026#34;,\u0026#34;total_size\u0026#34;:107374182400,\u0026#34;query_limit\u0026#34;:107374182400}] deploy_mode = cloud 3.3 启动集群 # # 部署集群 kubectl apply -f ddc-sample.yaml # 查看集群状态 kubectl get ddc # 预期输出 NAME CLUSTERHEALTH MSPHASE FEPHASE CGCOUNT CGAVAILABLECOUNT CGFULLAVAILABLECOUNT test-disaggregated-cluster green Ready Ready 2 2 2 3.4 创建远程存储后端 # kubectl get svc # test-disaggregated-cluster-fe ClusterIP 10.104.79.145 \u0026lt;none\u0026gt; 8030/TCP,9020/TCP,9030/TCP,9010/TCP 35m # MySQL 客户端启动 kubectl run mysql-client --image=mysql:5.7 -it --rm --restart=Never -- /bin/bash # 连接 Doris FE mysql -uroot -P9030 -h test-disaggregated-cluster-fe # MYSQL 命令执行：S3 Storage Vault CREATE STORAGE VAULT IF NOT EXISTS s3_vault PROPERTIES ( \u0026#34;type\u0026#34;=\u0026#34;S3\u0026#34;, \u0026#34;s3.endpoint\u0026#34; = \u0026#34;oss-cn-beijing.aliyuncs.com\u0026#34;, \u0026#34;s3.region\u0026#34; = \u0026#34;bj\u0026#34;, \u0026#34;s3.bucket\u0026#34; = \u0026#34;bucket\u0026#34;, \u0026#34;s3.root.path\u0026#34; = \u0026#34;big/data/prefix\u0026#34;, \u0026#34;s3.access_key\u0026#34; = \u0026#34;your-ak\u0026#34;, \u0026#34;s3.secret_key\u0026#34; = \u0026#34;your-sk\u0026#34;, \u0026#34;provider\u0026#34; = \u0026#34;OSS\u0026#34; ); # MYSQL 命令执行：设置默认数据后端 SET s3_vault AS DEFAULT STORAGE VAULT; 4. 测试集群 # 4.1 启动 MySQL 客户端 # # 创建一个临时的 MySQL 客户端 Pod kubectl run mysql-client --image=mysql:5.7 -it --rm --restart=Never -- mysql -h test-disaggregated-cluster-fe -P9030 -uroot 4.2 测试基本操作 # 执行一些查询语句和初始化测试表数据，验证集群是否正常工作。\n-- 查看 FE 节点状态 SHOW FRONTENDS; -- 查看 BE 节点状态 SHOW BACKENDS; -- 创建测试数据库 CREATE DATABASE test_db; USE test_db; -- 创建测试表 CREATE TABLE test_tbl ( id INT, name VARCHAR(50), score DECIMAL(10,2) ) UNIQUE KEY(id) DISTRIBUTED BY HASH(id) BUCKETS 3; -- 插入测试数据 INSERT INTO test_tbl VALUES (1, \u0026#39;Tom\u0026#39;, 89.5), (2, \u0026#39;Jerry\u0026#39;, 92.0), (3, \u0026#39;Jack\u0026#39;, 85.5); -- 查询数据 SELECT * FROM test_tbl; 4.3 验证存算分离 # -- 查看存储后端状态 SHOW STORAGE VAULTS; -- 查看表的存储位置 SHOW CREATE TABLE test_db.test_tbl; 5. 部署期间遇到的问题和解决办法 # 5.1 Coupute Group (BE) 无法启动 # 问题现象：\nBe 启动时输出如下日志：\nDefaulted container \u0026#34;compute\u0026#34; out of: compute, default-init (init) [Fri Feb 28 02:56:24 UTC 2025] [info] Process conf file be.conf ... /opt/apache-doris/be_disaggregated_entrypoint.sh: line 73: /opt/apache-doris/be/conf/: Is a directory [Fri Feb 28 02:56:24 UTC 2025] [info] use root no password show backends result 10221\ttest-disaggregated-cluster-cg1-1.test-disaggregated-cluster-cg1.default.svc.cluster.local\t9050\t-1\t-1\t-1\t-1\tNULL\tNULL\tfalse\tfalse\t0\t0.000 0.000 1.000 B\t0.000 0.00 %\t0.00 %\t0.000 {\u0026#34;cloud_unique_id\u0026#34; : \u0026#34;1:1751150972:t1Ws6Mrv\u0026#34;, \u0026#34;compute_group_status\u0026#34; : \u0026#34;NORMAL\u0026#34;, \u0026#34;private_endpoint\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;compute_group_name\u0026#34; : \u0026#34;cg1\u0026#34;, \u0026#34;location\u0026#34; : \u0026#34;default\u0026#34;, \u0026#34;public_endpoint\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;compute_group_id\u0026#34; : \u0026#34;ZNES_zRC\u0026#34;}\tjava.net.UnknownHostException: test-disaggregated-cluster-cg1-1.test-disaggregated-cluster-cg1.default.svc.cluster.local\t{\u0026#34;lastStreamLoadTime\u0026#34;:-1,\u0026#34;isQueryDisabled\u0026#34;:false,\u0026#34;isLoadDisabled\u0026#34;:false,\u0026#34;isActive\u0026#34;:true,\u0026#34;currentFragmentNum\u0026#34;:0,\u0026#34;lastFragmentUpdateTime\u0026#34;:0}\t287\t1\t0.00 10222\ttest-disaggregated-cluster-cg1-0.test-disaggregated-cluster-cg1.default.svc.cluster.local\t9050\t-1\t-1\t-1\t-1\tNULL\tNULL\tfalse\tfalse\t0\t0.000 0.000 1.000 B\t0.000 0.00 %\t0.00 %\t0.000 {\u0026#34;cloud_unique_id\u0026#34; : \u0026#34;1:1751150972:YA6LXvvg\u0026#34;, \u0026#34;compute_group_status\u0026#34; : \u0026#34;NORMAL\u0026#34;, \u0026#34;private_endpoint\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;compute_group_name\u0026#34; : \u0026#34;cg1\u0026#34;, \u0026#34;location\u0026#34; : \u0026#34;default\u0026#34;, \u0026#34;public_endpoint\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;compute_group_id\u0026#34; : \u0026#34;ZNES_zRC\u0026#34;}\tjava.net.UnknownHostException: test-disaggregated-cluster-cg1-0.test-disaggregated-cluster-cg1.default.svc.cluster.local\t{\u0026#34;lastStreamLoadTime\u0026#34;:-1,\u0026#34;isQueryDisabled\u0026#34;:false,\u0026#34;isLoadDisabled\u0026#34;:false,\u0026#34;isActive\u0026#34;:true,\u0026#34;currentFragmentNum\u0026#34;:0,\u0026#34;lastFragmentUpdateTime\u0026#34;:0}\t287\t1\t0.00 10223\ttest-disaggregated-cluster-cg1-2.test-disaggregated-cluster-cg1.default.svc.cluster.local\t9050\t-1\t-1\t-1\t-1\tNULL\tNULL\tfalse\tfalse\t0\t0.000 0.000 1.000 B\t0.000 0.00 %\t0.00 %\t0.000 {\u0026#34;cloud_unique_id\u0026#34; : \u0026#34;1:1751150972:Ox_RJuee\u0026#34;, \u0026#34;compute_group_status\u0026#34; : \u0026#34;NORMAL\u0026#34;, \u0026#34;private_endpoint\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;compute_group_name\u0026#34; : \u0026#34;cg1\u0026#34;, \u0026#34;location\u0026#34; : \u0026#34;default\u0026#34;, \u0026#34;public_endpoint\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;compute_group_id\u0026#34; : \u0026#34;ZNES_zRC\u0026#34;}\tjava.net.UnknownHostException: test-disaggregated-cluster-cg1-2.test-disaggregated-cluster-cg1.default.svc.cluster.local\t{\u0026#34;lastStreamLoadTime\u0026#34;:-1,\u0026#34;isQueryDisabled\u0026#34;:false,\u0026#34;isLoadDisabled\u0026#34;:false,\u0026#34;isActive\u0026#34;:true,\u0026#34;currentFragmentNum\u0026#34;:0,\u0026#34;lastFragmentUpdateTime\u0026#34;:0}\t287\t1\t0.00 10224\ttest-disaggregated-cluster-cg2-0.test-disaggregated-cluster-cg2.default.svc.cluster.local\t9050\t-1\t-1\t-1\t-1\tNULL\tNULL\tfalse\tfalse\t0\t0.000 0.000 1.000 B\t0.000 0.00 %\t0.00 %\t0.000 {\u0026#34;cloud_unique_id\u0026#34; : \u0026#34;1:1751150972:E_SJoMU8\u0026#34;, \u0026#34;compute_group_status\u0026#34; : \u0026#34;NORMAL\u0026#34;, \u0026#34;private_endpoint\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;compute_group_name\u0026#34; : \u0026#34;cg2\u0026#34;, \u0026#34;location\u0026#34; : \u0026#34;default\u0026#34;, \u0026#34;public_endpoint\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;compute_group_id\u0026#34; : \u0026#34;oZ2gH5Ml\u0026#34;}\tjava.net.UnknownHostException: test-disaggregated-cluster-cg2-0.test-disaggregated-cluster-cg2.default.svc.cluster.local\t{\u0026#34;lastStreamLoadTime\u0026#34;:-1,\u0026#34;isQueryDisabled\u0026#34;:false,\u0026#34;isLoadDisabled\u0026#34;:false,\u0026#34;isActive\u0026#34;:true,\u0026#34;currentFragmentNum\u0026#34;:0,\u0026#34;lastFragmentUpdateTime\u0026#34;:0}\t287\t1\t0.00 10251\ttest-disaggregated-cluster-cg2-1.test-disaggregated-cluster-cg2.default.svc.cluster.local\t9050\t-1\t-1\t-1\t-1\tNULL\tNULL\tfalse\tfalse\t0\t0.000 0.000 1.000 B\t0.000 0.00 %\t0.00 %\t0.000 {\u0026#34;cloud_unique_id\u0026#34; : \u0026#34;1:1751150972:B_h0m9vp\u0026#34;, \u0026#34;compute_group_status\u0026#34; : \u0026#34;NORMAL\u0026#34;, \u0026#34;private_endpoint\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;compute_group_name\u0026#34; : \u0026#34;cg2\u0026#34;, \u0026#34;location\u0026#34; : \u0026#34;default\u0026#34;, \u0026#34;public_endpoint\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;compute_group_id\u0026#34; : \u0026#34;oZ2gH5Ml\u0026#34;}\tjava.net.UnknownHostException: test-disaggregated-cluster-cg2-1.test-disaggregated-cluster-cg2.default.svc.cluster.local\t{\u0026#34;lastStreamLoadTime\u0026#34;:-1,\u0026#34;isQueryDisabled\u0026#34;:false,\u0026#34;isLoadDisabled\u0026#34;:false,\u0026#34;isActive\u0026#34;:true,\u0026#34;currentFragmentNum\u0026#34;:0,\u0026#34;lastFragmentUpdateTime\u0026#34;:0}\t287\t1\t0.00 10252\ttest-disaggregated-cluster-cg2-2.test-disaggregated-cluster-cg2.default.svc.cluster.local\t9050\t-1\t-1\t-1\t-1\tNULL\tNULL\tfalse\tfalse\t0\t0.000 0.000 1.000 B\t0.000 0.00 %\t0.00 %\t0.000 {\u0026#34;cloud_unique_id\u0026#34; : \u0026#34;1:1751150972:nmt5aHJC\u0026#34;, \u0026#34;compute_group_status\u0026#34; : \u0026#34;NORMAL\u0026#34;, \u0026#34;private_endpoint\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;compute_group_name\u0026#34; : \u0026#34;cg2\u0026#34;, \u0026#34;location\u0026#34; : \u0026#34;default\u0026#34;, \u0026#34;public_endpoint\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;compute_group_id\u0026#34; : \u0026#34;oZ2gH5Ml\u0026#34;}\tjava.net.UnknownHostException: test-disaggregated-cluster-cg2-2.test-disaggregated-cluster-cg2.default.svc.cluster.local\t{\u0026#34;lastStreamLoadTime\u0026#34;:-1,\u0026#34;isQueryDisabled\u0026#34;:false,\u0026#34;isLoadDisabled\u0026#34;:false,\u0026#34;isActive\u0026#34;:true,\u0026#34;currentFragmentNum\u0026#34;:0,\u0026#34;lastFragmentUpdateTime\u0026#34;:0}\t287\t1\t0.00 . [Fri Feb 28 02:56:24 UTC 2025] [info] Check myself (test-disaggregated-cluster-cg1-0.test-disaggregated-cluster-cg1.default.svc.cluster.local:9050) exist in FE, start be directly ... /etc/podinfo/annotationsis not exists. [Fri Feb 28 02:56:24 UTC 2025] run start_be.sh Disable swap memory before starting be 问题分析：\n问题详情参见： https://github.com/apache/doris/issues/48460 。一开始关注了 /opt/apache-doris/be_disaggregated_entrypoint.sh: line 73: /opt/apache-doris/be/conf/: Is a directory 以为是脚本 be_disaggregated_entrypoint.sh 有问题, 经过自己修改重新打包后发现问题依旧。重新检查日志和脚本发现在 start_be.sh 中, 检查了交换内存的命令，如果发现交换内存启用，就会退出。\n# start_be.sh line 193 if [[ \u0026#34;$(swapon -s | wc -l)\u0026#34; -gt 1 ]]; then echo \u0026#34;Disable swap memory before starting be\u0026#34; exit 1 fi 解决办法：\n通过在宿主机上执行 sudo swapoff -a 关闭交换内存后，Be 就可以正常启动了。\n5.2 FE 或者 MS 无法启动 # 问题现象：\nRuntimeLogger 2025-02-28 06:38:51,460 WARN (main|1) [CloudEnv.getLocalTypeFromMetaService():165] failed to get cloud cluster due to incomplete response, cloud_unique_id=1:1751150972:fe, clusterId=RESERVED_CLUSTER_ID_FOR_SQL_SERVER, response=status { code: INVALID_ARGUMENT msg: \u0026#34;empty instance_id\u0026#34; } 问题分析：\n这个问题没有找到原因，在 issue 中有相同问题：https://github.com/apache/doris/issues/47678\n解决办法：\n通过重新搭建 fdb + doris 集群解决。\n5.3 数据写入失败 # doris 集群和 Storage Vault 都部署成功后，通过 mysql 客户端连接到 FE 创建 test_db, test_tbl 并插入数据。插入数据时卡顿，且最终失败：\n\u0026gt; INSERT INTO test_tbl VALUES (1, \u0026#39;Tom\u0026#39;, 89.5), (2, \u0026#39;Jerry\u0026#39;, 92.0), (3, \u0026#39;Jack\u0026#39;, 85.5); \u0026gt; ERROR 1105 (HY000): errCode = 2, detailMessage = Backend Backend [id=10262, host=test-disaggregated-cluster-cg1-2.test-disaggregated-cluster-cg1.default.svc.cluster.local, heartbeatPort=9050, alive=false, lastStartTime=2025-02-28 08:46:45, process epoch=1740732405712, isDecommissioned=false, tags: {cloud_cluster_id=Ppt5BJ3g, cloud_unique_id=1:1751150972:JxbgZEBP, cloud_cluster_status=NORMAL, cloud_cluster_name=cg1, location=default, cloud_cluster_private_endpoint=, cloud_cluster_public_endpoint=}], backendStatus: [lastSuccess 与此同时 cg1 整体都触发了重启，查看日志发现：\nRuntimeLogger F20250228 09:32:36.952682 878 storage_engine.cpp:120] Check failed: _type == Type::CLOUD ( vs. ) *** Check failure stack trace: *** RuntimeLogger I20250228 09:32:36.955466 1376 workload_group_manager.cpp:200] Process Memory Summary: process memory used 856.95 MB(= 935.89 MB[vm/rss] - 78.94 MB[tc/jemalloc_cache] + 0[reserved] + 0B[waiting_refresh]), sys available memory 37.77 GB(= 37.77 GB[proc/available] - 0[reserved] - 0B[waiting_refresh]), all workload groups memory usage: 384.00 B, weighted_memory_limit_ratio: 0.9851607626358986 @ 0x5f7414439556 google::LogMessage::SendToLog() @ 0x5f7414435fa0 google::LogMessage::Flush() @ 0x5f7414439d99 google::LogMessageFatal::~LogMessageFatal() @ 0x5f7409adbc3a doris::BaseStorageEngine::to_cloud() @ 0x5f7409d600a7 doris::LoadChannel::open() @ 0x5f7409d5ab0a doris::LoadChannelMgr::open() @ 0x5f7409eaa68d std::_Function_handler\u0026lt;\u0026gt;::_M_invoke() @ 0x5f7409ec4ebb doris::WorkThreadPool\u0026lt;\u0026gt;::work_thread() @ 0x5f74173d1b20 execute_native_thread_routine @ 0x7ce578024ac3 (unknown) @ 0x7ce5780b6850 (unknown) @ (nil) (unknown) *** Query id: 66dcfbe890a240d8-b45c17c9fa19c124 *** *** is nereids: 0 *** *** tablet id: 0 *** *** Aborted at 1740735157 (unix time) try \u0026#34;date -d @1740735157\u0026#34; if you are using GNU date *** *** Current BE git commitID: 62a58bff4c *** *** SIGABRT unknown detail explain (@0x86) received by PID 134 (TID 878 OR 0x7ce39d600640) from PID 134; stack trace: *** RuntimeLogger I20250228 09:32:37.674818 463 wal_manager.cpp:485] Scheduled(every 10s) WAL info: [/opt/apache-doris/be/storage/wal: limit 3993782272 Bytes, used 0 Bytes, estimated wal bytes 0 Bytes, available 3993782272 Bytes.]; RuntimeLogger I20250228 09:32:37.726341 1376 daemon.cpp:239] os physical memory 62.66 GB. process memory used 1.12 GB(= 1.20 GB[vm/rss] - 79.15 MB[tc/jemalloc_cache] + 0[reserved] + 0B[waiting_refresh]), limit 56.40 GB, soft limit 50.76 GB. sys available memory 37.55 GB(= 37.55 GB[proc/available] - 0[reserved] - 0B[waiting_refresh]), low water mark 3.13 GB, warning water mark 6.27 GB. 0# doris::signal::(anonymous namespace)::FailureSignalHandler(int, siginfo_t*, void*) at /home/zcp/repo_center/doris_release/doris/be/src/common/signal_handler.h:421 1# 0x00007CE577FD2520 in /lib/x86_64-linux-gnu/libc.so.6 2# pthread_kill in /lib/x86_64-linux-gnu/libc.so.6 3# raise in /lib/x86_64-linux-gnu/libc.so.6 4# abort in /lib/x86_64-linux-gnu/libc.so.6 5# 0x00005F7414443E2D in /opt/apache-doris/be/lib/doris_be 6# 0x00005F741443646A in /opt/apache-doris/be/lib/doris_be 7# google::LogMessage::SendToLog() in /opt/apache-doris/be/lib/doris_be 8# google::LogMessage::Flush() in /opt/apache-doris/be/lib/doris_be 9# google::LogMessageFatal::~LogMessageFatal() in /opt/apache-doris/be/lib/doris_be 10# doris::BaseStorageEngine::to_cloud() in /opt/apache-doris/be/lib/doris_be 11# doris::LoadChannel::open(doris::PTabletWriterOpenRequest const\u0026amp;) at /home/zcp/repo_center/doris_release/doris/be/src/runtime/load_channel.cpp:131 12# doris::LoadChannelMgr::open(doris::PTabletWriterOpenRequest const\u0026amp;) at /home/zcp/repo_center/doris_release/doris/be/src/runtime/load_channel_mgr.cpp:108 13# std::_Function_handler\u0026lt;void (), doris::PInternalService::tablet_writer_open(google::protobuf::RpcController*, doris::PTabletWriterOpenRequest const*, doris::PTabletWriterOpenResult*, google::protobuf::Closure*)::$_0\u0026gt;::_M_invoke(std::_Any_data const\u0026amp;) at /var/local/ldb-toolchain/bin/../lib/gcc/x86_64-linux-gnu/11/../../../../include/c++/11/bits/std_function.h:291 14# doris::WorkThreadPool\u0026lt;false\u0026gt;::work_thread(int) at /home/zcp/repo_center/doris_release/doris/be/src/util/work_thread_pool.hpp:159 15# execute_native_thread_routine at ../../../../../libstdc++-v3/src/c++11/thread.cc:84 16# 0x00007CE578024AC3 in /lib/x86_64-linux-gnu/libc.so.6 17# 0x00007CE5780B6850 in /lib/x86_64-linux-gnu/libc.so.6 /opt/apache-doris/be/bin/start_be.sh: line 433: 134 Aborted (core dumped) ${LIMIT:+${LIMIT}} \u0026#34;${DORIS_HOME}/lib/doris_be\u0026#34; \u0026#34;$@\u0026#34; 2\u0026gt;\u0026amp;1 \u0026lt; /dev/null 问题分析：\n根据堆栈找到 storage_engine.cpp 中的 120 行：\nStorageEngine\u0026amp; BaseStorageEngine::to_local() { CHECK_EQ(_type, Type::LOCAL); return *static_cast\u0026lt;StorageEngine*\u0026gt;(this); } CloudStorageEngine\u0026amp; BaseStorageEngine::to_cloud() { CHECK_EQ(_type, Type::CLOUD); return *static_cast\u0026lt;CloudStorageEngine*\u0026gt;(this); } // CHECK_EQ 是 glog 的一个宏，用于检查表达式是否为真。如果表达式为假，则会打印错误消息并终止程序。 // https://github.com/google/glog/blob/master/src/glog/logging.h#L688 从这里可以得知 CHECK_EQ(_type, Type::CLOUD); 这个检查失败，导致 BE 重启。\n解决办法：\n将 BE 的存储引擎模式设置为 cloud。参见 https://doris.apache.org/zh-CN/docs/3.0/compute-storage-decoupled/compilation-and-deployment#541-%E9%85%8D%E7%BD%AE-beconf\n"},{"id":8,"href":"/2025/01/03/%E9%80%9A%E8%BF%87%E4%B8%80%E6%AC%A1%E6%8A%93%E5%8C%85%E6%9D%A5%E6%8E%8C%E6%8F%A1memcached/","title":"通过一次抓包来掌握memcached","section":"Posts","content":" 什么是 memcached # memcached 是一个高性能的“分布式”内存对象缓存系统，用于动态 Web 应用以减轻数据库负载。它通过在内存中缓存数据和对象来减少读取数据库的次数，从而提高动态、数据库驱动网站的速度。memcached 是自由软件，以 BSD 许可证发布。\n相比于大家熟知的 Redis，memcached 更加简单，只支持 key-value 存储，而 Redis 支持更多的数据结构，如 list、set、hash 等。\nGithub 地址：https://github.com/memcached/memcached\n为什么有 redis 还要使用 memcached # 从我个人的角度来说，要在采用一个缓存系统的时候，我会优先选择 Redis，因为 Redis 功能更加强大，支持更多的数据结构，而且 Redis 也支持持久化，在高可用和分布式部分的设计上也更加完善。\n但是 memcached 也有自己的优势，比如更加简单，更加轻量级，更加容易上手，因此在某些系统中也会选用 memcached。因此，了解 memcached 的设计也是有必要的。\nmemcached 协议概览 # memcached 支持基本的文本协议和元文本协议，其中元文本协议于 2019 年推出。memcached 还曾支持过二进制协议，但已经被废弃。\nmemcached 的协议是基于文本的，因此我们可以通过 telnet 或者 netcat 工具来模拟 memcached 的客户端，从而方便的进行测试。\n这两者是 “交叉兼容” 的，也就是说我们可以通过 文本协议来设置键值, 通过 元文本协议来查询，反之亦然。\nStandard Text Protocol # 详细的协议文档可以参考：https://github.com/memcached/memcached/blob/master/doc/protocol.txt\nmemcached 的标准文本协议是一个基于文本的协议，它使用 ASCII 字符串来进行通信。memcached 服务器监听在默认端口 11211 上，客户端通过 TCP 连接到服务器，然后发送命令和数据。因此我们可以很容易的通过 telnet 工具就可以完成 memcached 的基本操作。\ntelnet 127.0.0.1 11211 然后我们可以输入 memcached 的命令，如 set、get 等。\nset key 0 0 5 # set key flag expire_time data_length value # data STORED # 表示存储成功 get key VALUE key 0 5 # VALUE key flag data_length CAS value # data END # 表示获取结束 Storage Commands # 存储命令分为两类，一类是存储命令（set, add, replace, append 和 prepend），另一类是检查并设置命令（cas）。各自的格式如下：\n直接存储命令\n\u0026lt;command name\u0026gt; \u0026lt;key\u0026gt; \u0026lt;flags\u0026gt; \u0026lt;exptime\u0026gt; \u0026lt;bytes\u0026gt; [noreply]\\r\\n \u0026lt;data block\u0026gt; \\r\\n\nkey: 客户端声明的用于存储的 key flags: 任意的 16 位无符号整数用于存储额外的信息，对服务器透明，当数据被获取时会原封不动的返回。在 v1.2.1 之后，flags 可以是 32 位无符号整数。 exptime: 以秒为单位的过期时间，0 表示永不过期（但仍然可能会因为内存不足而被删除） bytes: 存储的数据长度 noreply: 可选参数，如果存在则不会返回响应 data block: 存储的数据 CAS 命令\ncas \u0026lt;key\u0026gt; \u0026lt;flags\u0026gt; \u0026lt;exptime\u0026gt; \u0026lt;bytes\u0026gt; \u0026lt;cas unique\u0026gt; [noreply]\\r\\n \u0026lt;data block\u0026gt; \\r\\n\ncas unique: 64 位无符号整数。使用 cas 前需要先通过 gets 获取到 token(cas unique)，使用 cas 时携带这个 token，否则会操作失败。 其他参数和存储命令一致 命令 说明 set 存储数据，会覆盖任何现有数据 add 仅当该数据尚不存在时才存储该数据 replace 替换数据，但前提是该数据已存在 append 在现有数据字节之后追加数据 prepend 在现有数据字节之前追加数据 cas 检查并设置，用于实现乐观锁 Retrieval Commands # 命令 说明 get 获取数据 get key1 key2 ... keyn \\r\\n gets 获取数据和 CAS 值 gets key1 key2 ... keyn \\r\\n Statistics Commands # 关于 stats 的详细字段解释查阅：https://github.com/memcached/memcached/blob/7d6bc7b09e3c6bb8eaff8b2b3d78d01e0bf17f6f/doc/protocol.txt#L1299-L1451\n命令 说明 stats 基本统计命令 stats \\r\\n stats items 获取服务器统计信息 stats items \\r\\n stats slabs 获取服务器统计信息 stats slabs \\r\\n stats sizes 获取服务器统计信息 stats sizes \\r\\n Other Commands # 命令 说明 version 获取服务器版本信息 version \\r\\n quit 关闭连接 quit \\r\\n flush_all 清空所有数据 flush_all \\r\\n delete 删除数据 delete key \\r\\n incr/decr value为无符号 64 位整数的字符串表示形式，可以进行递增/递减操作。key 不存在会操作失败。\nincr key value \\r\\n Meta Text Protocol # 元文本协议中会出现大量的 token 字样的描述, 其指代的是跟在 flag 后面 “语法单元” 或 理解为flags 参数亦可。 如 mg foo k v R30 中，R 就是一个 flag, 30 就是就是 R 携带的token。\n元文本协议（Meta Text Protocol）是 memcached 1.6.0 版本引入的新协议，它是一种基于文本的协议，用于支持 memcached 的元数据操作。相比于标准文本协议，元文本协议有以下特点：\n引入了新命令和（使用 ms 来取代 set；mg 来取代 get，gets, touch, GAT, GATS 等） 新增了元数据标志，如 l 标志自上一次访问以来的秒数，h 表示自存储以来是否被访问过，t 表示过期前剩余的秒数。 支持 key 采用二进制格式（传输时需要使用 base64 编码） 支持原子性操作 支持对频繁访问的对象进行提前缓存。通过 R 标志，可以在查询时就指示服务器刷新缓存。 新增的命令有：ms(meta set), mg(meta get), md(meta delete), me(meta debug) 等。\nMeta Set 命令：ms # 详细的协议文档可以参考：https://github.com/memcached/memcached/blob/master/doc/protocol.txt#L685\n在元文本协议中，请求的格式如下：\n# 请求 ms \u0026lt;key\u0026gt; \u0026lt;datalen\u0026gt; \u0026lt;flags\u0026gt;\\r\\n \u0026lt;data block\u0026gt;\\r\\n # 响应 \u0026lt;CD\u0026gt; \u0026lt;flags\u0026gt;\\r\\n 举例如下：\n# 设置一个 key=foo, value=bar 的数据, 其中 T90 表示 90 秒后过期, F1 表示 flags=1 ms foo 3 T90 F1\\r\\n bar\\r\\n # 响应 HD ms 响应的 CD 有以下几种：\nHD STORED 成功。\nNS NOT_STORED 数据因为错误而未存储。\nEX EXISTS 数据已经存在，往往是 CAS 语义，token失效的结果。\nNF NOT_FOUND 数据不存在, 往往是 CAS 语义，key 不存在的结果。\nms 支持的 flags 标志如下：\nFlags 说明 b key 采用 base64 编码 c 存储成功则返回 cas token，否则返回0（但 0 不能作为判断成功失败的依据）。 C(token) 比较 CAS 值，如果 token 与服务器的 CAS 值不一致则操作失败。可以配合 I 标记使用 E(token) 使用 token 作为新的 CAS 值，如果 item 被修改 F(token) 设置 client flags (32bit) I 使 item 失效，如果提供的 CAS 值比 item 的 CAS 值旧则操作失败 k 把 key 作为 token 返回（mg 默认不回携带 key, 那么可以通过此 flag 返回） O(token) 透明值, 会原封不动的返回 q 使用 noreply 语义 s 返回 item 字节大小 T(token) 设置 item 的 TTL M(token) 模式切换,默认 Set。Token 可选值 E(add), A(Append), P(Prepend), R(Replace) , S(Set) N(token) 如果是 append 模式下，如果不存在则会自动创建一个新的 item，接受 TTL 作为参数 Meta Get 命令：mg # 详细的协议文档可以参考：[https://github.com/memcached/memcached/blob/master/doc/protocol.txt#L685\nmg 命令用于获取数据，请求的格式如下：\n# 请求 mg \u0026lt;key\u0026gt; \u0026lt;flags\u0026gt;\\r\\n # 响应 \u0026lt;CD\u0026gt; \u0026lt;flags\u0026gt; \u0026lt;flags token\u0026gt;\\r\\n \u0026lt;data block\u0026gt;\\r\\n 举例如下：\n# 获取 key=foo 的数据, v 表示 value，t 表示过期时间，f 表示 flags mg foo t f v\\r\\n # 响应 VA 3 t79 f1 bat mg 响应的 CD 有以下几种：\nVA \u0026lt;size\u0026gt; \u0026lt;flags\u0026gt;\\r\\n \u0026lt;data block\u0026gt;\\r\\n 如果使用了 v 标志 且 服务器有数据。\nHD \u0026lt;flags\u0026gt;\\r\\n 如果没有使用 v 标志。\nEN\\r\\n 代表 key 不存在。\nmg 支持的 flags 标志如下：\nFlags 说明 b key 采用 base64 编码 c 返回 cas token f 返回 client flags token h 返回是否被访问过的标志 k 返回 key l 返回自上次访问以来的秒数 O 透明值 q 使用 noreply 语义 s 返回 item 大小 t 返回剩余的 TTL u 不更新 LRU v 返回 value E(token) 使用 token 作为新的 CAS 值，如果 item 被修改 N(token) 如果 item 不存在，创建一个新的 item，接受 TTL 作为参数 R(token) 如果剩余的 TTL 小于 token，为 recache 而胜出 T(token) 更新剩余的 TTL 由此可见，元文本协议相比于标准文本协议更加灵活，支持更多的元数据操作。尤其是 mg 跟 flags 的结合更加灵活，将以前需要执行多个命令的操作合并到了一个命令中，减少了网络往返的次数。\n元文本协议中当然不止这两个命令，还有 md, me 等命令，就不再一一列举了。通过 ms, mg 命令就足够掌握了元文本协议的设计理念和使用方法。\n抓包/源码分析 # 下面我们通过抓包的方式来分析 memcached 的协议交互过程，再结合一部分源码来分析下 memcached 是怎么支持“分布式”场景的。\n分析0: memcached 的协议长什么样？ # 要回答这个问题，我们甚至不需要抓包，从 memcached 的协议介绍我们就已经知道了它是一个文本协议，直接使用 telnet 就可以完成 memcached 的基本操作。下面就是一个 telnet 从连接到 set 并 get 的完整抓包截图：\n从截图我们可以看到，经过三次握手后，客户端发送了 ms foo 3 t90\\r\\nbar\\r\\n 命令，然后 memcached 服务器返回了 HD 表示存储成功。接着客户端发送了 mg foo t f v\\r\\n 命令，memcached 服务器返回了 VA 3 t-1 s3\\r\\nbar\\r\\n 表示获取成功。\n都非常的直观，这里唯一需要注意 t90 这 个flag 使用错了，应该是 T90，表示 90 秒后过期，但是 memcached 服务器并没有报错，而是直接存储了 -1 表示永不过期。\n另外我们也会注意到telnet客户端发送 ms foo 3 t90\\r\\nbar\\r\\n 是分成了两个 TCP 包发送的，这是因为 telnet 默认是行缓冲模式，需要按下回车键才会发送数据。\n分析1: memcached 集群是如何工作的？ # 我们通常看到 memcached 的介绍都会说 memcached 是一个“分布式”缓存系统，那么 memcached 是如何支持分布式场景的呢？从官方文档中我们还可以看到这样的描述：\nLogic Half in Client, Half in Server\nA “memcached implementation” is partially in a client, and partially in a server. Clients understand how to choose which server to read or write to for an item, what to do when it cannot contact a server.\nServers are Disconnected From Each Other\nMemcached servers are unaware of each other. There is no crosstalk, no synchronization, no broadcasting, no replication. Adding servers increases the available memory. Cache invalidation is simplified, as clients delete or overwrite data on the server which owns it directly.\n意思其实很简单，虽然 memcached 是一个“分布式”缓存系统，但是它的分布式是需要客户端配合的伪分布式，而不是通常意义上的分布式。它所支持的分布式是指客户端可以采取分片算法来决定数据存储在哪个服务器上，而 memcached 服务器本身是不会相互通信的，也不会进行数据同步的。如下图所示：\n这样的设计有利有弊，优点是 memcached 服务器之间不需要相互通信，不需要同步数据，因此 memcached 服务器可以很容易的扩展，只需要增加服务器就可以增加缓存容量。缺点也是显而易见的，分布式的特性需要客户端来实现，这就需要客户端有一定的分片算法，如果想要实现高可用(某台机器宕机)，还需要客户端支持“复制”机制。\n接下来我们就可以通过，抓包来验证这一点：\n部署 memcached 集群 # 通过 docker 启动两个 memcached 实例, 分别监听宿主机的 11211 和 11212 端口 version: \u0026#39;3\u0026#39; services: memcached1: image: memcached:1.5.6 ports: - \u0026#34;11211:11211\u0026#34; memcached2: image: memcached:1.5.6 ports: - \u0026#34;11212:11211\u0026#34; 客户端代码 from pymemcache.client.hash import HashClient client = HashClient([ \u0026#34;127.0.0.1:11211\u0026#34;, \u0026#34;127.0.0.1:11212\u0026#34;, ]) client.set(\u0026#34;key-0\u0026#34;, \u0026#34;I\u0026#39;ll be sent to node1(11211)\u0026#34;, expire=30) client.set(\u0026#34;key-2\u0026#34;, \u0026#34;I\u0026#39;ll be sent to node2(11212)\u0026#34;, expire=30) 通过 wirehsark 抓包，我们可以看到客户端按照预期发送了两个 set 命令，分别存储在 11211 和 11212 端口的 memcached 服务器上：\n而客户端（pymemcache）的逻辑也很简单，通过使用 Rendezvous Hashing 算法来计算组合 key (node-$key) 的 hash 值，选择 score 最高的服务器，如下：\n# self.hash_function = lambda x: murmur3_32(x) def get_node(self, key): high_score = -1 winner = None for node in self.nodes: score = self.hash_function(f\u0026#34;{node}-{key}\u0026#34;) if score \u0026gt; high_score: (high_score, winner) = (score, node) elif score == high_score: (high_score, winner) = (score, max(str(node), str(winner))) return winner 这里有意思的是，客户端的做法不是非常常见的 hash 取模的方式，而是通过一种叫做 HRW（Highest Random Weight）Hashing 的算法，这种算法的优点是当节点增加或者减少时，影响的数据量最小，而且不需要维护一致性哈希环。\n假如我们有三个节点（node1, node2, node3），key1 -\u0026gt; node1，key2 -\u0026gt; node2，key3 -\u0026gt; node3\n如果 node2 挂掉了，那么最坏的情况下：key1-\u0026gt; node3，key2 -\u0026gt; node3，key3 -\u0026gt; node1，这所有的 key 都要重新分配。\n而 HRW 算法的优点是：当 node2 挂掉了，只有 key2 -\u0026gt; node3 需要重新分配，其他的 key 仍然保持原来的分配，这是因为一开始 key1 -\u0026gt; node1 就已经是最高，那么节点减少这个状况也不会发生变化。\n分析2: 元文本协议有什么区别？ # 顾名思义，元文本协议还是基于文本的，但是它引入了更多的元数据标志，通过标志的组合避免了多次网络往返的问题，这对于需要优化 memcached 使用场景是非常有帮助的。\n但是这里，我们可以看看 memcached 内部是怎么处理元文本协议的，以下是 memcached 源码中对于元文本协议的处理：\n协议解析片段 void process_command_ascii(conn *c, char *command) { // ... 省略部分代码 // Meta commands are all 2-char in length. char first = tokens[COMMAND_TOKEN].value[0]; if (first == \u0026#39;m\u0026#39; \u0026amp;\u0026amp; tokens[COMMAND_TOKEN].length == 2) { switch (tokens[COMMAND_TOKEN].value[1]) { case \u0026#39;g\u0026#39;: process_mget_command(c, tokens, ntokens); break; case \u0026#39;s\u0026#39;: process_mset_command(c, tokens, ntokens); break; // ... 省略部分代码 } } else if (first == \u0026#39;g\u0026#39;) { // Various get commands are very common. WANT_TOKENS_MIN(ntokens, 3); if (strcmp(tokens[COMMAND_TOKEN].value, \u0026#34;get\u0026#34;) == 0) { process_get_command(c, tokens, ntokens, false, false); } // ... 省略部分代码 } else if (first == \u0026#39;s\u0026#39;) { if (strcmp(tokens[COMMAND_TOKEN].value, \u0026#34;set\u0026#34;) == 0 \u0026amp;\u0026amp; (comm = NREAD_SET)) { WANT_TOKENS_OR(ntokens, 6, 7); process_update_command(c, tokens, ntokens, comm, false); } else if (strcmp(tokens[COMMAND_TOKEN].value, \u0026#34;stats\u0026#34;) == 0) { process_stat(c, tokens, ntokens); } else { out_string(c, \u0026#34;ERROR\u0026#34;); } } else if (first == \u0026#39;d\u0026#39;) { if (strcmp(tokens[COMMAND_TOKEN].value, \u0026#34;delete\u0026#34;) == 0) { WANT_TOKENS(ntokens, 3, 5); process_delete_command(c, tokens, ntokens); } else { out_string(c, \u0026#34;ERROR\u0026#34;); } } else if (first == \u0026#39;t\u0026#39;) { if (strcmp(tokens[COMMAND_TOKEN].value, \u0026#34;touch\u0026#34;) == 0) { WANT_TOKENS_OR(ntokens, 4, 5); process_touch_command(c, tokens, ntokens); } else { out_string(c, \u0026#34;ERROR\u0026#34;); } } else if (strcmp(tokens[COMMAND_TOKEN].value, \u0026#34;version\u0026#34;) == 0) { process_version_command(c); } else if (strcmp(tokens[COMMAND_TOKEN].value, \u0026#34;quit\u0026#34;) == 0) { process_quit_command(c); } // ... 省略部分代码 return; } 可以看到 meta 协议是在文本协议之前的，通过判断第一个字符是否是 m 来判断是否是 meta 协议，然后再根据第二个字符来判断具体的 meta 命令。至于内部的处理逻辑，大体流程上和文本协议是一致的，只是在命令中增加了更多的元数据标志，这里就不再展开了。\n文本协议梳理 # 🟩 标记的是标准文本协议的命令 ⭕️ 标记的是元文本协议的命令\nCommand Usage Description Remark set set \u0026lt;key\u0026gt; \u0026lt;flags\u0026gt; \u0026lt;exptime\u0026gt; \u0026lt;bytes\u0026gt; [noreply]\\r\\n\u0026lt;data block\u0026gt;\\r\\n Stores data. 🟩 add add \u0026lt;key\u0026gt; \u0026lt;flags\u0026gt; \u0026lt;exptime\u0026gt; \u0026lt;bytes\u0026gt; [noreply]\\r\\n\u0026lt;data block\u0026gt;\\r\\n Stores data only if the server doesn\u0026rsquo;t already hold data for this key. 🟩 replace replace \u0026lt;key\u0026gt; \u0026lt;flags\u0026gt; \u0026lt;exptime\u0026gt; \u0026lt;bytes\u0026gt; [noreply]\\r\\n\u0026lt;data block\u0026gt;\\r\\n Stores data only if the server already holds data for this key. 🟩 append append \u0026lt;key\u0026gt; \u0026lt;bytes\u0026gt; [noreply]\\r\\n\u0026lt;data block\u0026gt;\\r\\n Appends data to an existing key. 🟩 prepend prepend \u0026lt;key\u0026gt; \u0026lt;bytes\u0026gt; [noreply]\\r\\n Prepends data to an existing key. 🟩 cas cas \u0026lt;key\u0026gt; \u0026lt;flags\u0026gt; \u0026lt;exptime\u0026gt; \u0026lt;bytes\u0026gt; \u0026lt;cas unique\u0026gt; [noreply]\\r\\n\u0026lt;data block\u0026gt;\\r\\n \u0026ldquo;Check and set\u0026rdquo; operation; stores data only if no one else has updated since it was last fetched. 🟩 ms (Meta Set) ms \u0026lt;key\u0026gt; \u0026lt;datalen\u0026gt; \u0026lt;flags\u0026gt;*\\r\\n\u0026lt;data block\u0026gt;\\r\\n Generic command for storing data to memcached. ⭕️ get get \u0026lt;key\u0026gt;*\\r\\n Retrieves data for one or more keys. 🟩 gets gets \u0026lt;key\u0026gt;*\\r\\n Retrieves data for one or more keys, including CAS unique values. 🟩 gat gat \u0026lt;exptime\u0026gt; \u0026lt;key\u0026gt;*\\r\\n Retrieves data and updates the expiration time of existing items. 🟩 gats gats \u0026lt;exptime\u0026gt; \u0026lt;key\u0026gt;*\\r\\n Retrieves data and updates the expiration time of existing items, including CAS unique values. 🟩 mg (Meta Get) mg \u0026lt;key\u0026gt; \u0026lt;flags\u0026gt;*\\r\\n Generic command for retrieving key data from memcached. ⭕️ delete delete \u0026lt;key\u0026gt; [noreply]\\r\\n Explicitly deletes items. 🟩 incr incr \u0026lt;key\u0026gt; \u0026lt;value\u0026gt; [noreply]\\r\\n Increments the value of an item. 🟩 decr decr \u0026lt;key\u0026gt; \u0026lt;value\u0026gt; [noreply]\\r\\n Decrements the value of an item. 🟩 touch touch \u0026lt;key\u0026gt; \u0026lt;exptime\u0026gt; [noreply]\\r\\n Updates the expiration time of an existing item without fetching it. 🟩 me (Meta Debug) me \u0026lt;key\u0026gt; \u0026lt;flag\u0026gt;\\r\\n Human readable dump of all available internal metadata of an item. ⭕️ md (Meta Delete) md \u0026lt;key\u0026gt; \u0026lt;flags\u0026gt;*\\r\\n Explicitly deletes items, marking them as \u0026ldquo;stale\u0026rdquo;. ⭕️ ma (Meta Arithmetic) ma \u0026lt;key\u0026gt; \u0026lt;flags\u0026gt;*\\r\\n Basic operations against numerical values, replacing \u0026ldquo;incr\u0026rdquo; and \u0026ldquo;decr\u0026rdquo; commands. ⭕️ mn (Meta No-Op) mn\\r\\n Returns a static response code. ⭕️ slabs reassign slabs reassign \u0026lt;source class\u0026gt; \u0026lt;dest class\u0026gt;\\r\\n Redistributes memory after the server hits its limit. 🟩 Memcached Client 实现 # 这是我实现的 Go 语言 memcached client 包：https://github.com/yeqown/memcached 支持标准文本协议和元文本协议、连接池和集群模式等功能，欢迎使用和反馈。\n"},{"id":9,"href":"/2024/12/17/%E8%BF%91%E6%9C%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/","title":"近期的一些经验总结","section":"Posts","content":" 这里不会过多的介绍软件的相关概念和架构，主要是针对实际问题的解决方案和思考。\n问题汇总 # CDC 相关\nCDC kafka-connect mysql sink 侧消费积压问题 CDC kafka-connect mysql source 侧删除事件投递了两条事件，导致删除动作数据量被放大 CDC kafka-connect mongodb 数据同步任务异常（消息超过 1MB ） 更新于: 2025-02-06\nCDC Elasticsearch sink 怎么自定义索引名称？ 自定义 transform 实现自定义索引名称 更新于：2025-10-11\nCDC kafka-connect mongodb 侧反复进行 “快照” 导致数据同步异常 DMS 数据同步相关\n数据迁移完成后，怎么对比源数据和目标数据是否一致？ 如果不一致怎么处理？ Istio 相关\nIstio 中多个 gateway 使用相同 host，analyze 是提示错误 Istio 中一个服务提供了多个端口的服务，怎么配置 Virtual Service ？ APISIX 相关\n使用 APISIX 作为网关，怎么进行有条件的响应重写？ APISIX 插件的执行顺序是怎么样的？ ShardingSphere Proxy\nHINT策略 在 ShardingSphere Proxy 中的使用 Kafka 相关\n如何将迁移kafka集群中的数据？ Pyroscope 相关\n使用 Go Pull 模式采集数据时为什么只有 cpu + gourotines + cpu samples 三个指标？ Doris 相关\n动态分区表插入数据时失败，提示 \u0026ldquo;no partition for this tuple\u0026rdquo; 1. CDC 相关 # CDC 是 Change Data Capture 的缩写，即变更数据捕获。CDC 是一种软件模式，用于捕获和跟踪数据库中的变更。CDC 通常用于复制数据、数据集成和数据仓库加载等场景。\n这里 CDC 的技术实现为使用 kafka-connect 连接 mysql 和 mongodb 将数据同步到异构系统中 elasticsearch。source 和 sink connector 使用的 debezium 插件。kafka-connect 是基于 kafka 构建的沟通数据系统的可靠工具，它最常用于将数据异构存储，以满足离线查询和分析、数据仓库、数据湖等需求。\n这里使用到 mysql source connector 的配置如下：\n{ \u0026#34;name\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;connector.class\u0026#34;: \u0026#34;io.debezium.connector.mysql.MySqlConnector\u0026#34;, \u0026#34;include.schema.changes\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;topic.prefix\u0026#34;: \u0026#34;mysql-xxx\u0026#34;, // 配置了 Reroute transform，将 真实表 归集到一个 逻辑表对应的 topic 中 \u0026#34;transforms\u0026#34;: \u0026#34;Reroute\u0026#34;, \u0026#34;transforms.Reroute.type\u0026#34;: \u0026#34;io.debezium.transforms.ByLogicalTableRouter\u0026#34;, \u0026#34;transforms.Reroute.topic.regex\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;transforms.Reroute.key.enforce.uniqueness\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;transforms.Reroute.topic.replacement\u0026#34;: \u0026#34;\u0026lt;topic_prefix\u0026gt;\u0026#34;, \u0026#34;schema.history.internal.kafka.topic\u0026#34;: \u0026#34;mysql-connector.schemahistory.xxx\u0026#34;, \u0026#34;schema.history.internal.kafka.bootstrap.servers\u0026#34;: \u0026#34;\u0026lt;kafka_server\u0026gt;\u0026#34;, \u0026#34;database.include.list\u0026#34;: \u0026#34;\u0026lt;database_name or regex\u0026gt;\u0026#34;, \u0026#34;database.port\u0026#34;: \u0026#34;\u0026lt;mysql_port\u0026gt;\u0026#34;, \u0026#34;database.hostname\u0026#34;: \u0026#34;\u0026lt;mysql_host\u0026gt;\u0026#34;, \u0026#34;database.password\u0026#34;: \u0026#34;\u0026lt;mysql_pass\u0026gt;\u0026#34;, \u0026#34;database.user\u0026#34;: \u0026#34;\u0026lt;mysql_user\u0026gt;\u0026#34;, \u0026#34;table.exclude.list\u0026#34;: \u0026#34;\u0026lt;table_name or regex\u0026gt;\u0026#34;, } mysql sink connector 的配置如下：\n{ \u0026#34;name\u0026#34;: \u0026#34;\u0026lt;connector name\u0026gt;\u0026#34;, \u0026#34;connector.class\u0026#34;: \u0026#34;io.confluent.connect.elasticsearch.ElasticsearchSinkConnector\u0026#34;, \u0026#34;behavior.on.null.values\u0026#34;: \u0026#34;DELETE\u0026#34;, \u0026#34;tasks.max\u0026#34;: \u0026#34;1\u0026#34;, // 任务数对应 consumer group 的消费者实例数 \u0026#34;key.ignore\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;write.method\u0026#34;: \u0026#34;UPSERT\u0026#34;, \u0026#34;connection.url\u0026#34;: \u0026#34;\u0026lt;es_addr\u0026gt;\u0026#34;, \u0026#34;topics.regex\u0026#34;: \u0026#34;\u0026lt;topic_regex\u0026gt;\u0026#34;, \u0026#34;transforms\u0026#34;: \u0026#34;unwrap,key\u0026#34;, // unwrap tranform 配置，处理删除事件：将删除事件的 value 设置为 null \u0026#34;transforms.unwrap.type\u0026#34;: \u0026#34;io.debezium.transforms.ExtractNewRecordState\u0026#34;, \u0026#34;transforms.unwrap.delete.handling.mode\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;transforms.unwrap.drop.tombstones\u0026#34;: \u0026#34;false\u0026#34;, // key tranform 配置，设置 ES 的 docID 为 mysql 数据表中的id (主键) \u0026#34;transforms.key.field\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;transforms.key.type\u0026#34;: \u0026#34;org.apache.kafka.connect.transforms.ExtractField$Key\u0026#34;, } 1.1 CDC kafka-connect mysql sink 侧消费积压问题 # 积压的原因，针对 mysql source / sink connector 只使用了一个分区。众所周知 topic 分区数会直接影响到消费的并发度，如果只有一个分区，那么只有一个消费者可以消费，消费者的消费速度就会受到限制。解决方案是增加分区数，增加消费者的并发度。\n这里存在的疑惑是：\nmysql 相关的 connector 是否只能使用一个分区？还能否保证顺序消费？ 有人反馈的的只能使用一个分区的 topic 是指的这部分吗？ 增加 topic 数之后，source connector 和 sink connector 都能够增加并行的任务吗？ 文档查阅和实验：\n在最开始使用时，当时确定为只有一个分区是由于文档中提及了：\nhttps://debezium.io/documentation/reference/stable/connectors/mysql.html#mysql-schema-change-topic\nNever partition the database schema history topic. For the database schema history topic to function correctly, it must maintain a consistent, global order of the event records that the connector emits to it.\nTo ensure that the topic is not split among partitions, set the partition count for the topic by using one of the following methods:\nIf you create the database schema history topic manually, specify a partition count of 1.\nIf you use the Apache Kafka broker to create the database schema history topic automatically, the topic is created, set the value of the Kafka num.partitions \u0026gt; configuration option to 1.\n但是实际上看来，这里的意思是指的 schema history topic 而不是其他的 topic。唯一找到限制是 mysql source connector task 只能为 1 如下所示：\ntasks.max\nDefault value: 1\nThe maximum number of tasks to create for this connector. Because the MySQL connector always uses a single task, changing the default value has no effect.\n因此分区数为 1 只是“历史”问题，实际上可以增加分区数，以增加消费侧并发度。\n另外文档中也说明了 source connector 投递消息的 message key 可以自定义 key, 其默认值为数据库表的主键，这样可以保证同一条数据的变更事件被投递到同一个分区中，从而保证了顺序性。\n如下是真实环境中一条数据的变更事件的 message key 示例：\n{ \u0026#34;schema\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;struct\u0026#34;, \u0026#34;fields\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;int64\u0026#34;, \u0026#34;optional\u0026#34;: false, \u0026#34;field\u0026#34;: \u0026#34;id\u0026#34; } ], \u0026#34;optional\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;$connectorName.$dbName.$tblName.Key\u0026#34; }, \u0026#34;payload\u0026#34;: { \u0026#34;id\u0026#34;: 1075966706511257600 // 唯一的变量，即主键列的值 } } 处理手段：\n调整 mysql sink connector 的 topic 分区数，增加并发度。 ./kafka-topics.sh --bootstrap-server \u0026lt;kafka_server\u0026gt; --alter --topic \u0026lt;topic_name\u0026gt; --partitions \u0026lt;new_partition_num\u0026gt; 调整 mysql sink connector 的任务数与分区数一致（这里20为示例），以保证每个任务都能够消费到数据。 - “tasks.max”: \u0026#34;1\u0026#34; + “tasks.max”: \u0026#34;20\u0026#34; 注意：增加分区数，大概率会导致某些消息从之前的分区被投递到新的分区，这样会导致消息的顺序性被打乱，因此要谨慎处理，要么能够容忍这种异常，要么事后恢复最终一致，也可以先停止生产等消费完毕后再行调整。\n1.2 CDC kafka-connect mysql source 侧删除事件投递了两条事件，导致删除动作数据量被放大 # 这个问题的背景是，mysql 部分表数据量极大，个别单表超过 100 million 条数据，且日增量极大，因此会定期删除。定期删除一开始选择的是使用 mysql Event Scheduler 定时任务 + Procedure 删除，但这种方式对于运维人员的要求较高，且不够灵活（分库分表场景下），其次在业务高峰期删除操作会影响到业务的正常运行。因此另外选择了自行实现一个定时任务，定时删除数据，以避开业务高峰。\n但是部署上线时发现一个问题：删除的数据量大概是 20 Million 条，但是 kafka 中的消息数量是 40 Million 条，且每条消息都是删除操作。\n演示如下图中所示，针对同一条数据的删除操作，source connector 投递了两条消息，导致数据量被放大。\n文档查阅：\nDebezium/Source Connector/Mysql#delete events Debezium/Source Connector/Mysql#tombstone events When a row is deleted, the delete event value still works with log compaction, because Kafka can remove all earlier messages that have that same key. However, for Kafka to remove all messages that have that same key, the message value must be null. To make this possible, after the Debezium MySQL connector emits a delete event, the connector emits a special tombstone event that has the same key but a null value.\n这意味着当一条数据被删除时，source connector 会先投递一条删除事件，然后再投递一条 tombstone 事件。这样做的目的是为了保证 kafka 的 log compaction 机制能够正常工作，即如果一条数据被删除后，当 log compaction 开启时，kafka 可以删除所有之前的消息，只保留最新的消息（即被删除的状态），当其他系统消费时，只需要关注最新的状态即可。\nKafka Log Compaction 参考：Kafka Log Compaction\n但在我们的实际场景中，Log Compaction 机制并不适用也没有开启，因此不需要 tombstone 事件，只需要删除事件即可。不适用的原因在于，当数据被删除时离它的创建时间已经过去了很久，同时 kafka 中也只保留 7d 的数据，因此 tombstone 事件对我们来说没有意义。\n处理手段：\n调整 source connecor 配置，增加 unwrap tranform 以删除不必要的删除事件。 - \u0026#34;transforms\u0026#34;: \u0026#34;Reroute\u0026#34;, + \u0026#34;transforms\u0026#34;: \u0026#34;Reroute,unwrap\u0026#34;, + \u0026#34;transforms.unwrap.type\u0026#34;: \u0026#34;io.debezium.transforms.ExtractNewRecordState\u0026#34;, + \u0026#34;transforms.unwrap.delete.handling.mode\u0026#34;: \u0026#34;none\u0026#34;, + \u0026#34;transforms.unwrap.delete.tomstones.handling.mode\u0026#34;: \u0026#34;tomstone\u0026#34;, 调整后的效果如下图所示：\n还可以直接调整 source connector 的配置，不产生任何的删除事件，从而减少消息量。 - \u0026#34;transforms\u0026#34;: \u0026#34;Reroute\u0026#34;, + \u0026#34;transforms\u0026#34;: \u0026#34;Reroute,unwrap\u0026#34;, + \u0026#34;transforms.unwrap.type\u0026#34;: \u0026#34;io.debezium.transforms.ExtractNewRecordState\u0026#34;, + \u0026#34;transforms.unwrap.drop.tombstones\u0026#34;: \u0026#34;true\u0026#34;, + \u0026#34;transforms.unwrap.delete.handling.mode\u0026#34;: \u0026#34;drop\u0026#34; 1.3 CDC kafka-connect mongodb 数据同步任务异常（消息超过 1MB ） # 这个问题是偶然发现 mongodb source connector 异常，查看日志发现如下异常：\norg.apache.kafka.connect.errors.ConnectException: Unrecoverable exception from producer send callback at org.apache.kafka.connect.runtime.WorkerSourceTask.maybeThrowProducerSendException(WorkerSourceTask.java:334) at org.apache.kafka.connect.runtime.WorkerSourceTask.prepareToSendRecord(WorkerSourceTask.java:128) at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.sendRecords(AbstractWorkerSourceTask.java:404) at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.execute(AbstractWorkerSourceTask.java:361) at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:204) at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:259) at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.run(AbstractWorkerSourceTask.java:75) at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:181) at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) at java.base/java.lang.Thread.run(Thread.java:829) Caused by: org.apache.kafka.common.errors.RecordTooLargeException: The message is 1096354 bytes when serialized which is larger than 1048576, which is the value of the max.request.size configuration. 如果是熟悉 java + kafka 开发的同学，应该一眼就能看出来是消息超过了 kafka 的 max.request.size 配置，导致消息发送失败。\n文档查阅：\n这里走了“弯路”去翻了文档，是因为对 java + kafka 开发不熟悉，只是大致猜测是消息尺寸超过了 kafka 的限制，因此查阅了 kafka 的文档。\nApache Kafka Producer Config#max.request.size Apache Kafka Broker Config#message.max.bytes 处理手段：\n调整 kafka broker 的 message.max.bytes 配置，增加消息的最大尺寸。\n./kafka-configs.sh --zookeeper \u0026lt;zookeeper\u0026gt; --entity-type brokers --entity-name \u0026lt;broker_id\u0026gt; --alter --add-config message.max.bytes=10485760 调整 kafka connect 的配置，增加 producer.max.request.size 配置。\n+ producer.max.request.size=10485760 // 10MB 1.4 CDC Elasticsearch sink 怎么自定义索引名称？ # 背景： 通过 CDC 将数据异构到 Elasticsearch 中， MySQL 的逻辑表 跟 ES 索引一一对应。随着业务增长，数据量不断增长，索引数量也在不断增加，直到超过了 Elasticsearch 的限制 单一分片的文档数量不能超过 2^31 - 1 约为 21亿个文档。\n该数据为流水数据，其业务特性为：只写不改，保存3个月，因此这里的解决思路有两种：\n调整索引的分片数（之前为 1） 根据日期分别索引，比如：index_name_20240101，index_name_20240102，以此类推。 两种方案有其各自的优缺点：\nES 会尽量让分片分散保存在不同的节点上，降低单个节点故障时的影响。分片的分配策略主要考量两个因素：\n分片数量：大多数搜索会命中多个分片，每个分片的搜索会占用搜索线程和其他资源。ES 默认单个节点最多 1000 分片。\n分片大小: 分片合理的大小为 10-50GB。一个节点可以容纳的分片数量与节点的堆内存成正比，小于20分片/每GB堆内存；\n适当增加分片数会增加系统的吞吐量，提高利用率；但是分片数过多会导致系统的性能下降，增加系统的负担。\n方案一：调整分片数 （如：1 =\u0026gt; 3）\n优点： 业务无感，不需要修改业务代码 可以提高吞吐量 缺点： 分片数调整不灵活，当数据增量再次超限制时，还需要再来一次（重建成本更高）。 需要重建索引，在数据量非常大的情况下需要慎重操作，避免长时间降低集群性能 方案二：根据日期分别索引，业务中使用 alias 指向全部的索引\n优点： 可以灵活的删除索引数据 可以提高查询性能 缺点： 索引数量，分片数量增加较多，需要考虑集群的性能 需要\u0026quot;重建\u0026quot;索引 ES Sink Connector 需要按照日期索引的能力 The Kafka source topic name is used to create the destination index name in Elasticsearch. You can change this name prior to it being used as the index name with a Single Message Transformation (SMT)–RegexRouter or TimeStampRouter–only when the flush.synchronously configuration property is set to true.\nhttps://docs.confluent.io/kafka-connectors/elasticsearch/current/overview.html\n这里综合实际情况，选择了方案二。但问题也随之而来，ES Sink Connector 默认使用的 index = topic 也就是 cdc.dbname.tablename 这种形式, 而我们想要的是 index = cdc.dbname.tablename-yyyyMMdd 这种形式。\n文档查阅：\nConfluent/Elasticsearch Sink Connector#index Confluent/SMT/RegexRouter Confluent/SMT/TimestampRouter 经过查阅文档，发现可以使用 SMT 来实现自定义索引名称，其中 RegexRouter 和 TimestampRouter 都是可以的，但是符合我们的场景的是 TimestampRouter。\n在 ES Sink Connector 中增加如下配置：\n\u0026#34;transforms\u0026#34;: \u0026#34;TimestampRouter\u0026#34;, \u0026#34;transforms.TimestampRouter.type\u0026#34;: \u0026#34;org.apache.kafka.connect.transforms.TimestampRouter\u0026#34;, \u0026#34;transforms.TimestampRouter.topic.format\u0026#34;: \u0026#34;foo-${topic}-${timestamp}\u0026#34;, \u0026#34;transforms.TimestampRouter.timestamp.format\u0026#34;: \u0026#34;YYYYMM\u0026#34; 这样就可以实现自定义索引名称了 ordersTopic 经过上述配置后: foo-ordersTopic-202401。但是这里需要注意 timestamp 的来源是 kafka message.timestamp，而不是 message.key/value 中的 timestamp 字段。\n一般来说流水这种记录场景是够用的，可能会存在日期偏移的情况。比如：record 的实际创建时间为 2024-01-01 23:59:59，但是实际写入 kafka 的时间为 2024-01-02 00:00:00，这样该条记录会被路由到 foo-ordersTopic-20240201 索引中。\n处理手段：\n参考 TimestampRouter 的设计和代码，我们自定义一个 transform，实现根据 message.value 中的 field 字段来生成时间戳, 再组装 ES index 名。具体实现参见 #1.5 自定义 MessageTimestampRouter。\n1.5 自定义 MessageTimestampRouter 实现动态索引名称 # 预期的使用效果是：\n{ \u0026#34;transforms.TimestampRouter.type\u0026#34;: \u0026#34;com.custom.kafka.connect.transforms.MessageTimestampRouter\u0026#34;, \u0026#34;transforms.TimestampRouter.topic.format\u0026#34;: \u0026#34;${topic}-${timestamp}\u0026#34;, \u0026#34;transforms.TimestampRouter.timestamp.format\u0026#34;: \u0026#34;yyyyMMdd\u0026#34;, \u0026#34;transforms.TimestampRouter.message.timestamp.field\u0026#34;: \u0026#34;created_at\u0026#34; } messageValue : {\u0026#34;created_at\u0026#34;: \u0026#34;2024-01-01 00:00:00\u0026#34;, \u0026#34;id\u0026#34;: 1} sourceTopicName : ordersTopic destinationIndexName: ordersTopic-20240101 文档查阅：\ngithub/kafka/org.apache.kafka.connect.transforms.TimestampRouter Custom Kafka Connect Single Message Transforms 处理手段：\n新建一个 java 工程，创建一个 class 继承 org.apache.kafka.connect.transforms.Transformation 接口。 /** * Single message transformation for Kafka Connect record types. * \u0026lt;p\u0026gt; * Connectors can be configured with transformations to make lightweight message-at-a-time modifications. * \u0026lt;p\u0026gt;Kafka Connect may discover implementations of this interface using the Java {@link java.util.ServiceLoader} mechanism. * To support this, implementations of this interface should also contain a service provider configuration file in * {@code META-INF/services/org.apache.kafka.connect.transforms.Transformation}. * * @param \u0026lt;R\u0026gt; The type of record (must be an implementation of {@link ConnectRecord}) */ public interface Transformation\u0026lt;R extends ConnectRecord\u0026lt;R\u0026gt;\u0026gt; extends Configurable, Closeable { /** * Apply transformation to the {@code record} and return another record object (which may be {@code record} itself) * or {@code null}, corresponding to a map or filter operation respectively. * \u0026lt;p\u0026gt; * A transformation must not mutate objects reachable from the given {@code record} * (including, but not limited to, {@link org.apache.kafka.connect.header.Headers Headers}, * {@link org.apache.kafka.connect.data.Struct Structs}, {@code Lists}, and {@code Maps}). * If such objects need to be changed, a new {@link ConnectRecord} should be created and returned. * \u0026lt;p\u0026gt; * The implementation must be thread-safe. * * @param record the record to be transformed; may not be null * @return the transformed record; may be null to indicate that the record should be dropped */ R apply(R record); /** Configuration specification for this transformation. */ ConfigDef config(); /** Signal that this transformation instance will no longer will be used. */ @Override void close(); } 定义 ConfigDef 配置项，用于配置 transformation。 /** * 配置参数 * message.timestamp.field: 时间戳字段名 * topic.format: 主题格式 * timestamp.format: 时间戳处理格式化 */ public static final ConfigDef CONFIG_DEF = new ConfigDef() .define(ConfigName.MESSAGE_TIMESTAMP_FIELD, ConfigDef.Type.STRING, \u0026#34;created_at\u0026#34;, ConfigDef.Importance.HIGH, \u0026#34;时间戳字段名\u0026#34;) .define(ConfigName.TOPIC_FORMAT, ConfigDef.Type.STRING, \u0026#34;${topic}-${timestamp}\u0026#34;, ConfigDef.Importance.HIGH, \u0026#34;主题格式\u0026#34;) .define(ConfigName.TIMESTAMP_FORMAT, ConfigDef.Type.STRING, \u0026#34;yyyyMMdd\u0026#34;, ConfigDef.Importance.HIGH, \u0026#34;时间戳处理格式\u0026#34;); @Override public ConfigDef config() { return CONFIG_DEF; } 实现 apply 方法，用于按照需求处理 record 的 topic。 @Override public R apply(R record) { Long timestamp = null; // 尝试从记录值中获取时间戳 if (record.value() instanceof Struct) { Struct value = (Struct) record.value(); try { Field field = value.schema().field(messageTimestampField); if (field != null) { timestamp = value.getInt64(messageTimestampField); } } catch (Exception e) { log.warn(\u0026#34;Failed to extract timestamp from field {}: {}\u0026#34;, messageTimestampField, e.getMessage()); } } // 如果无法从字段获取时间戳，则使用记录的时间戳 if (timestamp == null) { log.warn(\u0026#34;No timestamp found in record.value {}, trying record.timestamp\u0026#34;, messageTimestampField); timestamp = record.timestamp(); if (timestamp == null) { log.warn(\u0026#34;No timestamp found in record, using current time\u0026#34;); timestamp = System.currentTimeMillis(); } } // 格式化时间戳 String formattedTimestamp = timestampFormat.get().format(new Date(timestamp)); // 替换主题格式中的变量 String updatedTopic = topicFormat .replace(\u0026#34;${topic}\u0026#34;, record.topic()) .replace(\u0026#34;${timestamp}\u0026#34;, formattedTimestamp); // log.info(\u0026#34;Updated topic: {}\u0026#34;, updatedTopic); // 创建新记录 return record.newRecord( updatedTopic, record.kafkaPartition(), record.keySchema(), record.key(), record.valueSchema(), record.value(), record.timestamp() ); } 1.6 CDC kafka-connect mongodb 侧反复进行 “快照” 导致数据同步异常 # 导致这个现象的起始原因是，mongodb 数据库中有多个集合，其中拥有非常多的过期数据，超过 8000w 条，导致了明显的慢查询，因此开发增加了 TTL 索引来自动删除过期数据。当然这两个集合配置在了 mongodb source connector 中，需要同步变更。\n但是在实际运行中，发现 mongodb source connector 反复进行“快照”，mongodb source connector 的生产速率监控如下：\n某一个 collection 生产速率 该集合正常生产的速率约为 20条/s，图中的速率已经达到 2000条/s，且反复出现。\n文档查阅：\nDebezium/Source Connector/Mongodb#performing-a-snapshot However, if no offset is found, or if the oplog no longer contains that position, the task must first obtain the current state of the replica set contents by performing a snapshot.\n如果没有找到 offset，或者 oplog 中不再包含该位置，则任务必须首先通过执行快照来获取副本集内容的当前状态。\n总结一下，Mongodb source connector 会在以下情况下执行快照：\n初次启动 connector 时，找不到保存的偏移量 偏移量过期，oplog 中不再包含该位置 这里遇到的情况属于第二种情况，即偏移量过期，oplog 中不再包含该位置。由于删除的数据量非常大，导致 oplog 滚动非常快，mongodb source connector 保存的偏移量很快就过期了，因此会触发快照。\n而导致反复快照的原因也类似，执行快照的集合中有非常大的集合超过 100 million 条数据，导致快照时间非常长，快照过程中，oplog 继续滚动，导致快照结束后，保存的偏移量已经过期，因此会再次触发快照。陷入了一个死循环。\n处理手段：\n这里优化的方向有两个：\n缩短快照持续时间 增大 oplog 的大小 db.oplog.rs.stats() // 其中 maxSize 字段表示 oplog 的最大大小，单位为 B { maxSize: 10632303360 // 约 10GB } 缩短快照可以 增加快照线程数 来提高并发度：\n+ \u0026#34;snapshot.max.threads\u0026#34;: 6 // 默认值为 1 还可以 拆分成多个 connector 来增加并发度。\n个人经验: 采用 include 这种显式包含的方式来指定需要同步的集合，而不是 exclude 排除的方式。\n至于是调大 oplog ，则需要根据实际情况来定。可以通过查看 oplog 的使用情况来判断是否需要调大 oplog 的大小：\n\u0026gt; rs.printReplicationInfo() actual oplog size \u0026#39;10139.754638671875 MB\u0026#39; --- configured oplog size \u0026#39;10139.754638671875 MB\u0026#39; --- log length start to end \u0026#39;5012612 secs (1392.39 hrs)\u0026#39; --- // 其他 如果确实需要调大 oplog 的大小，可以参考如下文档：\nMongoDB/Replication/Oplog Size db.adminCommand( { replSetResizeOplog: 1, size: \u0026lt;double\u0026gt;, // 以 MB 为单位 minRetainHours: \u0026lt;double\u0026gt; // 可选，最小保留时间，单位为小时, 如 1.5 代表 1h30m } ) 另外文档中还提到了，增量快照 的方式来解决大数据量的快照问题：\n增量快照的含义就是不会一次性快照完数据库的完整状态，而是分批（块）的快照每个集合（可以自定义集合和块大小）。 增量快照运行是, oplog stream change 应用也会同步进行，而不用等待快照完成后 增量快照停止后恢复也不会从头开始，而是从上次快照的地方继续 由于增量快照是基于信号触发的（向 数据库信号集合 插入新数据 / 发布Kafka 信号消息），因此可以随时触发快照 小结：增量快照可以很好规避的大数据量快照的问题，只是会增加使用成本。\n经验总结：\n如果存在超大数据量的集合，建议使用增量快照 Mongodb source connector 的快照线程默认为 1，对于大数据量的集合，建议增加快照线程数 如果 oplog 滚动过快，建议增大 oplog 的大小 2. DMS 数据同步相关 # DMS 是 Data Migration Service 的缩写，即数据迁移服务。DMS 是一种数据迁移服务，用于将数据从一个地方迁移到另一个地方。DMS 通常用于数据迁移、数据备份、数据恢复等场景。\n2.1 数据迁移完成后，怎么对比源数据和目标数据是否一致？ # 在实际场景中，可能会有数据迁移的需求，比如说将数据从一个数据库迁移到另一个数据库，或者将数据从一个表迁移到另一个表。在迁移完成后，我们需要对比源数据和目标数据是否一致，以保证数据的一致性。\n这里记录下在检索中出来可以使用的一些工具：\ntidb-tools/sync_diff_inspector: 是一个可以对比两个数据库并且输出差异的工具，支持 MySQL 和 TiDB 数据库。使用文档参考：sync_diff_inspector#usage pt-table-checksum: 是 Percona Toolkit 中的一个工具，用于检查 MySQL 复制的一致性。 pt-table-sync: 是 Percona Toolkit 中的一个工具，用于同步两个表的数据。 2.2 如果不一致怎么处理？ # tidb-tools/sync_diff_inspector 可以输出修复SQL。Percona Toolkit 中的 pt-table-sync 也可以用于同步两个表的数据。\n3. Istio 相关 # 3.1 Istio 中多个 gateway 使用相同 host，analyze 是提示错误 # 在 Istio 中，gateway 是一个虚拟服务，用于将流量路由到对应的服务。gateway 有一个 host 字段，用于指定域名。在实际场景中，可能会有多个 gateway 使用相同的 host，这样就会导致 analyze 时提示错误。\n3.2 Istio 中一个服务提供了多个端口的服务，怎么配置 Virtual Service ？ # 我们有一个短信服务，它同时提供了 HTTP 和 gRPC 服务，分别使用了 8000 和 50051 端口。在没有使用 istio 去路由流量的时候，在 k8s 中配置这个服务其实很简单，只需要在 Service 中配置多个端口即可。如下：\napiVersion: v1 kind: Service metadata: name: sms-service spec: selector: app: sms-service ports: - name: http port: 8000 targetPort: 8000 - name: grpc port: 50051 targetPort: 50051 但是在使用 istio 时，需要配置 Virtual Service 来路由流量，那么如何配置 Virtual Service 来支持多个端口的服务呢？\n文档查阅：\nIstio Virtual Service#HTTPRoute HTTPRoute\nDescribes match conditions and actions for routing HTTP/1.1, HTTP2, and gRPC traffic. See VirtualService for usage examples.\nhttp[].match.port (Optional) uint32\nSpecifies the ports on the host that is being addressed. Many services only expose a single port or label ports with the protocols they support, in these cases it is not required to explicitly select the port.\n从这一节文档，我们得知 HTTPRoute 可以用于路由 HTTP/1.1、HTTP2 和 gRPC 流量，因此我们可以使用 HTTPRoute 来配置 Virtual Service。match 匹配条件中的 port 字段可以用于指定端口，专门用于单个服务提供多端口的场景。需要注意的是 port 代表的是访问地址中的端口，而不是服务暴露的端口。\n处理手段：\n在 Virtual Service 配置中，新增多个 httpRoute 来路由到不同的端口。 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: sms-service spec: hosts: - api.my http: - name: http match: - uri: prefix: / port: 80 route: - destination: host: sms-service port: number: 8000 - name: grpc match: - port: 50051 route: - destination: host: sms-service port: number: 50051 4. APISIX 相关 # 4.1 使用 APISIX 作为网关，怎么进行有条件的响应重写？ # 响应重写属于网关的基本功能，APISIX 作为一个开源的网关，也支持响应重写。但是在实际场景中，可能会有一些特殊的需求，比如说只有满足一定条件时才进行响应重写。举个实际的例子：\n当服务器要进行停机维护的时候，我们希望所有的请求，不论是客户端还是第三方请求的请求，都响应 503 状态码，并且返回一个提示信息 “服务器正在维护中，预计维护时间为 2024-12-17 00:00:00 - 2024-12-17 06:00:00”，同时还有另外一个要求当服务器恢复时，可以允许特定的请求通过，比如说只有来自公司内部的请求才可以通过。\n这里默认客户端是支持展示这种响应的，因此不考虑客户端展示问题。\n文档查阅：\nAPISIX#Response-Rewrite APISIX#Response-Rewrite Attributes APISIX#Response-Rewrite GITHUB 这里关注该插件的以下几个属性：\nstatus_code: 响应状态码 body: 响应体，可以是字符串或者 base64 编码的字符串。！！！注意 body 和 filters 不能同时使用。 body_base64: 用于指示 body 是否 base64 编码 vars: 用于匹配条件，支持多个匹配条件。 变量列表参考： Nginx 变量：https://nginx.org/en/docs/varindex.html APISIX 变量：https://apisix.apache.org/docs/apisix/apisix-variable/ 操作符参考：https://github.com/api7/lua-resty-expr#operator-list filters: 对 body 内容进行正则匹配并替换。 vars 配置举例：\n# 匹配所有查询参数带有 pkg=com.company.io 的请求 vars: - - arg_pkg - ~= - \u0026#34;com.company.io\u0026#34; filter 配置举例：\n# 这个配置会将响应体中的所有 Example 替换为 Example-Replaced filters: - regex: Example scope: global replace: Example-Replaced options: \u0026#34;jo\u0026#34; # 正则匹配选项，参见 https://github.com/openresty/lua-nginx-module#ngxrematch 处理手段：\n在 ApisixRoute 配置中，配置 response-rewrite 插件，设置 body 为维护提示信息，设置 vars 为特定请求的匹配条件。\n代码片段 apiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: account-api spec: http: - name: account-api backends: - serviceName: account-api servicePort: 8000 match: hosts: - api.example.com paths: - /account/* plugins: - name: cors enable: true - name: response-rewrite enable: true body: \u0026gt;- { \u0026#34;code\u0026#34;: 503, \u0026#34;message\u0026#34;: \u0026#34;服务器正在维护中，预计维护时间为 2024-12-17 00:00:00 - 2024-12-17 06:00:00\u0026#34; } body_base64: false # 是否 body base64，适用于二进制数据 vars: - - arg_pkg - ~= - \u0026#34;com.company.io\u0026#34; 4.2 APISIX 插件的执行顺序是怎么样的？ # APISIX 会优先执行全局的插件，然后再执行路由级别的插件。每个插件内部定义了一个优先级 priority，优先级越高的插件越先执行。\n如果想要调整插件的顺序，可以配置 _meta 字段, 参见 APISIX#Plugin Custom priority\n5. ShardingSphere Proxy # 5.1 HINT策略 在 ShardingSphere Proxy 中的使用 # 在前文 ShardingSphere Proxy 问题几则 中提到了 Shardingsphere 支持 HINT 策略，即通过 SQL Hint 来指定路由规则。\n这里的场景是针对已经使用了标准分片算法中 Inline 策略的场景，但是个别场景没有办法使用分片键进行路由，因此需要使用 HINT 策略。比如说有一个 t_order 表，进行了数据分片，分库分表的策略是：根据 mch_id 取模分为 2 个库，根据 user_id 取模分为 2 个表。\n现在要针对这个表清理数据。最简单的想法就是直接执行 DELETE FROM t_order WHERE created_at \u0026lt; 'yyyy-MM-dd' 来清理数据，但是很明显直接删除会导致性能问题。可以考虑使用批量删除的方式，例如：\nDELETE FROM t_order WHERE created_at \u0026lt; \u0026#39;yyyy-MM-dd\u0026#39; limit 1000; 但是这样的方式不被 Shardingsphere Proxy 支持，因为这样的 SQL 语句没有分片键会导致全路由，Shardingsphere Proxy 并不支持。\n文档查阅：\nShardingSphere Proxy#SQL HINT 这里需要提前开启 sqlCommentParseEnable 选项，以支持 sql comment 解析。\n处理手段：\n想要让这样的删除语句可以落到特定的分片上去，那么我们可以使用 HINT 策略，即在 SQL 语句中添加 Hint 来指定路由规则，如下：\n/* ShardingSphere hint: dataSourceName=sharding_db_0 */ delete from t_order_0 where created_at \u0026lt; \u0026#39;yyyy-MM-dd\u0026#39; limit 1000; 6. Kafka 相关 # 6.1 如何将迁移kafka集群中的数据？ # 在实际场景中，可能会有迁移 kafka 集群的需求，比如说迁移到新的集群，或者迁移到新的版本等。这里的迁移是指迁移数据，而不是迁移集群。一般说来，kafka 数据迁移分为两种：\n集群内迁移：比如新增一个 broker, 需要将现有的数据重新分布，已实现负载均衡。bin/kafka-reassign-partitions.sh 脚本可以用于迁移数据, 参见 Kafka#automigrate 集群间迁移：比如将 cluster-ea 中的 topic-demo 整体迁移到 cluster-eu 中。 查阅文档/思路总结：\n跨集群迁移的思路是：\n使用 mirror-maker 先将数据从 cluster-ea 中复制到 cluster-eu 中。 迁移所有的消费者到 cluster-eu 中 迁移所有的生产者到 cluster-eu 中 处理手段：\n以下命令仅供参考，具体的参数需要根据实际情况调整。\n使用 mirror-maker 复制数据\n./kafka-mirror-maker.sh --consumer.config consumer.properties --producer.config producer.properties --whitelist \u0026#34;topic-demo\u0026#34; 迁移消费者\n修改消费者配置文件中的 bootstrap.servers 为新的集群地址\n迁移生产者\n修改生产者配置文件中的 bootstrap.servers 为新的集群地址\n停止 mirror-maker\n7. Pyroscope 相关 # Pyroscope 是一个开源的性能监控工具，支持多种语言 SDK，支持多种采集模式。Pyroscope 有两种采集模式：\nPull 模式：Pyroscope 会定时从应用程序中拉取数据。程序需要主动暴露相应的端口，参见：Pyroscope#Scrape Push 模式：应用程序通过内置 SDK 主动推送数据到 Pyroscope，参见：Pyroscope#Push 如下：\n7.1 使用 Go Pull 模式采集数据时为什么只有 cpu + gourotines + cpu samples 三个指标？ # 按照官方文档配置好 Alloy 服务，并在应用上暴露 /debug/pprof/* 等端口之后（使用标准库的 pprof），发现在 Pyroscope 中只有 cpu + gourotines + cpu samples 三个指标，而没有其他的指标。如下图：\n此时使用的 alloy scrape 配置如下：\n代码片段 pyroscope.scrape \u0026#34;scrape_job_name\u0026#34; { // 这部分仅作演示 targets = [{\u0026#34;__address__\u0026#34; = \u0026#34;localhost:4040\u0026#34;, \u0026#34;service_name\u0026#34; = \u0026#34;example_service\u0026#34;}] forward_to = [pyroscope.write.write_job_name.receiver] // 关注这部分配置 profiling_config { profile.process_cpu { enabled = true } profile.godeltaprof_memory { enabled = true } profile.memory { // disable memory, use godeltaprof_memory instead enabled = false } profile.godeltaprof_mutex { enabled = true } profile.mutex { // disable mutex, use godeltaprof_mutex instead enabled = false } profile.godeltaprof_block { enabled = true } profile.block { // disable block, use godeltaprof_block instead enabled = false } profile.goroutine { enabled = true } } } 文档查阅：\nPyroscope#Scrape 通过文档就很明了了，因为 alloy scrape 任务配置的 memory、mutex、block 等指标都都指向了 godeltaprof_memory、godeltaprof_mutex、godeltaprof_block 等指标，其访问路径期望是 /debug/pprof/delta_heap 而不是应用实际暴露的 /debug/pprof/allocs，所以才有没有 memory 相关的指标。\n而 delta_heap 等指标是在应用程序中通过 SDK 新增的指标，对于 Go 标准库中的 pprof 并不包含。\n关于 godeltaprof 这个包，是 runtime/pprof 的一个 fork 版本：减少了内存分配，相应的GC压力更小；同时支持惰性采样，采样更加高效（样本尺寸更小）。\n处理手段1：\n在应用程序中使用 pyroscope SDK，新增 godeltaprof 指标。\n处理手段2：\n继续在应用程序中使用标准库的 pprof，调整 scrape 配置，将 memory、mutex、block 等指标指向标准库的 pprof。\n代码片段 profile.godeltaprof_memory { - enabled = true + enabled = false } profile.memory { // disable memory, use godeltaprof_memory instead - enabled = false + enabled = true } profile.godeltaprof_mutex { - enabled = true + enabled = false } profile.mutex { // disable mutex, use godeltaprof_mutex instead - enabled = false + enabled = true } profile.godeltaprof_block { - enabled = true + enabled = false } profile.block { // disable block, use godeltaprof_block instead - enabled = false + enabled = true } 效果展示：\n调整后的效果如下图所示，已经正常展示：\n8. Doris 相关 # 8.1 动态分区表插入数据时失败，提示 \u0026ldquo;no partition for this tuple\u0026rdquo; # 存在这么一个动态分区表 根据 day 字段动态分区，其 DDL 如下：\ncreate table if not exists a_daily_tbl ( `mch_id` INT not null, `user_id` BIGINT not null, `day` DATE not null, `type` VARCHAR(64) not null, `count` INT SUM, `total` BIGINT SUM, `add_total` BIGINT SUM, `updated_at` DATETIME MAX ) AGGREGATE KEY(`mch_id`, `user_id`, `day`,`type`) PARTITION BY range (`day`) () DISTRIBUTED BY HASH(`mch_id`, `user_id`) BUCKETS AUTO PROPERTIES ( \u0026#34;replication_allocation\u0026#34; = \u0026#34;tag.location.default: 3\u0026#34;, \u0026#34;dynamic_partition.enable\u0026#34; = \u0026#34;true\u0026#34;, \u0026#34;dynamic_partition.time_unit\u0026#34; = \u0026#34;DAY\u0026#34;, \u0026#34;dynamic_partition.start\u0026#34; = \u0026#34;-90\u0026#34;, \u0026#34;dynamic_partition.end\u0026#34; = \u0026#34;1\u0026#34;, \u0026#34;dynamic_partition.prefix\u0026#34; = \u0026#34;a_daily_tbl_prefix\u0026#34;, \u0026#34;dynamic_partition.buckets\u0026#34;=\u0026#34;2\u0026#34;, \u0026#34;dynamic_partition.create_history_partition\u0026#34;=\u0026#34;true\u0026#34; ); 当某一天如 2024-12-31 这一天的数据想要插入未来的一条数据，执行如下 SQL 语句：\nINSERT INTO a_daily_tbl values (101, 101743183, \u0026#39;2025-01-02\u0026#39;, \u0026#39;type_1\u0026#39;, 1, 5000000, 5000000, \u0026#39;2025-01-02 02:40:16\u0026#39;); 提示错误：\nerrCode = 2, detailMessage = Insert has filtered data in strict mode. url: http://HOST:PORT/api/_load_error_log?file=__shard_3/error_log_insert_stmt_16be9eb83cde44be-a0ecc5687c9ead8e_16be9eb83cde44be_a0ecc5687c9ead8e # 链接中的内容如下： Reason: no partition for this tuple. tuple= +---------------+---------------+---------------+---------------+-----------------+-----------------+-----------------+----------------------+ |(Int32) |(Int64) |(DateV2) |(String) |(Nullable(Int32))|(Nullable(Int64))|(Nullable(Int64))|(Nullable(DateTimeV2))| +---------------+---------------+---------------+---------------+-----------------+-----------------+-----------------+----------------------+ | 101| 101743183| 2025-01-02| type_1| 1| 5000000| 5000000| 2025-01-02 02:40:16| +---------------+---------------+---------------+---------------+-----------------+-----------------+-----------------+----------------------+ . src line []; 文档查阅：\nDoris#Dynamic Partition Doris#Alter Table Property 根据文档，动态分区表的分区范围是在 dynamic_partition.start 和 dynamic_partition.end 之间的，如果插入的数据超出了这个范围，就会提示 \u0026ldquo;no partition for this tuple\u0026rdquo;。\ndynamic_partition.start\nThe starting offset of the dynamic partition, usually a negative number. Depending on the time_unit attribute, based on the current day (week / month), the partitions with a partition range before this offset will be deleted. If not filled, the default is -2147483648, that is, the history partition will not be deleted.\n代表动态分区的起始偏移量，通常是一个负数。根据 time_unit 属性的不同，基于当前的天（周/月），在这个偏移量之前的分区范围的分区将被删除。如果未填写，则默认为 -2147483648，即历史分区不会被删除。\ndynamic_partition.end\nThe end offset of the dynamic partition, usually a positive number. According to the difference of the time_unit attribute, the partition of the corresponding range is created in advance based on the current day (week / month).\n代表动态分区的结束偏移量，通常是一个正数。根据 time_unit 属性的不同，基于当前的天（周/月），提前创建对应范围的分区。\n处理手段：\n因此从 DDL 中可以看出，这个表的动态分区范围是从 -90 天到 1 天，因此当我们在 2024-12-31 这一天插入 2025-01-02 (cur + 2) 这一天的数据时，超出了动态分区的范围，因此提示分区不存在。\n那么解决办法要么就是调整插入数据的时间，要么就是调整动态分区的范围。\n调整动态分区的范围，将 dynamic_partition.end 调整为 2 天。\nALTER TABLE a_daily_tbl SET (\u0026#34;dynamic_partition.end\u0026#34; = \u0026#34;2\u0026#34;); 然后稍等片刻，确认分区已经创建 show partitions from a_daily_tbl，然后再插入数据就可以了。\n"},{"id":10,"href":"/2024/08/18/shardingsphere-proxy%E9%97%AE%E9%A2%98%E5%87%A0%E5%88%99/","title":"ShardingSphere-Proxy问题几则","section":"Posts","content":"ShardingSphere Proxy 是 Apache ShardingSphere 的一个子项目，是一个基于 MySQL 协议的数据库中间件，用于实现分库分表、读写分离等功能。在使用过程中，遇到了一些问题，记录如下。\n这里主要针对的是 分库分表 的使用场景。\n问题概述 # 数据库往往是一个系统最容易出现瓶颈的点，当遇到数据库瓶颈时，我们可以通过数据拆分来缓解问题。数据拆分的方式通常分为横向拆分和纵向拆分，横向拆分即分库分表；纵向拆分即把一个库表中的字段拆分到不同的库表中去。这两种手段并不互斥，而是在实际情况中相辅相成。本文即是横向拆分相关内容。\n常见的部署方式有哪些？ 数据分片规则怎么配置？ 数据分片数应该怎么确定？ 数据分片后唯一索引还有用吗？ 数据分片后数据迁移？ 数据分片后如何确定实际执行 SQL 语句？ 数据分片后的查询优化？ 0. 常见的部署方式 # 官方提供了两种部署方式：\n单机部署：将 ShardingSphere Proxy 部署在单台服务器上，用于测试和开发环境。 集群部署：将 ShardingSphere Proxy 部署在多台服务器上，用于生产环境。集群模式下使用 zookeeper 来存储元数据。 关于元数据，元数据是 ShardingSphere Proxy 的核心，用于存储分库分表规则、读写分离规则等信息。 官方建议使用集群模式部署 生产环境的 ShardingSphere Proxy\n如果不按照官方的指引，选择部署了多个 Standalone 模式的 ShardingSphere Proxy，那么需要注意“每个这样的 proxy 节点会有自己的元信息，他们之间并不互通”。在这些情况下会出现节点之间元数据不一致的问题，参看如下测试：\n# 启动 3 个 standalone 模式的 ShardingSphere Proxy +-------+ | LB | +-------+ | |-------------|--------------| | | | +-------+ +-------+ +-------+ | Node1 | | Node2 | | Node3 | +-------+ +-------+ +-------+ 初始表结构如下：\nCREATE TABLE t_user ( id BIGINT NOT NULL comment \u0026#39;主键\u0026#39;, /* 主键 */ user_id BIGINT NOT NULL unique comment \u0026#39;用户ID\u0026#39;, /* 用户ID, 分表 key */ mch_id BIGINT NOT NULL comment \u0026#39;商户ID\u0026#39;, /* 商户ID, 分库 key */ PRIMARY KEY (user_id) ) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4; 然后在任意一个 standalone 模式的 ShardingSphere Proxy 执行更新表结构的 SQL 语句：\nALTER TABLE t_user ADD COLUMN name VARCHAR(255) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;姓名\u0026#39;; 这时候在另外一个 standalone 模式的 ShardingSphere Proxy 查询表结构 和 查询数据的语句：\ndesc t_user; select * from t_user; 会发现表结构描述已经是最新的，但是实际查询数据时，没有新增的字段，如下图：\n这就是元数据不一致的问题，所以在生产环境中，建议使用集群模式部署 ShardingSphere Proxy。当然我们可以通过手动同步元数据的方式来解决这个问题，参考如下：\nREFRESH TABLE METADATA 这样操作后再查询数据，就能拿到新增的字段了, 如下图：\n1. 如何配置分库分表规则？算法有哪些？ # 规则配置参考：https://shardingsphere.apache.org/document/current/cn/user-manual/shardingsphere-jdbc/yaml-config/rules/sharding/\n这个问题很好理解，分库分表的规则配置是整个数据分片的核心，也是最重要的一环。在配置文件中，可以通过 shardingAlgorithms 配置项指定分库分表算法，如下：\n- !SHARDING tables: # 分片表配置 t_user: # 逻辑表名 actualDataNodes: ds_${0..1}.t_user_${0..1} # 实际数据节点 databaseStrategy: # 分库策略 standard: # 用于单分片键的分片策略 shardingColumn: mch_id # 分片键 shardingAlgorithmName: database_inline # 分库算法 tableStrategy: # 分表策略 standard: # 用于单分片键的分片策略 shardingColumn: user_id # 分片键 shardingAlgorithmName: t_user_inline # 分表算法 shardingAlgorithms: # 分片算法配置 database_inline: # 算法1 - 分库算法 props: algorithm-expression: ds_${mch_id % 2} # 分库算法表达式 t_user_inline: # 算法2 - 分表算法 props: algorithm-expression: t_user_${user_id % 2} # 分表算法表达式 在这个例子中，我们配置了一个 t_user 表，使用 standard 算法，分库分表的规则如下：\n分库规则：根据 mch_id 字段进行分库，分库算法为 database_inline，分库算法表达式为 ds_${mch_id % 2}。 分表规则：根据 user_id 字段进行分表，分表算法为 t_user_inline，分表算法表达式为 t_user_${user_id % 2}。 这里的分库分表算法都是基于 “取模” 的算法，这是一种简单的分片算法，适用于大部分场景。ShardingSphere Proxy 支持的分片算法如下：\nhttps://shardingsphere.apache.org/document/current/cn/user-manual/common-config/builtin-algorithm/sharding/\n自动分片算法\n需要注意的是，自动分片算法的分片逻辑由 ShardingSphere 自动管理，需要通过配置 autoTables 分片规则进行使用。\n自动分片算法又分为：\n取模分片算法（MOD）：根据分片键（int）对分片数取模，然后路由到对应的分片上执行。\nHash取模分片算法（HASH_MOD）：根据分片键（int）进行 Hash 计算，然后对分片数取模，然后路由到对应的分片上执行。\n基于分片容量的范围分片算法（VOLUME_RANGE）：配置分片键的范围上限（range-upper）和下限（range-lower），同时还需要配置分片容量（sharding-volume）。适用于数据增长趋势相对均匀，按分片容量将数据均匀地分布到不同的分片表中，可以有效避免数据倾斜问题；由于数据已经被按照范围进行分片，支持频繁进行范围查询场景。\n基于分片边界的范围分片算法（BOUNDARY_RANGE）：根据数据的取值范围进行分片，特别适合按数值范围频繁查询的场景，比如数据中如果包含 date: 202408 这种字段，可以按照 date 字段进行分片，在查询时可以直接根据 date 字段的范围进行查询。\n自动时间段分片算法（AUTO_INTERVAL）：配置分片的起始时间范围（datetime-lower）和结束时间范围（datetime-upper），同时还需要配置单一分片能承载的最大时间（sharding-seconds）。如下配置：\nshardingAlgorithms: t_order_auto_interval: props: datetime-lower: 2024-08-01 00:00:00 datetime-upper: 2029-08-31 23:59:59 sharding-seconds: 31536000 # 1年 该配置表示，将数据按照时间范围进行分片，存储从 2024-08-01 到 2029-08-31 的数据，每个分片最多存储 1 年的数据。\n标准分片算法\nShardingSphere Proxy 实现的标准分片算法有：\n行表达式分片算法（INLINE）：提供对 SQL 中的 = 和 IN 运算符的分片操作支持，只支持单分片键。\n时间范围分片算法（INTERVAL）：针对于时间字段（字符串类型）作为分片健的范围分片算法，适用于按照天、月、年这种固定区间的数据分片。举例如下：\nshardingAlgorithms: t_order_interval: props: datetime-pattern: \u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34; # 分片字段格式 datetime-lower: \u0026#34;2024-01-01 00:00:00\u0026#34; # 范围下限 datetime-upper: \u0026#34;2024-06-30 23:59:59\u0026#34; # 范围上限 sharding-suffix-pattern: \u0026#34;yyyyMM\u0026#34; # 分片名后缀，可以是MM，yyyyMMdd等。 datetime-interval-amount: 1 # 分片间隔，这里指一个月 datetime-interval-unit: \u0026#34;MONTHS\u0026#34; # 分片间隔单位 这里的配置表示，将数据按照时间范围进行分片，存储从 2024-01-01 到 2024-06-30 的数据，每个分片最多存储 1 个月的数据。数据分片后的表名后缀为 yyyyMM，即每个月一个分片。\n复合分片算法\n复合行表达式分片算法（COMPLEX_INLINE）：这个也是类似于 INLINE 算法，但是支持多分片键的场景。比如在单分片键的情况下，我们使用 t_user.user_id 来进行分片，现在我们需要使用 t_user.mch_id 和 t_user.user_id 来进行分片，这时候就可以使用 COMPLEX_INLINE 算法。如下：\n- !SHARDING tables: t_user: actualDataNodes: ds_${0..1}.t_user_${0..1} databaseStrategy: complex: shardingColumns: mch_id, user_id shardingAlgorithmName: complex_inline shardingAlgorithms: complex_inline: props: algorithm-expression: ds_${mch_id + user_id % 2} Hint 分片算法\nHint 行表达式分片算法（HINT_INLINE）：类似于 INLINE 算法，但是支持不依赖于 SQL 的分片键场景。java 可以直接使用 hint API 进行使用，其他语言可以通过 SQL 注释进行使用。如下：\nHintManager hintManager = HintManager.getInstance(); hintManager.addDatabaseShardingValue(\u0026#34;t_order\u0026#34;, 1); hintManager.addTableShardingValue(\u0026#34;t_order\u0026#34;, 1); /* SHARDINGSPHERE_HINT: {key} = {value}, {key} = {value} */ SELECT * FROM t_user WHERE id = 1; 可以使用 keys 参见 https://shardingsphere.apache.org/document/5.5.0/cn/user-manual/common-config/sql-hint/\n注意如果使用的是 shardingsphere-proxy 代理，需要在配置文件中开启 sqlCommentParseEnabled 开关。\nprops: sql-show: true sqlParser: sqlCommentParseEnabled: true 自定义分片算法\n自定义分片算法（CLASS_BASED）：用户自定义分片算法。 一般情况下，我们可以根据业务场景选择合适的分片算法，如果没有特殊需求，可以使用自动分片算法，这样可以减少配置的复杂度。\n2. 分片数如何确定？ # 说到数据分片数，那么必须要问一下为什么要进行数据分片，不分片不行吗？大部分业务可能都用不上分库分表，考虑分库分表时我们面临的场景大部分都是：“单表数据超过一个界限导致性能受到严重影响”，这个界限现在说法比较多的是 500w - 2000w，但这并不是铁律，还是要以实际情况为准。\n在决定分片数时候，有几个考虑因素：\n数据量：根据数据量的大小来决定分片数。确保分片数能够降低单表的性能压力，同时为未来预留一定的增长空间。 分片键的选择：选择合适的分片键非常重要，分片键的选择会直接影响到分片数。譬如说一个分片键的取值只有 A 和 B，那我们设置超过2个分片数也没有太大的意义。 分片算法：选择 mod 和 range 两种分片算法时，分片数也会不一样。譬如，业务中根据时间分片，那么需要考虑是什么时间范围，按月还是年？而使用 mod 分片则需要考虑数据量和分片键。 维护成本：分片数越多，需要的资源和维护成本就越高。 这个当然需要结合具体情况具体分析，上述只是一个简单的分析办法，实际情况中我们还要考虑数据倾斜、数据增长和部署方式等因素。\n除掉上述的因素考虑外，选择的数值尽量是 2 的幂次。免责条款：纯个人经验，不保真\n3. 分库分表后唯一索引的生效范围如何？ # 分库分表后，唯一索引的生效范围是一个比较棘手的问题。在分库分表的场景下，唯一索引的生效范围是在单个分片内，而不是整个分片集群内。这就意味着，如果要保证唯一索引的唯一性，需要在应用层进行处理。\n举个例子：\nCREATE TABLE `t_order` ( `order_id` BIGINT(20) NOT NULL AUTO_INCREMENT, `user_id` INT(11) NOT NULL, `status` VARCHAR(45) NOT NULL, PRIMARY KEY (`order_id`), UNIQUE KEY `uk_order_id` (`order_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; 在这个例子中，order_id 字段是唯一索引，但是在分库分表的场景下，唯一索引的生效范围是在单个分片内，而不是整个分片集群内。这就意味着，如果要保证唯一索引的唯一性，需要在应用层进行处理。\n如下伪代码：\n// 生成订单号 String orderId = generateOrderId(); // 根据订单号查询订单 Order order = orderService.findByOrderId(orderId); if (order != null) { // 订单号已存在 throw new OrderIdExistException(); } // 保存订单 orderService.save(order); 在这个例子中，通过在应用层进行处理，可以保证唯一索引的唯一性。当然这里会存在并发问题，同样需要在应用层进行处理。可以考虑其他的解决方案，如分布式锁等，这里不再赘述。\n这里还引申出来另一个问题：分库分表后怎么生成全局唯一ID？\n这个问题在 ShardingSphere 中可以通过设置 snowflake 算法来生成主键ID (具体参考官方文档说明配置)，如下：\n# 设置表的主键生成策略 your_table: keyGenerateStrategy: column: id keyGeneratorName: snowflake # 设置主键生成器 keyGenerators: snowflake: type: SNOWFLAKE props: worker-id: 1 # 注意，如果有多台机器，这里需要设置不同的值。如果不理解，请熟读雪花算法 4. 分库分表后如何迁移数据？ # 虽然在数据分片时已经提前预估了数据容量，但是难免会有数据增长超出预期，或者数据意外倾斜导致单个/某个数据分片数据量超出承载能力，这时候往往通过数据迁移来解决这个问题。\n有些情况可以通过删除无效数据来解决。譬如一些行为数据，历史数据 等等。\nShardingSphere-Scaling 提供了数据迁移的功能，目前处于实验室阶段，可以用于数据迁移。参考：https://shardingsphere.apache.org/document/5.1.0/cn/user-manual/shardingsphere-scaling/\n除此之外，就是互联网中常用的迁移数据的方式，如：\n停机迁移：停止服务，将老数据依次应用新的分片规则写入到新的数据源中；执行切换；启动服务。 不停机迁移： 编写迁移程序，将历史数据迁移到新的分片上； 更新业务应用，添加双写/读写切换 功能。打开双写， 数据分片后迁移会非常麻烦，建议在设计分片规则时，尽量考虑到未来的数据增长情况，避免数据迁移（适当的增加分片数,分片键避免产生数据倾斜）。\n5. 如何确定实际执行的SQL？ # ShardingSphere Proxy 提供了 DistSQL 操作语言，分为：RDL、RQL、RAL 和 RUL，描述如下：\n操作语言 描述 RDL Resource \u0026amp; Rule Definition Language，负责资源和规则的创建、修改和删除。 RQL Resource \u0026amp; Rule Query Language，负责资源和规则的查询和展现。 RAL Resource \u0026amp; Rule Administration Language，负责强制路由、熔断、配置导入导出、数据迁移控制等管理功能。 RUL Resource \u0026amp; Rule Utility Language，负责 SQL 解析、SQL 格式化、执行计划预览等功能。 如果我们要确定实际执行的 SQL 语句，可以通过 RUL 操作语言来实现，如下：\n-- 预览执行计划 PREVIEW SELECT * FROM t_user WHERE user_id = 1; 这里我们分别试验下，不同的查询条件的实际执行有什么异同。\nCREATE TABLE t_user ( id BIGINT NOT NULL comment \u0026#39;主键\u0026#39;, /* 主键 */ user_id BIGINT NOT NULL unique comment \u0026#39;用户ID\u0026#39;, /* 用户ID, 分表 key */ mch_id BIGINT NOT NULL comment \u0026#39;商户ID\u0026#39;, /* 商户ID, 分库 key */ PRIMARY KEY (user_id) ) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4; 5.1. 查询条件不是分片键 # 可以发现当查询条件中没有包含任何的分片键时，ShardingSphere Proxy 会将 SQL 语句发送到所有的分片节点（分库 \u0026amp; 分表）上执行，然后将结果合并返回。\n5.2. 查询条件是分片键（分库键） # 当查询条件中包含分片键（分库键）时，ShardingSphere Proxy 会根据分片键的值，将 SQL 语句直接路由到对应的分库上执行，但是仍然要聚合分表的结果。\n5.3. 查询条件是分片键（分表键） # 当查询条件中包含分片键（分表键）时，ShardingSphere Proxy 会根据分片键的值，还需要将 SQL 语句路由到所有的分库上执行，但是分表已经确定所以不需要聚合分表的结果。\n5.4. 查询条件是分片键（分库键和分表键） # 当查询条件中包含分片键（分库键和分表键）时，ShardingSphere Proxy 会根据分片键的值，已经能准确的确定到分库和分表，所以直接将 SQL 语句路由特定的数据分片上执行。\n从上述测试结果，测试了 RUL 中的 PREVIEW 语句，可以看到 ShardingSphere Proxy 在执行 SQL 时，会根据 SQL 中的查询条件，将 SQL 语句路由到特定的数据分片上执行，这样可以减少不必要的数据传输和计算，提高查询效率。同时也提醒我们在使用分库分表时，需要选择合理的分片键，以提高查询效率。在编写 SQL 时，也要尽量包含分片键，以减少不必要的数据传输和计算。\n6. 分库分表后怎么优化查询？ # 想要优化 Shardingsphere Proxy 的查询，首先要了解 Shardingsphere Proxy 的查询过程。Shardingsphere Proxy 的查询过程如下：\n客户端发送 SQL 语句到 Shardingsphere Proxy。 Shardingsphere Proxy 解析 SQL 语句，根据 SQL 语句中的分片键，将 SQL 语句路由到特定的数据分片上执行。 数据分片执行 SQL 语句，返回结果。 Shardingsphere Proxy 将结果聚合返回给客户端。 其中第二步是最重要的一步，Shardingsphere Proxy 会根据 SQL 语句中的分片键，将 SQL 语句路由到特定的数据分片上执行。通过前面五个问题，我们也已经知道了使用 RUL 去分析 SQL 的实际执行计划，这样我们就可以知道 SQL 语句是如何执行的。我们能发现分片键在查询中会直接影响到 SQL 的执行计划，所以我们可以通过合理的使用分片键来优化查询。\n6.1 Shardingsphere Proxy 路由策略 # 这里针对 Shardingsphere Proxy 路由引擎展开一下，我们从第五个问题只能知道带不带分片键会影响到 SQL 的执行计划，但是具体的路由引擎是怎么路由的呢？参见下图：\nhttps://shardingsphere.apache.org/document/5.1.1/cn/reference/sharding/route/\n路由策略 分类 描述 直接路由 分片路由 直接路由到特定的数据分片上执行。如：hintManager.setDatabaseShardingValue(3); 标准路由 分片路由 不包含关联查询或仅包含绑定表之间关联查询的 SQL，当分片运算符是等于号时，路由结果将落入单库（表），当分片运算符是 BETWEEN 或 IN 时，则路由结果不一定落入唯一的库（表）。 笛卡尔路由 分片路由 无法定位分片规则，表之间的关联查询需要拆解为笛卡尔积组合执行。如后文中的 order 和 order_item 表的关联查询 。 全库路由 广播路由 全库路由用于处理对数据库的操作，包括用于库设置的 SET 类型的数据库管理命令，以及 TCL 这样的事务控制语句。如：SET autocommit=0; 全库表路由 广播路由 全库表路由用于处理对数据库中与其逻辑表相关的所有真实表的操作，主要包括不带分片键的 DQL 和 DML，以及 DDL 等。如 SELECT * FROM t_order WHERE good_prority IN (1, 10); 全实例路由 广播路由 全实例路由用于 DCL 操作，授权语句针对的是数据库的实例。例如：CREATE USER customer@127.0.0.1 identified BY '123'; 单播路由 广播路由 单播路由用于获取某一真实表信息的场景，它仅需要从任意库中的任意真实表中获取数据即可。例如：DESCRIBE t_order 阻断路由 广播路由 阻断路由用于屏蔽 SQL 对数据库的操作，例如：USE order_db; 不会在真实数据库中执行。 6.2 绑定表 # https://shardingsphere.apache.org/document/current/cn/features/sharding/concept/#%E7%BB%91%E5%AE%9A%E8%A1%A8\n指分片规则一致的一组分片表。 例如：t_order表和t_order_item表，均按照order_id分片，则此两张表互为绑定表关系。绑定表之间的多表关联查询不会出现笛卡尔积关联，关联查询效率将大大提升。举例说明，如果SQL为：\nSELECT i.* FROM t_order o JOIN t_order_item i ON o.order_id=i.order_id WHERE o.order_id in (10, 11); 在不配置绑定表关系时，假设分片键order_id将数值10路由至第0片，将数值11路由至第1片，那么路由后的SQL应该为4条，它们呈现为笛卡尔积：\nSELECT i.* FROM t_order_0 o JOIN t_order_item_0 i ON o.order_id=i.order_id WHERE o.order_id in (10, 11); SELECT i.* FROM t_order_0 o JOIN t_order_item_1 i ON o.order_id=i.order_id WHERE o.order_id in (10, 11); SELECT i.* FROM t_order_1 o JOIN t_order_item_0 i ON o.order_id=i.order_id WHERE o.order_id in (10, 11); SELECT i.* FROM t_order_1 o JOIN t_order_item_1 i ON o.order_id=i.order_id WHERE o.order_id in (10, 11); 如果配置了绑定表关系，那么路由后的SQL应该为2条，它们不再呈现为笛卡尔积：\nSELECT i.* FROM t_order_0 o JOIN t_order_item_0 i ON o.order_id=i.order_id WHERE o.order_id in (10, 11); SELECT i.* FROM t_order_1 o JOIN t_order_item_1 i ON o.order_id=i.order_id WHERE o.order_id in (10, 11); 绑定表配置需要在 ShardingSphere Proxy 的配置文件中配置，如下：\nrules: - !SHARDING tables: t_order: actualDataNodes: ds_${0..1}.t_order_${0..1} t_order_item: actualDataNodes: ds_${0..1}.t_order_item_${0..1} bindingTables: # 绑定表配置 - t_order, t_order_item 6.3 广播表 # https://shardingsphere.apache.org/document/5.4.1/cn/user-manual/shardingsphere-jdbc/yaml-config/rules/broadcast/\n指所有的分片数据源中都存在的表，表结构和表中的数据在每个数据库中均完全一致。适用于数据量不大且需要与海量数据的表进行关联查询的场景，例如：字典表。\n广播表配置需要在 ShardingSphere Proxy 的配置文件中配置，如下：\nrules: - !BROADCAST tables: # 广播表规则列表 - t_country - t_address 广播表有以下特点：\n插入、更新操作会实时在所有节点上执行，保持各个分片的数据一致性 查询操作，只从一个节点获取 可以跟任何一个表进行 JOIN 操作 6.4 总结 # 通过前面的分析，我们大致可以得出以下几个优化查询的方法：\n合理选择和使用分片键，让 SQL 能够被路由到特定的数据分片上执行，避免数据聚合和笛卡尔积。 可以通过配置绑定表来避免笛卡尔积。 同样的，合理的识别并设置广播表，可以提高查询效率（避免跨 库/实例）。 通过 DistSQL 的 PREVIEW 语句，可以预览 SQL 的执行计划，从而优化 SQL。 在编写针对数据分片的 SQL 时，虽然 ShardingSphere Proxy 能做到大部分场景的透明化，但我们还是需要清晰的了解其路由策略和执行方式，这样才能了然于心，针对性的编写 SQL，提高查询效率。\n总结 # 当我们选择分库分表的方案时，我们需要决定 分片的量，分片的算法和分片键，而这一切都基于我们对业务的理解和对分库分表的认知。而最终决定这一切的都是业务的需求。在决策时，我们往往需要考虑数据分片后是否会出现：分布不均匀，出现热点数据？ 分片算法扩展性不足，导致扩展时需要数据迁移？ 数据分片键选择不合理，不符合业务中的查询模式，影响查询性能？\n分库分表不是万能的，也不是完全无损的。分库之后一定会带来的问题是：事务；join 查询；排序；分页；group 分组；所以决定分片前一定要结合业务分析能不能分片。\n参考 # https://www.cnblogs.com/chengxy-nds/p/18097298 https://shardingsphere.apache.org/document/current/en/overview/ "},{"id":11,"href":"/2024/03/25/istio-idle-timeout%E9%97%AE%E9%A2%98%E5%A4%8D%E7%8E%B0%E5%92%8C%E8%A7%A3%E5%86%B3/","title":"Istio Idle Timeout问题复现和解决","section":"Posts","content":" 更新 # 更新 2024-04-01 # 通过调整 tcp_proxy 的 idle_timeout 参数后，部分中间件（redis, mongo）的异常问题不再出现，但是 mysql(sharding-spere) 和 memcached 仍然存在 \u0026ldquo;invalid connection\u0026rdquo; 错误，所以还需要找到能够解决 mysql(sharding-spere) 和 memcached 的方法。\n上述描述的现象非常主观，不一定正确也不能作为最终结论，但是 idle_timeout 配置确实没有解决所有的问题。\n这里，需要知道 istio 在 inject 时会通过 iptables 对应用的流量进行劫持，对于 outbound 的流量 iptables 规则拦截转发到 OUTPUT 链。OUTPUT 的链转发流量到 ISTIO_OUTPUT，这个链会决定服务访问外部服务的流量发往何处。\n这样产生的效果是，除了应用自己建立的连接之外 envoy 也会创建一个代理连接。\n$ netstat -notp | grep 3307 tcp 0 172.23.105.25:41030 x.x.x.x:3307 ESTABLISHED - off(0.00/0/0) tcp 0 172.23.105.25:41022 x.x.x.x:3307 ESTABLISHED 1/./app keepalive(0.88/0/0) 我遇到的问题可以确定问题就出在 envoy 建立的连接上，因为应用自己建立的连接是正常的，同时还可以看到应用自己的连接开启了 keepalive, 而 envoy 建立的连接没有开启。这样可能会出现这个连接会因为超时而被关闭的情况，或者其他原因导致连接被释放。那如果可以避免将中间件的流量通过 envoy 代理，这样就可以避免这个问题。\n在官方的文档中也提到，https://istio.io/latest/zh/docs/tasks/traffic-management/egress/egress-control/ 对于外部服务，可以通过配置 excludeOutboundPorts 或者 excludeOutboundIPRanges 来使得某些服务的流量不经过 istio sidecar。其背后的原理是通过 iptables 的规则中排除这些端口或者 IP 地址。\n经过实验，发现这个方法可以 解决（绕过）这个问题。还是没有定位到 envoy 代理过程中发生了什么导致连接被关闭的原因，这个问题还需要进一步的分析。\n问题描述 # 在 kubernetes 集群中使用 istio 时，开发同学反馈经常会出现 “invalid connection” 错误，导致业务逻辑错误，而在没有使用 istio 的情况下，这个问题并不会出现。通过查看日志，发现这种错误常发生在集群外的服务连接，如：memcached、redis, mysql, mongodb 等。\n通过上述的描述，首先怀疑的是 istio 的 sidecar 代理导致的问题，因为这种错误只有在使用 istio 时才会出现。\nIstio 和 envoy 的基本概念 # Istio 服务网格从逻辑上分为数据面和控制面，其中数据面由 Envoy 代理组成，控制面由 Pilot、Citadel、Galley 等组件组成。数据面是由 Envoy 代理组成的，负责实际的流量代理和控制。\n也就是说，当我们在 kubernetes 集群中部署了 istio 时，每个 pod 都会有一个 sidecar 容器，这个容器中就包含了 Envoy 代理，会劫持 pod 中的所有流量。再回到上面的问题，在没有这个代理的情况下，这个问题并不会出现，所以这个问题很有可能是由 Envoy 代理导致的。\n猜测问题原因 # 数据库连接超时 # 这里使用的开发语言为 go，\u0026ldquo;invalid connection\u0026rdquo; 错误往往发生在“连接超时”的情况下（连接被服务端单方面关闭，而客户端还在使用）。在数据库连接场景中，往往还会有“连接池”这个概念，连接池会在连接空闲一段时间后关闭连接，与此同时服务端也会设置相应的超时时间，当连接空闲时间超过这个时间时，服务端会主动关闭连接。\n如：\nmysql 的 wait_timeout 参数，当连接空闲时间超过这个时间时，服务端会主动关闭连接。 mongodb 的 maxIdleTimeMS 参数，连接在池中可保持空闲状态的最大毫秒数，超过这个时间后，连接会被删除或关闭。 在外部没有代理的情况时，如果出现 “invalid connection” 错误，很有可能是由于连接超时导致的。可以检查下客户端和服务端的连接超时时间是否合理设置了（服务端的超时时间要大于客户端的超时时间）。\nenvoy 关闭连接 # 根据上面的描述，我们可以猜测这个问题是由于 Envoy 代理产生的，那么经过在网上查找资料，发现这个问题可能是由于 Envoy 的 tcp_idle_timeout 导致的。\nhttps://github.com/istio/istio/issues/24387\nhttps://www.envoyproxy.io/docs/envoy/latest/faq/configuration/timeouts#tcp\nThe TCP proxy idle_timeout is the amount of time that the TCP proxy will allow a connection to exist with no upstream or downstream activity. The default idle timeout if not otherwise specified is 1 hour.\nTCP 代理空闲超时是 TCP 代理允许连接在没有上游或下游活动的情况下存在的时间量。如果没有另外指定，默认空闲超时为 1 小时。\nhttps://stackoverflow.com/questions/63843610/istio-proxy-closing-long-running-tcp-connection-after-1-hour\n那是不是 enovy 关闭了连接，导致了这个问题呢？下面，我们将通过设计一个简单的复现场景来验证这个问题。\n设计复现 # 准备一个 k8s 集群 ,并安装好 istio 在集群外部署一个 redis 服务 设置一个较小的 tcp idle timeout 参数（10s），便于观察 在集群内部署一个两个 POD, 一个接入 istio，一个不接入 istio（两个 POD 都通过 telnet 连接 redis 服务） 调整 istio 的 tcp_idle_timeout 参数，观察连接情况 使用到的相关文件参见 https://github.com/yeqown/playground/tree/master/k8s/istio-idle-timeout\nEnovyFilter 修改 tcp_idle_timeout 参数 kubectl apply -f envoyfilter-10s.yaml 部署 POD kubectl create ns istio-idle-timeout \u0026amp;\u0026amp; kubectl label ns istio-idle-timeout istio-injection=enabled # 部署没有 istio 的 POD kubectl apply -f deployment.yaml -n default # 部署有 istio 的 POD kubectl apply -f deployment.yaml -n istio-idle-timeout 连接外部 redis 服务 在本地通过 minikube 启动 k8s 集群，并且在本机部署一个 redis 服务。\ntime telnet host.minikube.internal 3306 观察并配置调整 # 没有 istio sidecar 的 POD, 连接不会断开。 有 istio sidecar 的 POD, 10s 后连接会断开。 / # time telnet 192.168.105.1 6379 Connected to 192.168.105.1 Connection closed by foreign host Command exited with non-zero status 1 real\t0m 10.00s user\t0m 0.00s sys\t0m 0.00s 清除 idle_timeout 参数 # kubectl delete -f envoyfilter-remove.yaml 重新连接到 redis ，观察连接情况。\n/ # time telnet 192.168.105.1 6379 Connected to 192.168.105.1 Connection closed by foreign host Command exited with non-zero status 1 real\t1h 0m 00s user\t0m 0.00s sys\t0m 0.00s 总结 # enovy 的 tcp_idle_timeout 参数默认为 1h，这个参数会导致一些连接超时的问题，可以通过修改 istio EnvoyFilter 来调整这个参数。\napiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: idle-timeout namespace: istio-system spec: configPatches: - applyTo: NETWORK_FILTER match: context: SIDECAR_OUTBOUND listener: filterChain: filter: name: envoy.filters.network.tcp_proxy patch: operation: MERGE value: name: envoy.filters.network.tcp_proxy typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy idle_timeout: 10s 需要注意的是，这里的 envov filter 是作用在 sidecar 的 outbound 上的，所以只会影响 sidecar 代理的连接。但是 envoy 还会作为 ingress 和 egress 的代理，这个参数不会影响到这两个场景。\n解决 # 知道 envoy 的 tcp_idle_timeout 参数的影响后，那么可以通过调大这个参数来解决这个问题。比如设置为比客户端超时时间更大的值，比服务端超时时间更大的值。\n其他 # 可能相关的日志输出 # 调整日志级别 curl -X POST http://127.0.0.1:15000/logging?level=debug 通过日志发现，连接在 10s 后断开时伴随着 invoking idle callbacks 的日志输出。如下：(前后一共测试了4次)\n➜ istio-1.19.3 klf istio-idle-timeout-demo-5b4894b67d-tdpb5 -n istio-idle-timeout -c istio-proxy | grep \u0026#34;invoking idle callbacks\u0026#34; 2024-03-01T07:22:21.481180Z\tdebug\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:454\tinvoking idle callbacks - is_draining_for_deletion_=false\tthread=22 2024-03-01T07:22:21.481310Z\tdebug\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:454\tinvoking idle callbacks - is_draining_for_deletion_=false\tthread=22 2024-03-01T07:24:03.947896Z\tdebug\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:454\tinvoking idle callbacks - is_draining_for_deletion_=false\tthread=23 2024-03-01T07:24:03.947903Z\tdebug\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:454\tinvoking idle callbacks - is_draining_for_deletion_=false\tthread=23 2024-03-01T07:25:04.884340Z\tdebug\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:454\tinvoking idle callbacks - is_draining_for_deletion_=false\tthread=22 2024-03-01T07:25:04.884394Z\tdebug\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:454\tinvoking idle callbacks - is_draining_for_deletion_=false\tthread=22 2024-03-01T07:25:17.629907Z\tdebug\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:454\tinvoking idle callbacks - is_draining_for_deletion_=false\tthread=22 2024-03-01T07:25:17.629912Z\tdebug\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:454\tinvoking idle callbacks - is_draining_for_deletion_=false\tthread=22 应用 envoy filter 超时设置后，已经建立的连接不会应用新的超时设置，只有新的连接才会应用新的超时时间？ EnovyFilter 设置超时后，可以通过设置为空来清除超时设置。 patch: operation: MERGE value: name: envoy.filters.network.tcp_proxy typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy idle_timeout: "},{"id":12,"href":"/2024/02/26/nats%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/","title":"Nats设计与实现","section":"Posts","content":" NATS设计与实现 # https://github.com/nats-io/nats-sevrer\nNATS 就是一个消息中间件，提供了 Pub/Sub 核心数据流，并基于此构建了 Request/Reply API 和 JetStream 用来提供可靠的分布式存储能力，和更高的 QoS（至少一次 + ACK）。\n1. 核心概念 # 序号 名词（ENG） 名词（zh-CN） 解释 1 Publish 发布 发布动作，往 subject 中投递一条消息\n2 Subscribe 订阅 订阅动作，表示想要接受发布相应 subject 的消息 3 Subject 主题 唯一标识，用于标识一种或者一类事件的概念。订阅时，可以使用 nats 约定的通匹配符来接收一类 subjects，如 orders.\u0026gt; 。 4 Core Nats NATS 核心 CORE NATS 提供了以下能力：\n- PUB / SUB https://docs.nats.io/nats-concepts/core-nats/pubsub\n- Request / Reply https://docs.nats.io/nats-concepts/core-nats/reqreply\n- Queue Groups https://docs.nats.io/nats-concepts/core-nats/queue\n提供 最多一次 的消息传递保证 5 Request / Reply 请求 / 回复 NATS 基于 PUB / SUB 实现的对 request 异步回复的功能，依赖于消息中的 reply 字段。reply 是内置实现，随机生成一个 “inbox” subject。\n订阅者中间也是存在 Queue Group 概念的。\n注意：发起 request 如果没有reply，那么服务端会返回 No Responders 消息。 6 Queue groups 队列组 订阅者使用同一个队列名称，它们就会成为一个队列组。每次队列组接收到消息时，只有队列组中_随机选择_的一个订阅者会消费一条消息，内置的负载均衡。 7 Message 消息 or 事件 一条消息包含了以下内容：\n- Subject.\n- A payload in the form of a byte array.\n- Any number of header fields.\n- An optional \u0026lsquo;reply\u0026rsquo; address field. 8 Subject wildcards\nhttps://docs.nats.io/nats-concepts/subjects#wildcards 主题通配符 订阅者可以使用这些通配符_通过单个订阅收听多个主题。_\n注意：但发布者将始终使用完全指定的主题，而不使用通配符。 9 JetStream\nhttps://docs.nats.io/nats-concepts/jetstream 无 / 流 NATS 中一个功能特性，它是一个内置的分布式存储，在 CORE nats 的基础上扩展了更多的功能和更高的 QoS。功能上：\n- Stream：将 NATS 消息存储在流中，提供多种保留策略、限制、丢弃策略和主题映射功能。\n- Consumer: 让客户端应用订阅或拉取流中的消息，支持多种重放策略、确认机制和流控功能\n- Persistence: 将流的数据复制到多个 NATS 服务器，提供容错能力和加密存储功能\n- KV Store: 可以将消息与键关联，提供存储、检索、删除和监听键值变化的功能。\n- Object Store: JetStream 可以存储任意大小的对象（如文件），提供分块、校验和、元数据等功能。\n其中最核心（开发常用）的概念就是：stream 和 consumer。 10 Stream\nhttps://docs.nats.io/nats-concepts/jetstream/streams 流 流即是消息存储，它定义了消息的存储方式以及保留的限制。\n更具体的内容参见 #5.1 Stream 配置解析 11 Consumer 消费者 消费者作为客户端的接口，使用存储在流中的消息，跟踪客户端传递和确认的消息。Nats 同时支持 pull 和 push 两种消费模式；consumer 还提供了 durable 配置，用于持久化 consumer 消费信息（除非设置了 InactiveThreshold）\n更具体的内容参见 #5.2 Consumer 配置解析 12 Replay / Redelivery 13 Raft Group\nhttps://docs.nats.io/running-a-nats-service/configuration/clustering/jetstream_clustering RAFT 组 对于特定内容达成一致的分组，nats 中有 meta, stream, consumer 几种组。\nMeta: 全部节点都是组成员。负责：JetStream API + 集群管理。\nStream: (根据 replicas 配置选择组内的服务器成员)。负责：stream 数据\nConsumer: （根据 stream group的成员来确定消费者组内的成员）。负责：消费者状态\n14 KV Store 键值存储 15 Object Store 对象存储 2. 软件架构（集群模式） # Nats 自身是提供了多种运行方式:\n单机 只运行了一个 nats 实例。\n普通集群： 运行了 3 / 5 个实例，其中一个为集群 leader，此时集群如下：\n超级集群： 存在多个集群，集群之间可以通过 网关 来传播消息_。_网关提供了三种传播机制：这里 A / B 分别代表两个集群。\nOptimistic Mode 乐观模式 当 A 中的发布者发布“foo”时，A 网关将检查集群 B 是否已注册对“foo”没有兴趣。如果没有，则将“foo”转发给B。如果B收到“foo”后，在“foo”上没有订阅者，则B将向A发送一条网关协议消息，表示它对“foo”没有兴趣，从而阻止将来的消息关于“foo”被转发。\n当后续 B 中有订阅者关注 \u0026ldquo;foo\u0026rdquo;, 那么 B 会发送一条网关协议，用来取消对 \u0026ldquo;foo\u0026rdquo; 没兴趣的设置。\nInterest-only Mode 兴趣（关注）模式 当 A 上的网关发送许多关于 B 不感兴趣的各种主题的消息时。 B 发送一条网关协议消息，要求 A 乐观地停止发送，如果已知对该主题感兴趣，则发送。随着 B 上的订阅不断增多，B 将向 A 更新其主题兴趣。\nQueue Subscriptions 队列订阅模式 服务器将始终首先尝试为本地队列订阅者提供服务，并且仅在未找到本地队列订阅者时进行故障转移。服务器将选择 RTT 最低的集群。\n叶子结点/集群（代理模式）：\n透明地从本地客户端路由消息到一个或多个远程 NATS 系统。叶子结点采用本地的认证进行认证，连接远程 NATS 结点时，采用远程 NATS 系统的认证。通常用来降低 local 服务的延迟和流量。 注意：如果集群中一个节点配置为叶子结点，那么其余结点也要配置为叶子结点。\n一般场景下最常用的就是集群模式。\nQs:\n运维时怎么保证集群各节点的负载均衡？怎么保证集群的高可用？\n3. Nats 客户端协议 # https://docs.nats.io/reference/reference-protocols/nats-protocol\nNats 协议是一个_文本协议_ 这意味着，我们通过 telnet 就可以与之交互，通过抓包工具也可以很轻松的分析客户端和服务器之间的交互行为\nOp 操作 发送方 操作场景描述 注意事项 INFO 服务器 当客户端建立连接时，或者服务器集群拓扑发生变化时，服务器发送自身的信息，配置和安全要求给客户端 CONNECT 客户端 当客户端收到服务器的 INFO 消息后，客户端发送自身的信息和安全信息给服务器，以完成连接 verbose 字段默认为 false，表示服务器不会对每个消息回复 +OK PUB 客户端 当客户端想要发布一个消息给指定的主题时，客户端发送 PUB 消息，可选地提供一个回复主题 消息内容是可选的，如果没有内容，需要把内容大小设置为 0，并且仍然需要第二个 CRLF HPUB 客户端 和 PUB 相同，但是消息内容包含了 NATS 头部信息 消息内容是可选的，如果没有内容，需要把总消息大小设置为头部大小，并且仍然需要第二个 CRLF SUB 客户端 当客户端想要订阅一个主题时，客户端发送 SUB 消息，可选地加入一个分布式队列组 主题名称必须是合法的，不能包含空格或者分隔符 UNSUB 客户端 当客户端想要取消订阅一个主题时，客户端发送 UNSUB 消息，可选地指定一个消息数量，达到后自动取消订阅 MSG 服务器 当服务器向客户端发送一个应用消息时，服务器发送 MSG 消息，包含主题，sid，可选的回复主题和内容 HMSG 服务器 和 MSG 相同，但是消息内容包含了 NATS 头部信息 PING 服务器或客户端 当服务器或客户端想要检测对方是否存活时，发送 PING 消息 服务器会定期发送 PING 消息给客户端，如果客户端没有及时回复 PONG，服务器会断开连接 PONG 服务器或客户端 当服务器或客户端收到 PING 消息时，回复 PONG 消息 服务器会把正常的流量当作 PING/PONG 的代理，所以如果客户端有消息流动，可能不会收到服务器的 PING +OK 服务器 当服务器收到客户端的合法消息时，如果 verbose 字段为 true，服务器回复 +OK 消息 大多数客户端会把 verbose 字段设置为 false -ERR 服务器 当服务器遇到协议错误，授权错误，或者其他运行时错误时，服务器发送 -ERR 消息给客户端 大多数这些错误会导致服务器关闭连接，客户端需要异步处理这些错误 从表里没有看到 Request 对不对？那是因为 Request 是基于 Pub / Sub 实现的 API，因此不在通信协议中，属于应用层的功能。 另外也没有看到 JetStream 相关的 Op 对不对？翻下代码就可以发现，它是基于 Request 机制实现的 pub，但通过前面的介绍，我们已经知道 CORE Nats 的 Reply 其实是 Subscriber 回复的，而 JetStream 是提供可靠存储的 ACK 并不能依赖于 Consumer ，所以这里服务器一定是有特殊处理。我们在 #6.5 JetStream 消息的投递和消费 一节中再详细展开。\n4. NATS-Server 配置解析 # https://docs.nats.io/running-a-nats-service/configuration\nNats 端口列表\n默认端口号 作用 补充说明 443 WebSocket 协议端口，通过 websocket 来进行交互 1883 MQTT 协议支持 4222 NATS 自身端口 4111 叶节点允许本地客户端通过端口 4111 连接，且不需要任何认证 6222 集群路由监听端口 7222 网关监听端口 7422 叶子结点监听端口 8222 监控服务端口 5. JetStream 配置解析 # Nats 的 jetstream 是通过 raft 来解决分布式一致性问题，以此实现 stream 相关的功能。\n上图中展示了 jetstream 的一些关键点：\nStream 可以存储多个 subject的消息。 消费者有多种消费模式（pull/push），还可以过滤stream 中的 subject 进行消费。 消费有多种确认模式（ack） 5.1 Stream 配置清单 # https://docs.nats.io/nats-concepts/jetstream/streams#configuration\n我们当前使用的 nats 版本对应 2.9.21 下表配置中，Metadata，compression, FirstSeq 和 SubjectTransform 暂不可用。\n配置选项 描述 版本 可编辑 Name 流的名称，必须是唯一的 2.2.0 ❌ Storage 流的存储类型，可以是 File（默认）或 Memory 2.2.0 ❌ Subjects 流订阅的主题列表，可以使用通配符 2.2.0 ✅ Replicas 流的副本数量，必须大于等于 1 2.2.0 ✅ Retention 流的保留策略，决定了何时删除消息 2.2.0 ✅ MaxAge 流允许的最大消息存活时间，0 表示无限制 2.2.0 ✅ MaxBytes 流允许的最大字节数，-1 表示无限制 2.2.0 ✅ MaxMsgs 流允许的最大消息数量，-1 表示无限制 2.2.0 ✅ MaxMsgSize 流允许接收的最大消息体 2.2.0 ✅ MaxConsumers 流允许的最大消费者数量，-1 表示无限制 2.2.0 ❌ NoAck 流是否禁用确认机制，如果为 true，则不需要消费者确认消息。 2.2.0 ✅ Retention 声明流的保留策略 2.2.0 ❌ Discard 流达到限制时的丢弃策略，可以是 DiscardOld（默认）、DiscardNew 或 DiscardNewPerSubject 2.2.0 ✅ DuplicateWindow 流的去重窗口，用于检测和删除重复的消息，0 表示禁用去重 2.2.0 ✅ Placement 流的放置选项，可以指定集群和标签 2.2.0 ✅ Mirror 流的镜像选项，可以指定源流和过滤条件 2.2.0 已设置不可修改 Sources 流的源选项，可以指定一个或多个源流和过滤条件 2.2.0 ✅ MaxMsgsPerSubject 流允许的每个主题的最大消息数量，0 表示无限制 2.3.0 ✅ Description 描述信息 2.3.3 ✅ Sealed stream 封存，不允许删除。 2.6.2 可修改一次 DenyDelete 限制通过 API 从 stream 中删除消息。 2.6.2 ❌ DenyPurge 限制通过 API 从 stream 中清除消息。 2.6.2 ❌ AllowRollup 流是否允许滚动更新，如果为 true，则可以使用 Nats-Rollup 头部删除旧的消息 2.6.2 ✅ RePublish 存储在 stream 会马上重新往配置的 subject 发布消息 2.8.3 ✅ AllowDirect ？？？ ✅ MirroDirect ？？？ 2.9.0 ✅ DiscardNewPerSubject 如果为 true, 丢弃策略为 DiscardNew 时，则丢弃每个 subject 新来的消息。 2.9.0 ✅ Metadata ???? 2.10.0 ✅ Compression 文件存储压缩算法，s2 = snappy 2.10.0 ✅ FirstSeq 指定 stream 的初始序列号 2.10.0 ❌ SubjectTransform 流的主题转换选项，可以指定源主题和目标主题，用于在存储消息时修改主题 2.10.0 ✅ 5.2 Consumer 配置清单 # https://docs.nats.io/nats-concepts/jetstream/consumers\n5.2.1 通用配置 # 配置选项 描述 版本 是否可编辑 Durable 订阅绑定到使用者，直到被显示删除 2.2.0 ❌ FilterSubject 过滤 consumer 消费的主题，不能和 FilterSubjects 同时使用 2.2.0 ✅ AckPolicy 客户端确认策略：\nAckExplicit: 默认策略，每条消息单独确认。\nAckNone: 不缺人任何消息，服务器发送时即确认。\nAckAll：收到系列消息，只需要确认最后一条。 2.2.0 ❌ AckWait 一旦任何单个消息被传递给消费者，服务器将等待其确认的持续时间。如果没有及时收到确认，消息将被重新发送。 2.2.0 ✅ DeliverPolicy 消费者接受消息的策略：\nDeliverAll：默认策略，将从最早可用的消息开始消费\nDeliverLast: 开始消费后，消费加入到 stream 中的最新一条消息（如果过滤，则是匹配的最后一条）\nDeliverNew: 开始消费后，只会开始接收在消费者创建之后创建的消息。\nDeliverByStartSequence: 匹配序列号的第一条或者下一条seq \u0026gt;= OptStartSeq\nDeliverByStartTime: 匹配时间的第一条或者下一条 time \u0026gt;= OptStartTime\nDeliverLastPerSubject: 开始消费后，消费（匹配的）每个 subject 的最新一条消息。 2.2.0 ❌ OptStartSeq DeliverByStartSequence 配合使用 2.2.0 ❌ OptStartTime DeliverByStartTime 配合使用 2.2.0 ❌ Description 消费者描述 InactiveThreshold Consumer 超过这个时间未活动会被清理，在 2.9 以前仅对 临时消费者生效。 2.2.0 ✅ MaxAckPending 定义为确认的消息的最大数量，一旦到达这个限制，那么将暂定投递消息。 2.2.0 ✅ MaxDeliver 尝试传送消息的最大次数。消费者未确认（NAck或者未发送确认）时会重新传递。 2.2.0 ✅ ReplayPolicy ReplayOriginal：模拟接收到消息的时间向 consumer 推送。\nReplayInstant：消息将尽快推送到客户端。 2.2.0 ❌ Replicas 设置 consumer group 的副本数，默认继承 stream。 2.8.3 ✅ MemoryStorage 设置将 Consumer 的状态保留在内存中，而不是继承 stream 的存储类型。 2.8.3 ✅ SampleFrequency 采样率：？？？ 2.2.0 ✅ Metadata 用于关联消费者的元数据 2.10.0 ✅ FilterSubjects 类似于 FilterSubject，但是多个 2.10.0 ✅ 5.2.2 拉模式 - 专属配置 # 配置选项 描述 版本 是否可编辑 MaxWaiting 最大拉取请求数 2.2.0 ❌ MaxRequestExpires 单个拉取请求等待消息可供拉取的最长时间 2.2.0 ❌ MaxRequestBatch 单个请求的最大数量 (MaxRequestMaxBytes 共同作用，先到先限制) 2.7.0 ✅ MaxRequestMaxBytes 单个请求的最大字节数 2.8.3 ✅ 5.2.3 推模式 - 专属配置 # 配置选项 描述 版本 是否可编辑 DeliverSubject 设置服务器推送的主题，将隐式的设置消费者是基于推送。 2.2.0 ❌ DeliverGroup 类似于 queue group 2.2.0 ✅ FlowControl 启用滑动窗口协议（服务端和客户端通信交换），用于控制服务器向客户端推送多少消息。 2.2.0 ✅ IdleHeartbeat 服务器定时向客户端发送状态消息（在心跳周期内没有新消息发送），客户端可以感知到服务器的状态。 2.2.0 ✅ RateLimit 限制向消费者发送消息的的速率 bits Per Second 2.2.0 ✅ HeadersOnly 仅传送流中的消息标头，而不传送正文。其中会携带 Nats-Msg-Size 来标识负载的大小。 2.6.2 ✅ 6. 关键流程 # 这里梳理了一些 Nats 的关键流程，用来理解 nats 中的设计和实现原理。\n6.1 服务启动 / 集群 # 集群 gossip 流程示意图：\nnats-server -D -p 4222 -cluster nats://localhost:6222 nats-server -D -p 4333 -cluster nats://localhost:6333 -routes nats://localhost:6222 nats-server -D -p 4444 -cluster nats://localhost:6444 -routes nats://localhost:6333 流程解释说明：\nnats-1 启动 nats-2 连接到 nats-1, nats-1 回复 INFO 消息，并将 nats-2 添加到自己的路由，同时建立到 nats-2 的连接。 nats-3 连接到 nats-2, nats-2 回复 INFO 消息，并将 nats-3 添加到自己的路由，同时建立到 nats-3 的连接。这里 nats-2 会将新路由（ nats-3 ）传播（INFO 消息）到自己已知的服务节点，也就是 nats-1, 而 nats-1 收到 INFO 消息后，会向 nats-3 建立连接，与前面的流程类似。 6.2 PUB / SUB # 整体流程概览如下：\n6.2.1 SUB # 当客户端新增订阅时，会向服务器发送一条 SUB 消息，服务器则会更新客户端（及对应账户）的订阅关系，同时向其余集群中的 route (服务节点) 发送 RS+ （集群通讯协议）消息以更新订阅。\n6.2.2 PUB # 客户端发布一条消息，服务器内部匹配（有缓存设计以提高匹配效率）相关订阅客户端。将消息加入到客户端的发送缓冲区（注意：订阅中区分 普通订阅和 队列，队列模式则需要随机选中一个客户端），当相应的 route 节点收到消息后，会再从本地的订阅列表中匹配当前节点中的客户端推送消息。\n6.3 Request / Reply # 从 Nats 设计的协议已经知道，Request 和 Reply 两个 API 是基于 PUB / SUB 机制。PUB 参数中可以携带一个 reply 参数，该 reply 参数是发布时自动创建的一个 _INBOX subject。服务端在推送消息时，会将 reply 放到 MSG 消息头传递给订阅者。\n// This processes the sublist results for a given message. // Returns if the message was delivered to at least target and queue filters. func (c *client) processMsgResults(acc *Account, r *SublistResult, msg, deliver, subject, reply []byte, flags int) (bool, [][]byte) { // ... var creply = reply // ... // Loop over all normal subscriptions that match. for _, sub := range r.psubs { ... // Normal delivery mh := c.msgHeader(dsubj, creply, sub) c.deliverMsg(prodIsMQTT, sub, acc, dsubj, creply, mh, msg, rplyHasGWPrefix) } } 6.4 新建 jetstream # 在开始之前，我们需要对 multi-raft 有个大致的概念，示意图如下： stream raft group 就对应上图中的一个 raft group。与此同时，集群中所有开启 jetstream 功能的节点还会组成一个 meta raft group, 用来管理 jestream API 的执行和 jetstream 集群拓扑（节点上线离线）。\n从 nats-cli 的源码入手，我们可以找到：\n// NewStreamFromDefault creates a new stream based on a supplied template and optionsfunc (m *Manager) NewStreamFromDefault(name string, dflt api.StreamConfig, opts ...StreamOption) (stream *Stream, err error) { // ... var resp api.JSApiStreamCreateResponse err = m.jsonRequest(fmt.Sprintf(api.JSApiStreamCreateT, name), \u0026amp;cfg, \u0026amp;resp) if err != nil { return nil, err } return m.streamFromConfig(\u0026amp;resp.Config, resp.StreamInfo), nil } Manager 的含义是 JetStreamManager，代表与服务端 JS API 交互，其中 api. JSApiStreamCreateT string = \u0026quot;$JS.API.STREAM.CREATE.%s\u0026quot; 可以发现底层还是通过 PUB / SUB 机制实现的交互。\n在 nats-server 中检索可以发现，在 nats-server 内部定义了 JetStream 相关API 的 msgHandler。\nfunc (s *Server) setJetStreamExportSubs() error { // Start the go routine that will process API requests received by the // subscription below when they are coming from routes, etc.. s.jsAPIRoutedReqs = newIPQueue[*jsAPIRoutedReq](s, \u0026#34;Routed JS API Requests\u0026#34;) s.startGoRoutine(s.processJSAPIRoutedRequests) // This is the catch all now for all JetStream API calls. if _, err := s.sysSubscribe(jsAllAPI, js.apiDispatch); err != nil { return err } // ... // API handles themselves. pairs := []struct { subject string handler msgHandler }{ {JSApiAccountInfo, s.jsAccountInfoRequest}, {JSApiTemplateCreate, s.jsTemplateCreateRequest}, {JSApiTemplates, s.jsTemplateNamesRequest}, {JSApiTemplateInfo, s.jsTemplateInfoRequest}, {JSApiTemplateDelete, s.jsTemplateDeleteRequest}, {JSApiStreamCreate, s.jsStreamCreateRequest}, {JSApiStreamUpdate, s.jsStreamUpdateRequest}, {JSApiStreams, s.jsStreamNamesRequest}, {JSApiStreamList, s.jsStreamListRequest}, {JSApiStreamInfo, s.jsStreamInfoRequest}, {JSApiStreamDelete, s.jsStreamDeleteRequest}, {JSApiStreamPurge, s.jsStreamPurgeRequest}, {JSApiStreamSnapshot, s.jsStreamSnapshotRequest}, {JSApiStreamRestore, s.jsStreamRestoreRequest}, {JSApiStreamRemovePeer, s.jsStreamRemovePeerRequest}, {JSApiStreamLeaderStepDown, s.jsStreamLeaderStepDownRequest}, {JSApiConsumerLeaderStepDown, s.jsConsumerLeaderStepDownRequest}, {JSApiMsgDelete, s.jsMsgDeleteRequest}, {JSApiMsgGet, s.jsMsgGetRequest}, {JSApiConsumerCreateEx, s.jsConsumerCreateRequest}, {JSApiConsumerCreate, s.jsConsumerCreateRequest}, {JSApiDurableCreate, s.jsConsumerCreateRequest}, {JSApiConsumers, s.jsConsumerNamesRequest}, {JSApiConsumerList, s.jsConsumerListRequest}, {JSApiConsumerInfo, s.jsConsumerInfoRequest}, {JSApiConsumerDelete, s.jsConsumerDeleteRequest}, } js.mu.Lock() defer js.mu.Unlock() for _, p := range pairs { sub := \u0026amp;subscription{subject: []byte(p.subject), icb: p.handler} if err := js.apiSubs.Insert(sub); err != nil { return err } } return nil } 在 client 消息分发(deliverMsg) 的逻辑中会对匹配的 subscription 执行 icb (msgHandler)。\n在集群模式下，这些 JetStream API 的订阅也是会通过 route 客户端传导到集群中的每个节点。相当于每个节点都订阅/处理，JS API 的消息，但是只有 leader 节点可以处理。\njsStreamCreateRequest 后续代码逻辑（集群模式下, 单机只需要在本地操作即可）:\njsStreamCluster(jetstream meta group) 的启动路径： Reload / Start -\u0026gt; enableJetStream -\u0026gt; enableJetStreamClustering -\u0026gt; startRaftNode/setupMetaGroup -\u0026gt; monitorCluster\n通过 meta raft group 提交一条新增 stream 的日志消息（其中已经设置 raftgroup 的基本信息）参见 jsClusteredStreamRequest 函数 jetstream 集群中的节点接收到该日志后（monitorCluster），会执行（applyMetaEntries）其中的操作 （assignStreamOp）添加 stream 执行逻辑参见 processStreamAssignment -\u0026gt; processClusterCreateStream 新建一个 stream raft group 启动 raft 节点 内部创建一个 stream 数据结构（mset= stream） 启动监视协程 monitorStream，处理 raft group 中的事件（快照，领导者变更，日志应用） 6.5 JetStream 消息的投递和消费 # 最简单的开始使用 JetStream 消费者的代码如下：\n// connect to nats server nc, _ := nats.Connect(nats.DefaultURL) // create jetstream context from nats connection js, _ := jetstream.New(nc) ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second) defer cancel() // get existing stream handle stream, _ := js.Stream(ctx, \u0026#34;foo\u0026#34;) // retrieve consumer handle from a stream cons, _ := stream.Consumer(ctx, \u0026#34;cons\u0026#34;) // consume messages from the consumer in callback cc, _ := cons.Consume(func(msg jetstream.Msg) { fmt.Println(\u0026#34;Received jetstream message: \u0026#34;, string(msg.Data())) msg.Ack() }) defer cc.Stop() 6.5.1 消费者创建 # 消费者可以显式或者隐式的创建，如果隐式创建往往都代表临时，也就是如果消费者不活跃则会被服务端删除。之后，客户端开始订阅消费者下发消息的 subject，push 方式等服务端推送，而 pull 模式则需要定时往服务端发送一条消息，触发消息下推。\n从 nats.go 的源码出发，StreamConsumerManager 接口定义了一个 JetStream 中 Consumer 的相关操作。我们首先看下 consumer 的创建，同样的客户端是通过 JS API 来完成调用： 服务端统一使用了 jsConsumerCreateRequest 进行处理，其中逻辑与添加 stream 类似：jetstream集群模式下会创建一个raft group 设置好 leader, 并通过 meta group 来提交新增 consumer 的日志，每个节点会处理该消息（processConsumerAssignment-\u0026gt;processClusterCreateConsumer）：\n获取到流 通过 addConsumerWithAssignment 添加一个 consumer （leader）发送响应 这个操作会在 stream raft group 的 部分/全部（取决于 consumer 配置的 Replicas）节点中创建一个 raft group (consumer) 用来保存 consumer 的相关信息（消费点位）。同时在 stream 对应的节点中添加 consumer 实例。\n再回到客户端，consumer 创建（查询）后，获取到 consume 的信息，从中获取到 delivery subject（代表 consumer 消费的消息会从这个 subject 推送出来），后续客户端会订阅这个这个 subject。\n这里可以说明服务端 consumer 和 客户端 consumer 没有必然的的联系，后续另外的客户端使用同一个 consumer 信息时，也只是共享 consumer 的配置，订阅相同的 subject，回到了 CORE NATS 提供的数据模型。\n6.5.2 消息消费 # consumer 提供了两种模式 pull (PullSubscribe) 和 push(Subscribe) 模式，虽然叫做 pull 和 push，但是这两种机制的底层实现还是基于 nats 的 pub / sub 机制。push 等价于 subscribe，服务端会将消息直接推送到客户端；pull 则是客户端主动调用 JS API ”请求“，服务端 “响应” 数据给客户端消费。参见下图抓包数据分析截图（提前创建了 stream 和 consumer，指定了拉模式）\nnats.go 的 jetstream 包针对 pull consumer 提供了丰富的API（尤其是 Consume）让 pull consumer 可以和 push consumer 用相似的方式来持续的处理消息。\n6.5.2.1 pull 模式 # pullConsumer Consume 方法实现如下（已精简），与 PullSubscribe.Fetch 的原理类似（请求 JS API），区别在于 API 的形式表现不同：\nfunc (p *pullConsumer) Consume(handler MessageHandler, opts ...PullConsumeOpt) (ConsumeContext, error) { // 解析消费选项 consumeOpts, err := parseConsumeOpts(false, opts...) if err != nil { return nil, fmt.Errorf(\u0026#34;%w: %s\u0026#34;, ErrInvalidOption, err) } // pull subject 是 JS.API 下的一个 subject // apiRequestNextT string = \u0026#34;CONSUMER.MSG.NEXT.%s.%s\u0026#34; subject := apiSubj(p.jetStream.apiPrefix, fmt.Sprintf(apiRequestNextT, p.stream, p.name)) sub := \u0026amp;pullSubscription{ consumer: p, errs: make(chan error, 1), done: make(chan struct{}, 1), fetchNext: make(chan *pullRequest, 1), consumeOpts: consumeOpts, } // 创建一个 inbox subject 作为 pull 请求的 reply，internalHandler 就是 MsgHandler inbox := p.jetStream.conn.NewInbox() sub.subscription, err = p.jetStream.conn.Subscribe(inbox, internalHandler) if err != nil { return nil, err } go func() { for { select { case status, ok := \u0026lt;-sub.connStatusChanged: case err := \u0026lt;-sub.errs: if errors.Is(err, ErrNoHeartbeat) { sub.fetchNext \u0026lt;- \u0026amp;pullRequest{ Expires: sub.consumeOpts.Expires, Batch: batchSize, MaxBytes: sub.consumeOpts.MaxBytes, Heartbeat: sub.consumeOpts.Heartbeat, } } } } }() // 根据 fetchNext 通知向服务端发送 pull 请求 // fetchNext 是由一个 consumeOpts 的心跳设置的一个 timer 触发 ErrNoHeartbeat 来激活 go sub.pullMessages(subject) return sub, nil } 6.5.2.2 push 模式 # 使用 DeliverSubject (没有设置会自动生成一个) 来设置消费者订阅的主题，同时创建一个对该主题的订阅，用于接收处理消息。push 模式同样会在服务器创建消费者。\n小结：对于客户端来说消费 stream 中的消息，其实就是产生一个订阅主题，客户端会如同普通的 SUB 客户端一样消费这里的消息。但在此之前，需要根据场景和选项在服务器创建一个 consumer，告诉服务端自己的消费场景（是否持久化，关注stream中的哪些 subject, 消息的分发策略 等等）\n6.5.3 消息投递推送 # 通过前面的消息消费我们可以知道，pullConsumer 拉取消息时会往一个形如 CONSUMER.MSG.NEXT 的主题中发起请求，通过在服务端代码检索可以发现，consumer 结构内保存了这么一个 subject, 并关联了对应的 icb（processNextMsgReq），而这个动作是在 consumer 创建时，在 leader 节点上设置的（consumer.setLeader）。\n这里要注意，这个请求只能被 consumer group 中的 leader 处理。\n与此同时 consumer leader 节点还设置了（与消费相关的操作）:\nack订阅、请求订阅 和 流控订阅等 如果消费者是推模式，那么会注册“兴趣通知” 同时会启动定时器来清除不活跃的消费者 启动消息推送的 loopAndGatherMsgs 逻辑，往 consumer.outq -\u0026gt; stream.outq 发送消息, stream.outq 在 stream.internalLoop 中执行往客户端推送的逻辑（通过 stream 的内部客户端往 subject 推送一条消息，与 pub 流程类似再由 CORE NATS 完成消息的分发）。 到这里总结下这个环节的问题：\njetstream publish 的消息经过了什么样的流程？ 消息到服务端后会检查 subject 相关的订阅，而 stream(leader)设置时已经配置了一个内部订阅。订阅会传播到集群中。 processInboundJetStreamMsg 的目的就是将将消息提交到 raft group, 最终通过 processJetStreamMsg 执行。 stream 组提交后，各个副本保存，同时通知相关的 consumer server consumer（leader） 通过 loopAndGatherMsgs 往 subject 中推送消息 server consumer 订阅了客户端 ack 消息，用于更新消费者位点等数据 是先保存还是先推送？ 在stream 组内部提交保存后推送 consumer 的消息是从 leader 来还是可以从 follower 直接读取？ leader 服务端中 stream 和 consumer 这两个结构在其中承担什么样的职责？ stream 代表 stream （包括 store / raft group / stream config) 负责 jetstream 相关API的请求处理 stream 消息 复制/存储 通知/唤醒等待的 consumer consumer 代表 消费者（包括消费配置 / raft group ） 注意它不等于客户端实例 consumer 组内的数据保存（raft group） API 处理（拉消费的 API） 消息推送 消费确认处理 JetStream 消息投递API 也是基于 PUB / SUB 机制实现的，但是存在的区别是： JetStream 是存在确认机制的，而 Core Nats 的 Publish API 并没有。因此要严格保证消息发布到 stream 中，需要使用 JetStream 的 API 来发送。\n7. 扩展 # 7.1 RAFT Consensus Protocol # Consensus is a fundamental problem in fault-tolerant distributed systems. Consensus involves multiple servers agreeing on values. Once they reach a decision on a value, that decision is final.\n共识问题是分布式系统容错中的基础性问题，它描述的是：在多个服务节点间对某个值达成一致，一旦达成一致那么这个值就是最终的结果。RAFT 就是解决共识问题的一种算法，它以简单著名。\n常见的一种实现是，通过 复制/多副本 的方式来解决分布式问题中的 可靠性 问题。常见的，我们的系统提供了3个副本，当其中一个节点宕机时，也不影响系统对外提供服务。而 RAFT 则提供相应的机制（通过当选的领导者达成共识）来解决 复制/多副本 过程中的一致性问题。\nRaft 可视化介绍：https://thesecretlivesofdata.com/raft/\n它其中有几个概念：\nLeader Election（选举）产生唯一的一个 leader 角色。 Leader Candidate Follower Vote Term Log Replication（日志复制）用于服务器之间保持一致要素。 Replicated State Machine（复制状态机）_每个服务器存储一个包含一系列指令的日志，并且按顺序执行指令。由于日志都包含相同顺序的指令，状态机会按照相同的顺序执行指令，由于状态机是确定的（deterministic），因此状态机会产生相同的结果。 7.2 Gossip Protocol # 它基于流行病传播方式的节点或者进程之间信息交换的协议。以给定的频率，每台计算机随机选择另一台计算机，并共享任何消息。\n它的定义如下：\n如果有某一项信息需要在整个网络中所有节点中传播，那从信息源开始，选择一个固定的传播周期（譬如 1 秒），随机选择它相连接的 k 个节点（称为 Fan-Out）来传播消息。 每一个节点收到消息后，如果这个消息是它之前没有收到过的，将在下一个周期内，选择除了发送消息给它的那个节点外的其他相邻 k 个节点发送相同的消息，直到最终网络中所有节点都收到了消息，尽管这个过程需要一定时间，但是理论上最终网络的所有节点都会拥有相同的消息。 它对网络节点的 连通性和稳定性 几乎没有任何要求，它一开始就将网络某些节点只能与一部分节点_部分连通（Partially Connected Network）而不是以全连通网络_（Fully Connected Network）作为前提；没有任何中心化节点或者主节点的概念。\n相应的它的缺点是：无法准确地预计到需要多长时间才能达成全网一致；也存在重复传播的概率，消息在网路中冗余。\n由此，Gossip 设计了两种可能的消息传播模式：反熵（Anti-Entropy）和传谣（Rumor-Mongering）：\n反熵：会同步节点的全部数据，以消除各节点之间的差异，目标是整个网络各节点完全的一致。 传谣：仅仅发送新到达节点的数据，即只对外发送变更信息。 在 Nats 中，gossip 被用于实现 集群服务发现 https://docs.nats.io/reference/reference-protocols/nats-server-protocol。routes 里指定了一个种子服务器，新启动的服务器连接上种子服务器后，就可以获取到全部的服务器列表。在服务器的配置里，可以不用知道所有的节点，而只用配置一个即可，但通常为了配置简单，会统一使用一个种子服务器。如下：\nnats-server -D -p 4222 -cluster nats://localhost:6222 nats-server -D -p 4333 -cluster nats://localhost:6333 -routes nats://localhost:6222 nats-server -D -p 4444 -cluster nats://localhost:6444 -routes nats://localhost:6222 // 使用下面的命令启动也是可行的 nats-server -D -p 4444 -cluster nats://localhost:6444 -routes nats://localhost:6333 可以参见服务启动 / 集群的流程。\n7.3 Zero allocation byte parser # https://www.youtube.com/watch?v=ylRKac5kSOk\u0026t=646s 零分配 的含义是：nats 在解析协议时，避免不必要的内存分配：\n使用局部变量，在栈上分配\n使用 slice 复用底层数组\n采用状态机来解析协议的各个部分，而不是构建临时对象来存储中间状态。如: 一般情况下解析如下协议时，最常规的思路就是，先把 Command(PUB) 读出来，然后根据这个command 决定要后面的操作，这里再读去两个参数 subject, payload length, 有了 length 之后再读去 payload，这样我们就创建了_4个临时变量_在表示下面的命令。\nPUB foo.bar 7 goodbye 在 nats 中则是通过状态机，一步一步的解析中间没有使用临时变量来存储命令中的数据。\n避免使用字符串，网络IO场景中，操作的对象几乎都是字节，因此 nats parser 也避免使用字符串以减少内存分配和拷贝。\n7.4 Nats CLI # https://docs.nats.io/using-nats/nats-tools/nats_cli 能方便快捷的与 nats 交互 / 模拟 / 测试。\n"},{"id":13,"href":"/2023/11/17/%E5%9C%A8istio%E6%9C%8D%E5%8A%A1%E7%BD%91%E6%A0%BC%E4%B8%AD%E6%89%A9%E5%B1%95%E8%87%AA%E5%AE%9A%E4%B9%89%E5%8A%9F%E8%83%BD/","title":"在istio服务网格中扩展自定义功能","section":"Posts","content":" 前提 # 本文假设你已经对 Kubernetes、istio 和 Envoy 有一定的了解，如果你还不了解，可以先阅读下面的文章：\nKubernetes: https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/ istio: https://istio.io/latest/docs/concepts/what-is-istio/ Envoy: https://www.envoyproxy.io/docs/envoy/latest/intro/what_is_envoy 当然不仅限于知道这些，还需要对其有一定的实践经验，这样才能更好的理解本文的内容。当然本文也不会涉及太深，只是作为 istio 的一个入门扩展教程。\n为什么要扩展功能？#背景 # 用通俗的话去理解 istio 的作用就是，对我们部署在 kubernetes 上的应用进行流量控制（代理），给我们提供了：流量控制、流量监控、流量安全等功能。但是在实际的使用中，我们还是会遇到一些特殊的场景，需要我们自己基于自己的业务场景去扩展一些功能，比如：ip 白名单、ip 黑名单、统一认证等。这些功能往往和具体公司的业务场景有关，因此 istio 无法直接提供这些功能，需要我们自己去扩展。\n传统的扩展方式 # 在传统架构中，常常通过 API 网关这一组件来实现这一些扩展能力，常用的 API 网关有：kong、apisix、openresty，而扩展的原理就是插件，或者像 nginx/openresty 在请求链中的不同阶段提供了不同的 hook，我们可以基于这些 hook 来实现扩展功能。\n就我个人的经历来说，网关要么自己定制开发，要么基于openresty来实现，又或者使用一些开源的网关，如：kong、apisix，在此基础上进行二次开发。不过这些方式都是在传统API网关中去实现的，随着 Service Mesh 的发展，API网关的某些功能也被 Sidecar 代理所取代，比如：流量控制、流量监控、流量安全等。目前 Mesh 发展的趋势也是进一步在 Sidecar 代理中实现更多的功能，而不是在 API 网关中实现。\nEnvoy 的扩展能力 # https://www.envoyproxy.io/docs/envoy/latest/extending/extending https://www.youtube.com/watch?v=XdWmm_mtVXI envoy 提供了丰富的扩展能力，Access Logger, Access Log Filter, Clusters, Listen Filter, Network Filter, HTTP Filter 等等。这一块内容非常多，如果想要了解更多，请参考官方文档。\nistio 的扩展能力 # https://istio.io/latest/docs/reference/config/networking/envoy-filter/\nistio 作为一个 Service Mesh 框架，其底层的代理是 Envoy，因此 istio 也继承了 Envoy 的扩展能力。istio 提供了 EnvoyFilter 这样一种自定义 istio Pilot 生成的 Envoy 配置的机制。可以修改某些字段的值、添加特定过滤器。对于特定命名空间中的给定工作负载，可以存在任意数量的 EnvoyFilter。这些 EnvoyFilter 的应用顺序如下：配置根命名空间中的所有 EnvoyFilter，然后是工作负载命名空间中的所有匹配的 EnvoyFilter。\n需要谨慎的使用这个功能，因为错误的配置会影响整个网格的稳定性。\n实践：实现一个请求参数改写请求头的功能 # 假设我们有一个业务场景，需要将请求参数中的 cvn 参数的值，改写到请求头中的 x-istio-cvn 参数中，那我们应该怎么做呢？我们可以用伪代码来简单梳理下逻辑：\nfunction rewrite_cvn_to_header() cvn = request.query_params.cvn request.headers.set(\u0026#34;x-istio-cvn\u0026#34;, cvn) end 编写 EnvoyFilter # 从 envoy 的扩展能力中，我们可以知道，我们需要编写一个 HTTP Filter，用于在请求到达 Sidecar 代理时，对请求进行处理，将请求参数中的 cvn 参数的值，改写到请求头中的 x-client-cvn 参数中。而在 istio 中，我们需要编写一个 EnvoyFilter 资源，用于将 HTTP Filter 注入到 Envoy 中。\napiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: cvn-rewrite # EnvoyFilter 的名称 namespace: istio-system # EnvoyFilter 所在的命名空间, 作用于 ingressgateway spec: workloadSelector: labels: istio: ingressgateway configPatches: - applyTo: HTTP_FILTER # 应用于 HTTP_FILTER match: context: GATEWAY # 作用于 GATEWAY listener: filterChain: filter: name: envoy.filters.network.http_connection_manager subFilter: name: envoy.filters.http.router patch: operation: INSERT_BEFORE # 在 envoy.router 之前插入 value: name: envoy.lua typed_config: \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\u0026#34; inlineCode: | function envoy_on_request(request_handle) local cvn = request_handle:headers():get(\u0026#34;:path\u0026#34;):match(\u0026#34;cvn=([%w%.%-]+)\u0026#34;) if cvn == nil then cvn = \u0026#34;default\u0026#34; end request_handle:headers():add(\u0026#34;x-client-cvn\u0026#34;, cvn) end 如果想要了解 envoy 中关于 lua 更多的扩展内容，一定要先通读下官方文档，地址在此：https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/lua_filter.html#lua\n部署 # 为了方便测试我们可以编写这么一个应用用于打印请求头：\napp.py 服务代码：\nfrom flask import Flask, request app = Flask(__name__) @app.route(\u0026#34;/\u0026#34;) def index(): print(request.headers) dict = {} for key, value in request.headers: dict[key] = value return dict if __name__ == \u0026#34;__main__\u0026#34;: app.run(host=\u0026#34;0.0.0.0\u0026#34;, port=8080) Dockerfile 文件进行打包：\nnerdctl.lima build -t docker.io/yeqown/istio-envoy-filter-demo:0.0.1 . --build-arg VERSION=0.0.1 如果不想自己打包，可以直接用我的镜像：docker.io/yeqown/istio-envoy-filter-demo:0.0.1\nFROM python:3.8-slim-buster WORKDIR /app COPY . . RUN pip install flask EXPOSE 8080 CMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] k8s 资源配置 deployment.yaml 文件：\napiVersion: apps/v1 kind: Deployment metadata: name: istio-envoy-filter-demo namespace: default labels: app: istio-envoy-filter-demo spec: selector: matchLabels: app: istio-envoy-filter-demo replicas: 1 template: metadata: labels: app: istio-envoy-filter-demo spec: containers: - name: istio-envoy-filter-demo image: docker.io/yeqown/istio-envoy-filter-demo:0.0.1 imagePullPolicy: IfNotPresent ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: istio-envoy-filter-demo namespace: default spec: selector: app: istio-envoy-filter-demo ports: - name: http-port port: 8080 targetPort: 8080 deployment.networking.yaml istio 配置文件：\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: istio-envoy-filter-demo namespace: default spec: hosts: - \u0026#34;*\u0026#34; gateways: - istio-envoy-filter-demo-gateway http: - name: \u0026#34;istio-envoy-filter-demo\u0026#34; match: - uri: exact: /istio-envoy-filter-demo rewrite: uri: / route: - destination: host: istio-envoy-filter-demo.default.svc.cluster.local --- apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: istio-envoy-filter-demo-gateway namespace: default spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;*\u0026#34; 将这些文件都部署后，可以通过以下的命令来检查时链路是否通畅：\nINGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.spec.ports[?(@.name==\u0026#34;http2\u0026#34;)].port}\u0026#39;) curl -X GET -v http://$INGRESS_HOST:$INGRESS_PORT/istio-envoy-filter-demo # \u0026gt; GET /istio-envoy-filter-demo HTTP/1.1 # \u0026gt; Host: 10.96.133.213 # \u0026gt; User-Agent: curl/8.4.0 # \u0026gt; Accept: */* # \u0026gt; # \u0026lt; HTTP/1.1 200 OK # \u0026lt; server: istio-envoy # \u0026lt; date: Wed, 27 Dec 2023 06:56:18 GMT # \u0026lt; content-type: application/json # \u0026lt; content-length: 691 # \u0026lt; x-envoy-upstream-service-time: 1 # \u0026lt; # { [691 bytes data] # 100 691 100 691 0 0 182k 0 --:--:-- --:--:-- --:--:-- 224k # * Connection #0 to host 10.96.133.213 left intact # { # \u0026#34;Accept\u0026#34;: \u0026#34;*/*\u0026#34;, # \u0026#34;Host\u0026#34;: \u0026#34;10.96.133.213\u0026#34;, # \u0026#34;User-Agent\u0026#34;: \u0026#34;curl/8.4.0\u0026#34;, # \u0026#34;X-B3-Parentspanid\u0026#34;: \u0026#34;914dc42200412837\u0026#34;, # \u0026#34;X-B3-Sampled\u0026#34;: \u0026#34;1\u0026#34;, # \u0026#34;X-B3-Spanid\u0026#34;: \u0026#34;8c40029a531c2e93\u0026#34;, # \u0026#34;X-B3-Traceid\u0026#34;: \u0026#34;c992738f6d8be43e914dc42200412837\u0026#34;, # \u0026#34;X-Envoy-Attempt-Count\u0026#34;: \u0026#34;1\u0026#34;, # \u0026#34;X-Envoy-Internal\u0026#34;: \u0026#34;true\u0026#34;, # \u0026#34;X-Envoy-Original-Path\u0026#34;: \u0026#34;/istio-envoy-filter-demo\u0026#34;, # \u0026#34;X-Forwarded-Client-Cert\u0026#34;: \u0026#34;By=spiffe://cluster.local/ns/default/sa/default;Hash=da144c5982372ecb6463dbfed384d2c4430f7ebfd5e4f5183238f19f8efb565a;Subject=\\\u0026#34;\\\u0026#34;;URI=spiffe://cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account\u0026#34;, # \u0026#34;X-Forwarded-For\u0026#34;: \u0026#34;10.244.0.1\u0026#34;, # \u0026#34;X-Forwarded-Proto\u0026#34;: \u0026#34;http\u0026#34;, # \u0026#34;X-Request-Id\u0026#34;: \u0026#34;f94024c6-8948-9a1c-af4c-aec40c395f8a\u0026#34; # } 这里我们可以注意到，请求头中除了常见的 Accept, Host, User-Agent 还多了一些，如：X-B3-Parentspanid, X-Request-Id 等，这些都是 istio 为我们提供的功能，用于链路追踪，我们可以通过这些参数来追踪请求的链路。当然其中没有包含 x-client-cvn 因为我们还没有部署 EnvoyFilter。\n部署并测试自定义 envoyFilter # 将上面编写的 EnvoyFilter 部署到 k8s 中：\nkubectl apply -f envoyfilter.yaml 将这个资源都应用到 k8s 之后，我们再次访问服务，可以看到请求头中多了一个 x-client-cvn 参数，这就是我们自定义的 EnvoyFilter 所做的事情。\ncurl -X GET -v http://$INGRESS_HOST:$INGRESS_PORT/istio-envoy-filter-demo?cvn=v1.2.3-hotfix # * Connected to 10.96.133.213 (10.96.133.213) port 80 # \u0026gt; GET /istio-envoy-filter-demo?cvn=v1.2.3-hotfix HTTP/1.1 # \u0026gt; Host: 10.96.133.213 # \u0026gt; User-Agent: curl/8.4.0 # \u0026gt; Accept: */* # \u0026gt; # \u0026lt; HTTP/1.1 200 OK # \u0026lt; server: istio-envoy # \u0026lt; date: Wed, 27 Dec 2023 06:55:32 GMT # \u0026lt; content-type: application/json # \u0026lt; content-length: 715 # \u0026lt; x-envoy-upstream-service-time: 7 # \u0026lt; # { [715 bytes data] # 100 715 100 715 0 0 78857 0 --:--:-- --:--:-- --:--:-- 79444 # * Connection #0 to host 10.96.133.213 left intact # { # \u0026#34;Accept\u0026#34;: \u0026#34;*/*\u0026#34;, # \u0026#34;Host\u0026#34;: \u0026#34;10.96.133.213\u0026#34;, # \u0026#34;User-Agent\u0026#34;: \u0026#34;curl/8.4.0\u0026#34;, # \u0026#34;X-B3-Parentspanid\u0026#34;: \u0026#34;ea5a04a5505b30f2\u0026#34;, # \u0026#34;X-B3-Sampled\u0026#34;: \u0026#34;1\u0026#34;, # \u0026#34;X-B3-Spanid\u0026#34;: \u0026#34;579aefbe55ff1859\u0026#34;, # \u0026#34;X-B3-Traceid\u0026#34;: \u0026#34;22daad86fc4471d2ea5a04a5505b30f2\u0026#34;, # \u0026#34;X-Client-Cvn\u0026#34;: \u0026#34;v1.2.3-hotfix\u0026#34;, # \u0026#34;X-Envoy-Attempt-Count\u0026#34;: \u0026#34;1\u0026#34;, # \u0026#34;X-Envoy-Internal\u0026#34;: \u0026#34;true\u0026#34;, # \u0026#34;X-Envoy-Original-Path\u0026#34;: \u0026#34;/istio-envoy-filter-demo?cvn=v1.2.3-hotfix\u0026#34;, # \u0026#34;X-Forwarded-Client-Cert\u0026#34;: \u0026#34;By=spiffe://cluster.local/ns/default/sa/default;Hash=da144c5982372ecb6463dbfed384d2c4430f7ebfd5e4f5183238f19f8efb565a;Subject=\\\u0026#34;\\\u0026#34;;URI=spiffe://cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account\u0026#34;, # \u0026#34;X-Forwarded-For\u0026#34;: \u0026#34;10.244.0.1\u0026#34;, # \u0026#34;X-Forwarded-Proto\u0026#34;: \u0026#34;http\u0026#34;, # \u0026#34;X-Request-Id\u0026#34;: \u0026#34;c5041ac1-8b18-9999-be44-63bc6fbfa885\u0026#34; # } 到这里，我们就完成了一个简单的自定义 EnvoyFilter 的功能：在网关处将查询参数 cvn 转请求头 X-Client-Cvn ，当然这只是一个简单的例子，实际的使用中，我们可以基于 Envoy 的扩展机制，实现更多的功能，比如：ip 白名单、ip 黑名单、统一认证等。\n所有代码都可以在：https://github.com/yeqown/playground/tree/master/k8s/istio-envoy-filter 找到。\n参考资料 # istio: EnvoyFilter Envoy: Lua Filter envoy WASM "},{"id":14,"href":"/2023/10/10/fsnotify%E5%8E%9F%E7%90%86%E6%8E%A2%E7%A9%B6/","title":"fsnotify原理探究","section":"Posts","content":" 本文如果没有特殊说明，所有的内容都是指 linux 系统\n起因是从 kratos 群里看到有人问：“测了下kratos的config watch，好像对软链不生效”，他提供的屏幕截图如下类似：\n$ pwd /tmp/testconfig $ ls -l drwxr-xr-x 3 root root 4096 Oct 10 19:48 . drwxr-xr-x 10 root root 4096 Oct 10 19:48 .. drwxr-xr-x 1 root root 11 Oct 10 19:48 ..ver1 drwxr-xr-x 1 root root 11 Oct 10 19:48 ..ver2 lrwxr-xr-x 1 root root 11 Oct 10 19:48 ..data -\u0026gt; ..ver1 drwxr-xr-x 1 root root 11 Oct 10 19:48 data $ $ ll -a data drwxr-xr-x 3 root root 4096 Oct 10 19:48 . drwxr-xr-x 10 root root 4096 Oct 10 19:48 .. lrwxrwxrwx 1 root root 11 Oct 10 19:48 registry.yaml -\u0026gt; /tmp/testconfig/..data/registry.yaml 然后触发更新的动作其实是把 ..data 的源改成了 ..ver2，但是发现并没有触发更新，于是就问了一下。\n看到这个问题的第一反应是：软链接的文件无法监控，那么硬链接的文件可以监控吗？（第一反应是软链接的实现决定）于是便回复让这位小伙伴是否硬链接可以，他给答复是可以。\n突然我脑海里冒出了两个问题：\nQ1 软链接和硬链接的区别是什么？ Q2 fsnotify 的原理是什么？ 软链接和硬链接的区别 # 在使用的时候，软链接和硬链接的区别是什么呢？\n特点/操作 软链接 硬链接 创建 ln -s 源文件 目标文件 ln 源文件 目标文件 创建（源文件不存在） 可以创建 不可以创建 删除源文件 软链接文件无法访问 硬链接文件可以访问 修改源文件 软链接文件无法访问 硬链接文件可以访问 修改链接文件 源文件可以访问 源文件可以访问 实现差异 保存源文件的路径 和源文件的 inode 一致 这里的 inode 是 linux 文件系统的一个概念，每个文件都有一个 inode，inode 保存了文件的元数据，比如文件的权限、文件的大小、文件的创建时间等等。\n两者在 linux 中的实现区别 # 通过 man ln 手册，我们知道硬链接对应 link(2) 系统调用，软链接对应 symlink(2) 系统调用。硬链接声明系统调用 do_linkat，最终调用 vfs_symlink；而软链接声明系统调用 do_symlinkat, 最终调用 vfs_symlink。这两个都会调用实际的文件系统实现，比如 ext4 文件系统的实现。这里就只贴关键的部分代码：\nint do_linkat(int olddfd, struct filename *old, int newdfd, struct filename *new, int flags) { ... // 转到 vfs_link 函数 error = vfs_link(old_path.dentry, idmap, new_path.dentry-\u0026gt;d_inode, new_dentry, \u0026amp;delegated_inode); ... } int vfs_link(struct dentry *old_dentry, struct mnt_idmap *idmap, struct inode *dir, struct dentry *new_dentry, struct inode **delegated_inode) { // 调用具体的文件系统实现 // inode 中 i_nlink 表示的是硬链接的数量，因此一般的实现都会将这个计数器 +1 error = dir-\u0026gt;i_op-\u0026gt;link(old_dentry, dir, new_dentry); // 触发 fsnotify 事件 if (!error) fsnotify_link(dir, inode, new_dentry); return error; } /////////// 参考 ext4 文件系统的实现 /////////// int __ext4_link(struct inode *dir, struct inode *inode, struct dentry *dentry) { ... inode-\u0026gt;i_ctime = current_time(inode); ext4_inc_count(inode); ... } static int ext4_symlink(struct mnt_idmap *idmap, struct inode *dir, struct dentry *dentry, const char *symname) { ... // 新建一个 inode inode = ext4_new_inode_start_handle(idmap, dir, S_IFLNK|S_IRWXUGO, \u0026amp;dentry-\u0026gt;d_name, 0, NULL, EXT4_HT_DIR, credits); // 设置软链接的内容 // 1. 如果内容长度超过 EXT4_N_BLOCKS * 4，函数 ext4_init_symlink_block 会被调用用来分配一个新的符号链接块并填充它。 if ((disk_link.len \u0026gt; EXT4_N_BLOCKS * 4)) { err = ext4_init_symlink_block(handle, inode, \u0026amp;disk_link); } else { // 如果长度较短，内存会被直接拷贝到 inode 结构的 i_data 字段。 // 在设置链接内容后，还会更新 inode 的 i_size 和 i_disksize 字段以反映链接内容的长度。 ext4_clear_inode_flag(inode, EXT4_INODE_EXTENTS); memcpy((char *)\u0026amp;EXT4_I(inode)-\u0026gt;i_data, disk_link.name, disk_link.len); inode-\u0026gt;i_size = disk_link.len - 1; EXT4_I(inode)-\u0026gt;i_disksize = inode-\u0026gt;i_size; } } 小结： 想要彻底搞懂，还需要看下 VFS 的设计，尤其是 inode 的结构，这里就不展开了。\nfsnotify 的实现 # kratos 使用的是 fsnotify 这个仓库，因此我们可以直接从这个仓库入手，看看它是如何实现的。从仓库对应的 README 中可以发现这么一个使用实例：\nfunc main() { // Create new watcher. watcher, err := fsnotify.NewWatcher() // ... defer watcher.Close() // Start listening for events. go func() { for { select { case event, ok := \u0026lt;-watcher.Events: log.Println(\u0026#34;event:\u0026#34;, event) if event.Has(fsnotify.Write) { log.Println(\u0026#34;modified file:\u0026#34;, event.Name) } case err, ok := \u0026lt;-watcher.Errors: // ... } } }() // Add a path. err = watcher.Add(\u0026#34;/tmp\u0026#34;) } 从中可以发现这个库的主要 API/对象 有如下几个：\nNewWatcher() Watcher.Add() Watcher.Events 这个库拥有跨平台支持能力，我们这里也只关注 linux 平台的实现也就是 backend_inotify.go 这个文件\nWatcher 和 NewWacther 构造方法 # Watcher 是一个结构体，定义如下：\ntype Watcher struct { // Events 是一个文件系统变更事件的 channel，可以发送以下的事件： // fsnotify.Create // fsnotify.Remove // fsnotify.Rename // fsnotify.Write // fsnotify.Chmod // 具体什么场景下会触发什么事件，不同平台上可能也会有差异，就参考官方文档为准。 Events chan Event // Errors 是一个错误的 channel，当发生错误的时候，会发送到这个 channel Errors chan error // Store fd here as os.File.Read() will no longer return on close after // calling Fd(). See: https://github.com/golang/go/issues/26439 fd int inotifyFile *os.File // inotify 文件 watches *watches // 监听对象的集合 done chan struct{} closeMu sync.Mutex doneResp chan struct{} } // 这两个结构用于管理 通过 inotify_add_watch() 添加的监听文件对象 type ( watches struct { mu sync.RWMutex wd map[uint32]*watch // wd -\u0026gt; watch path map[string]uint32 // pathname -\u0026gt; wd } watch struct { wd uint32 // Watch descriptor (as returned by the inotify_add_watch() syscall) flags uint32 // inotify flags of this watch (see inotify(7) for the list of valid flags) path string // Watch path. } ) NewWatcher 函数的实现如下：\n// NewWatcher creates a new Watcher. func NewWatcher() (*Watcher, error) { // Need to set nonblocking mode for SetDeadline to work, otherwise blocking // I/O operations won\u0026#39;t terminate on close. fd, errno := unix.InotifyInit1(unix.IN_CLOEXEC | unix.IN_NONBLOCK) if fd == -1 { return nil, errno } w := \u0026amp;Watcher{ fd: fd, inotifyFile: os.NewFile(uintptr(fd), \u0026#34;\u0026#34;), watches: newWatches(), Events: make(chan Event), Errors: make(chan error), done: make(chan struct{}), doneResp: make(chan struct{}), } go w.readEvents() return w, nil } 可以看到其中最核心的就是 unix.InotifyInit1 调用了，通过深入源码可以发现，这个函数的实现是调用了 linux 的系统调用 inotify_init1，而这个系统调用的作用是：初始化一个新的 inotify 实例并返回与新的 inotify 事件队列关联的文件描述符，这个文件描述符可以用于后续的操作（可以先理解成和网络编程中的监听套接字一样的文件描述符）。\n这里关于 inotify 先不急着展开，我们继续看另外的两个函数。\nWatcher.Add # Watcher.Add 是对 Watcher.AddWith 的包装，因此我们直接看 AddWith 的实现：\n// AddWith 允许传入一些选项： // - WithBufferSize 设置缓冲区大小，这个选项只对 windows 有效，其他平台无效 默认是 64K func (w *Watcher) AddWith(name string, opts ...addOpt) error { if w.isClosed() { return ErrClosed } name = filepath.Clean(name) _ = getOptions(opts...) var flags uint32 = unix.IN_MOVED_TO | unix.IN_MOVED_FROM | unix.IN_CREATE | unix.IN_ATTRIB | unix.IN_MODIFY | unix.IN_MOVE_SELF | unix.IN_DELETE | unix.IN_DELETE_SELF return w.watches.updatePath(name, func(existing *watch) (*watch, error) { if existing != nil { flags |= existing.flags | unix.IN_MASK_ADD } // inotify 系统调用，把文件/目录添加到 inotify 实例中 wd, err := unix.InotifyAddWatch(w.fd, name, flags) if wd == -1 { return nil, err } if existing == nil { return \u0026amp;watch{ wd: uint32(wd), path: name, flags: flags, }, nil } existing.wd = uint32(wd) existing.flags = flags return existing, nil }) } 从这里又发现了一个新的 inotify 系统调用 inotify_add_watch, 它传入的参数是一个文件描述符，一个文件路径和一些 flags，这个系统调用的作用是：将监视添加到初始化的 inotify 实例中。\n同样的关于这个系统调用的细节，我们先不展开，我们继续看最后一个API。\nWatcher.Events # 要分析和理解 Watcher.Events 的使用，需要先看一下 Watcher.readEvents 的实现：代码贴在这里稍微有点冗长，因此只显示了核心的部分，完整的代码可以参考 https://github.com/fsnotify/fsnotify/blob/main/backend_inotify.go#L453-L560\nfunc (w *Watcher) readEvents() { // ... 省略部分代码 var ( buf [unix.SizeofInotifyEvent * 4096]byte // Buffer for a maximum of 4096 raw events errno error // Syscall errno ) for { // See if we have been closed. if w.isClosed() { return } // 从 inotify 文件中读取 n, err := w.inotifyFile.Read(buf[:]) // 省略错误处理的代码 if n \u0026lt; unix.SizeofInotifyEvent { // 如果读取的数据字节数小于一个事件的大小，那么就认为是错误，那么进行错误处理 // 省略处理逻辑 } var offset uint32 // 处理缓冲区中的所有事件，至少要有一个事件 for offset \u0026lt;= uint32(n-unix.SizeofInotifyEvent) { var ( // unix.InotifyEvent 是一个结构体，定义如下： // type InotifyEvent struct { // Wd int32 // Mask uint32 // Cookie uint32 // Len uint32 // } // 这里使用了 unsafe.Pointer 将 buf 中正在处理的事件，转换成了 InotifyEvent 结构体 raw = (*unix.InotifyEvent)(unsafe.Pointer(\u0026amp;buf[offset])) mask = uint32(raw.Mask) nameLen = uint32(raw.Len) ) // 省略部分代码 // 如果有事件发生在监控的目录或者文件上，但是内核不会将文件名附加到事件上，但是又希望 // \u0026#34;Name\u0026#34; 能说明文件名，因此从 watches.path 中根据 wd 获取文件名。 watch := w.watches.byWd(uint32(raw.Wd)) // 自动移除监控的目录或者文件，如果目录或者文件被删除了 if watch != nil \u0026amp;\u0026amp; mask\u0026amp;unix.IN_DELETE_SELF == unix.IN_DELETE_SELF { w.watches.remove(watch.wd) } if watch != nil \u0026amp;\u0026amp; mask\u0026amp;unix.IN_MOVE_SELF == unix.IN_MOVE_SELF { err := w.remove(watch.path) // 省略错误处理 } var name string // 省略 name 处理 // 事件处理完成后就可以给使用方发送事件了 event := w.newEvent(name, mask) if mask\u0026amp;unix.IN_IGNORED == 0 { if !w.sendEvent(event) { return } } // 处理下一个事件 offset += unix.SizeofInotifyEvent + nameLen } } } 至此我们已经弄清楚了 fsnotify 中在 linux 系统上在处理流程，那么可以更加深入的去了解关于 inotify 的一些细节了。\ninotify 系统 # 从 fsnotify 的 backend_inotify.go 文件命名上我们早就可以发现 inotify 这个词，那么它是什么呢？\ninotify 是 linux VFS 的一个子系统，它可以监控文件系统的变化，当文件系统发生变化的时候，内核会将这些变化通知给用户空间，用户空间可以根据这些变化做一些事情。\n从 fsnotify 的代码中我们已经发现了 inotify 相关的系统调用了，我们可以看一下它的系统调用的文档：\n初始化一个 inotify 实例：\n// 如果flags为0，则inotify_init1()与inotify_init()相同 // // flags 包含以下的标志： // - IN_NONBLOCK 说明 inotify 文件描述符应该被设置为非阻塞模式，这样的话，如果没有事件发生，inotify_read() 将会立即返回，而不是阻塞等待 // - IN_CLOEXEC 说明 inotify 文件描述符应该被设置为 close-on-exec，这样的话，当调用 execve(2) 时，inotify 文件描述符将会被关闭 // // 成功后，这些系统调用将返回一个新的文件描述符。出错时，返回 -1，并设置 errno 来指示错误。 int inotify_init(void); int inotify_init1(int flags); 添加移除文件的监控：\n// 为路径名中指定位置的文件添加新的监视，或修改现有的监视；调用者必须具有该文件的读取权限。fd 参数是 inotify 实例对应的文件描述符。要监视路径名的事件在掩码位参数中指定。有关可在 mask 中设置的位的完整说明，请参阅 inotify(7)。 int inotify_add_watch(int fd, const char *pathname, uint32_t mask); // 从与文件描述符 fd 关联的 inotify 实例中删除与监视描述符 wd 关联的监视。 int inotify_rm_watch(int fd, int wd); 关于读取的部分 inotify(7) 中是这么说的：\nTo determine what events have occurred, an application read(2)s from the inotify file descriptor. If no events have so far occurred, then, assuming a blocking file descriptor, read(2) will block until at least one event occurs (unless interrupted by a signal, in which case the call fails with the error EINTR; see signal(7)).\n为了确定发生了哪些事件，应用程序从 inotify 文件描述符中读取(2)。如果到目前为止还没有发生任何事件，则假设有一个阻塞文件描述符，read(2) 将阻塞，直到至少发生一个事件（除非被信号中断，在这种情况下，调用会失败并出现错误 EINTR；请参阅signal(7)）。\n每次成功的读取都会返回一个包含以下一个或多个结构的缓冲区：\nstruct inotify_event { int wd; /* 监视的文件描述符，也就是 inotify_add_watch 返回的 wd */ uint32_t mask; /* 包含描述 发生 的事件的掩码位 */ uint32_t cookie; /* 连接相关事件的唯一整数标识，目前仅用于 rename 事件 */ uint32_t len; /* 说明 name 的长度 */ char name[]; /* 文件名仅在被监视目录中的事件发生时才有值，也就是直接监视文件，这里是不会有文件名的 */ }; inotify 相关的东西我们也搞清楚了，可以通过以下一个简单的 c 代码例子来串联一下：\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;sys/inotify.h\u0026gt; #define EVENT_SIZE (sizeof (struct inotify_event)) #define BUF_LEN (1024 * (EVENT_SIZE + 16)) int main(int argc, char *argv[]) { int fd, wd; char buf[BUF_LEN]; ssize_t len; struct inotify_event *event; fd = inotify_init(); if (fd == -1) { perror(\u0026#34;inotify_init\u0026#34;); exit(EXIT_FAILURE); } wd = inotify_add_watch(fd, \u0026#34;/tmp/test\u0026#34;, IN_MODIFY); if (wd == -1) { perror(\u0026#34;inotify_add_watch\u0026#34;); exit(EXIT_FAILURE); } printf(\u0026#34;Watching /tmp/test for changes...\\n\u0026#34;); while (1) { len = read(fd, buf, BUF_LEN); if (len == -1 \u0026amp;\u0026amp; errno != EAGAIN) { perror(\u0026#34;read\u0026#34;); exit(EXIT_FAILURE); } if (len \u0026lt;= 0) { continue; } event = (struct inotify_event *) buf; if (event-\u0026gt;mask \u0026amp; IN_MODIFY) { printf(\u0026#34;File /tmp/test was modified!\\n\u0026#34;); } } inotify_rm_watch(fd, wd); close(fd); return 0; } 实际执行效果如下：\ninotify 的实现和 linux 文件系统 # 到现在我们才算是真正的搞清楚了 fsnotify 是如何实现的。但是我们还是不知道在 linux 内部 inotify 到底是怎么回事？跟文件系统有什么关系？\ninotify 的实现 # 为了搞清楚 inotify 的实现，我们有必要翻一下 linux 的源码，看看它是如何实现的。通过系统调用我们知道，inotify 的使用主要是通过 inotify_init 和 inotify_add_watch 这两个系统调用来实现 fsnotify 实例的初始化和 watch 添加, 通过 read 系统调用来获取发生的事件。因此我们主要解答这三个问题：\ninotify_init 做了什么事情？ inotify_add_watch 做了什么事情？ inotify 事件如何产生的？ 这里的源码是 linux 6.4.11，为了缩短篇幅只提供了这里最关心的部分代码\ninotify_init 做了什么？ # // fs/notify/inotify/inotify_user.c#L695 static int do_inotify_init(int flags) { // ... /* 初始化一个 fsnotify_group, 这是 linux 中 fsnotify 的一个概念，也就是前文提到的实例。 */ group = inotify_new_group(inotify_max_queued_events); // ... // 创建一个匿名 inode，这个 inode 会被用于 inotify 实例 // 继续追下去会发现， 这里会创建一个文件，ret 就是这个文件的文件描述符， // 同时 group 会被设置到文件的 private_data 中，这样就可以通过文件描述符获取到 group 了 //（后续 inotify_add_watch 等操作都是通过这个文件描述符来获取 group 的） // 同时 inotify_fops 会被设置到文件的 f_op 中，文件对应的 f_op 就是 inotify 相关的操作 ret = anon_inode_getfd(\u0026#34;inotify\u0026#34;, \u0026amp;inotify_fops, group, O_RDONLY | flags); return ret; } static const struct file_operations inotify_fops = { .show_fdinfo\t= inotify_show_fdinfo, .poll\t= inotify_poll, .read\t= inotify_read, .fasync\t= fsnotify_fasync, .release\t= inotify_release, .unlocked_ioctl\t= inotify_ioctl, .compat_ioctl\t= inotify_ioctl, .llseek\t= noop_llseek, }; 关于 fsnotify_group 的定义如下：\n// include/linux/fsnotify_backend.h#L185 // // group 是一个想要接收文件系统事件的通知的结构体。mask 保存了这个 group 关心的事件类型的子集。 // group 的 refcnt 由实现者决定，任何时候如果它变成了 0，那么所有的东西都会被清理掉。 struct fsnotify_group { const struct fsnotify_ops *ops; // 用于处理事件的回调函数，inotify 对应的是 inotify_fsnotify_ops refcount_t refcnt; // 表示这个 group 实例的引用计数，如果为0则销毁 struct list_head notification_list; // 用于保存通知的链表 wait_queue_head_t notification_waitq; // 用于等待通知的等待队列，阻塞在这个队列上的进程会被唤醒 unsigned int q_len; // 事件队列的长度 unsigned int max_events; // 事件队列的最大长度 ... struct fsnotify_event *overflow_event; // 当事件队列满了的时候，会将事件放到这里 }; 小结： inotify_init 主要做了两件事情：\n创建一个 fsnotify_group 实例 创建一个匿名 inode，这个 inode 会被用于 inotify 实例，同时设定了 f_op 为 inotify_fops，private_data 为 group inotify_add_watch 做了什么？ # 通过检索 inotify_add_watch 的调用链，我们可以找到它的实现，如下：\nSYSCALL_DEFINE3(inotify_add_watch, int, fd, const char __user *, pathname, u32, mask) { // 从 inotify_init 返回的文件描述符中获取 group 实例 f = fdget(fd); group = f.file-\u0026gt;private_data; // 创建或者更新 watch ret = inotify_update_watch(group, inode, mask); } static int inotify_update_watch(struct fsnotify_group *group, struct inode *inode, u32 arg) { // 先尝试更新已经存在的 watch ret = inotify_update_existing_watch(group, inode, arg); // 如果不存在，那么创建 if (ret == -ENOENT) ret = inotify_new_watch(group, inode, arg); } static int inotify_new_watch(struct fsnotify_group *group, struct inode *inode, u32 arg) { struct inotify_inode_mark *tmp_i_mark; int ret; struct idr *idr = \u0026amp;group-\u0026gt;inotify_data.idr; spinlock_t *idr_lock = \u0026amp;group-\u0026gt;inotify_data.idr_lock; tmp_i_mark = kmem_cache_alloc(inotify_inode_mark_cachep, GFP_KERNEL); if (unlikely(!tmp_i_mark)) return -ENOMEM; fsnotify_init_mark(\u0026amp;tmp_i_mark-\u0026gt;fsn_mark, group); tmp_i_mark-\u0026gt;fsn_mark.mask = inotify_arg_to_mask(inode, arg); tmp_i_mark-\u0026gt;fsn_mark.flags = inotify_arg_to_flags(arg); tmp_i_mark-\u0026gt;wd = -1; // idr 是 group-\u0026gt;inotify_data.idr，这是一个 radix 树，用于保存所有的 inotify_inode_mark ret = inotify_add_to_idr(idr, idr_lock, tmp_i_mark); if (ret) goto out_err; /* increment the number of watches the user has */ if (!inc_inotify_watches(group-\u0026gt;inotify_data.ucounts)) { inotify_remove_from_idr(group, tmp_i_mark); ret = -ENOSPC; goto out_err; } /* we are on the idr, now get on the inode */ ret = fsnotify_add_inode_mark_locked(\u0026amp;tmp_i_mark-\u0026gt;fsn_mark, inode, 0); if (ret) { /* we failed to get on the inode, get off the idr */ inotify_remove_from_idr(group, tmp_i_mark); goto out_err; } /* return the watch descriptor for this new mark */ ret = tmp_i_mark-\u0026gt;wd; out_err: /* match the ref from fsnotify_init_mark() */ fsnotify_put_mark(\u0026amp;tmp_i_mark-\u0026gt;fsn_mark); return ret; } 小结： inotify_add_watch 主要做了以下几件事情：\n创建一个 inotify_inode_mark 实例 将 inotify_inode_mark 实例添加到 group-\u0026gt;inotify_data.idr 中 inotify 事件如何产生的？ # 在阅读 inotify 相关的源码时 fsnotify.h 中有这么些函数：\n// include/linux/fsnotify.h // fsnotify_create - \u0026#39;name\u0026#39; was linked in static inline void fsnotify_create(struct inode *dir, struct dentry *dentry) // fsnotify_link - \u0026#39;name\u0026#39; was created static inline void fsnotify_link(struct inode *dir, struct inode *inode, struct dentry *new_dentry) // fsnotify_delete - @dentry was unlinked and unhashed static inline void fsnotify_delete(struct inode *dir, struct inode *inode, struct dentry *dentry) // fsnotify_access - @file was accessed static inline void fsnotify_access(struct file *file) // ... 还有更多 很明显，从名字就可以看出大概是给其他的系统调用提供了一些 hook，这样在这些系统调用中就可以调用这些 hook 来通知到用户空间了。跟踪下 fsnotify_access 的调用链，可以发现它存在于 read syscall \u0026gt; ksys_read \u0026gt; vfs_read \u0026gt; fsnotify_access 调用链中，也可以从侧面佐证这一点。\n这一部分链路较长，代码也很多，因此只关注两个点:\n文件触发事件后，哪些 group 应该收到事件，是怎么判断的？ 事件是怎么通知到用户空间的？ 下面我们以 fsnotify_access 为例，看看它是怎么通知到用户空间的：\nfsnoify_access -\u0026gt; fsnotify_file -\u0026gt; fsnotify_parent -\u0026gt; __fsnotify_parent -\u0026gt; fsnotify -\u0026gt; (group-\u0026gt;ops.handle_inode_event)inotify_handle_inode_event -\u0026gt; fsnotify_add_event -\u0026gt; static inline void fsnotify_access(struct file *file) { fsnotify_file(file, FS_ACCESS); } static inline int fsnotify_file(struct file *file, __u32 mask) { ... return fsnotify_parent(path-\u0026gt;dentry, mask, path, FSNOTIFY_EVENT_PATH); } /* Notify this dentry\u0026#39;s parent about a child\u0026#39;s events. */ static inline int fsnotify_parent(struct dentry *dentry, __u32 mask, const void *data, int data_type) { ... return __fsnotify_parent(dentry, mask, data, data_type); notify_child: return fsnotify(mask, data, data_type, NULL, NULL, inode, 0); } 这里重点关注 fsnotify 函数，它的主要作用就是\nint fsnotify(__u32 mask, const void *data, int data_type, struct inode *dir, const struct qstr *file_name, struct inode *inode, u32 cookie) { // 从 inode 结构中的 xxx_fsnotify_marks 获取到 fsnotify_mark 信息 iter_info.marks[FSNOTIFY_ITER_TYPE_SB] = fsnotify_first_mark(\u0026amp;sb-\u0026gt;s_fsnotify_marks); if (mnt) { iter_info.marks[FSNOTIFY_ITER_TYPE_VFSMOUNT] = fsnotify_first_mark(\u0026amp;mnt-\u0026gt;mnt_fsnotify_marks); } if (inode) { iter_info.marks[FSNOTIFY_ITER_TYPE_INODE] = fsnotify_first_mark(\u0026amp;inode-\u0026gt;i_fsnotify_marks); } if (inode2) { iter_info.marks[inode2_type] = fsnotify_first_mark(\u0026amp;inode2-\u0026gt;i_fsnotify_marks); } while (fsnotify_iter_select_report_types(\u0026amp;iter_info)) { // 遍历 iter_info.marks，检查是否有 group 关心这个事件，如果有的话 // 就将事件添加到 group 的事件队列中。 ret = send_to_group(mask, data, data_type, dir, file_name, cookie, \u0026amp;iter_info); // 继续遍历下一个 mark fsnotify_iter_next(\u0026amp;iter_info); } ret = 0; ... } 这部分逻辑解析得比较简单，代码也比较多，初次读会有很多问题，所以就掌握一点： inode 结构中记录一个 i_fsnotify_marks 字段, super_block 中也有一个 s_fsnotify_marks 字段，这两个字段都是 fsnotify_mark_connector 类型，fsnotify 系统可以通过这两个字段来获取到该文件对应的 fsnotify_mark 信息。\n最终，事件 fsnotify_insert_event 完成 event 的插入，然后唤醒等待事件的进程，增加计数：\n// 给特定 group 新增事件 int fsnotify_insert_event(struct fsnotify_group *group, struct fsnotify_event *event, int (*merge)(struct fsnotify_group *, struct fsnotify_event *), void (*insert)(struct fsnotify_group *, struct fsnotify_event *)) { int ret = 0; // event 队列 struct list_head *list = \u0026amp;group-\u0026gt;notification_list; // ... // 这里的判断是为了处理溢出事件，如果事件队列已经满了，那么就将事件放到溢出事件中 if (event == group-\u0026gt;overflow_event || group-\u0026gt;q_len \u0026gt;= group-\u0026gt;max_events) { ret = 2; // 溢出事件队列还不为空，那么该事件就丢弃 if (!list_empty(\u0026amp;group-\u0026gt;overflow_event-\u0026gt;list)) { return ret; } event = group-\u0026gt;overflow_event; goto queue; } // 如果事件队列不为空且 merge 被指定，那么就尝试合并事件 // 这里合并的意思是：如果事件队列中已经有了这个事件（判断最后一个事件），那么就不用添加 if (!list_empty(list) \u0026amp;\u0026amp; merge) { ret = merge(group, event); if (ret) { return ret; } } queue: // 将事件添加到队尾，然后唤醒等待事件的进程，增加计数 group-\u0026gt;q_len++; list_add_tail(\u0026amp;event-\u0026gt;list, list); // 唤醒等待事件的进程 wake_up(\u0026amp;group-\u0026gt;notification_waitq); } 最后再重新梳理下 inotify 的事件通知流程：\n系统调用触发事件，比如 read 系统调用触发了 fsnotify_access fsnotify_access 经过一系列的逻辑判断，最终调用 fsnotify 函数，该函数的主要作用是从 inode 结构（xxx_fsnotify_marks）开始迭代循环调用 send_to_group send_to_group 函数检查 group 是否关心这个事件，如果是，那么就将事件添加到 group 的事件队列中，并唤醒等待事件的进程。 总结 # 本文从一个 kratos 的问题出发，分析了 fsnotify 的实现，在 linux 中 fsnotify 其实是基于 inotify 系统调用而实现的。inotify 是 linux VFS 的一个子系统，它可以监控文件系统的变化，当文件系统发生变化的时候，内核会将这些变化通知给用户空间，用户空间可以根据这些变化做一些事情。\n这里还是留了些问题：\nQ1 多层软链接的情况下，inotify 源文件的改动是否还能被监听到？ Q2 本文开头说的场景能不能实现（两层软链接，修改第二层的时候有变更事件）？ 参考资料 # https://linux.die.net/man/7/inotify https://github.com/fsnotify/fsnotify "},{"id":15,"href":"/2023/08/22/c%E5%92%8Clua%E4%BA%92%E6%93%8D%E4%BD%9C%E5%AE%9E%E8%B7%B5/","title":"C和lua互操作实践","section":"Posts","content":"相信了解 redis 和 openresty 的小伙伴们都知道 lua 代码可以嵌入这两种程序中运行，极大的提高了软件的扩展性；尤其是 openresty 中，通过使用 lua 我们可以很快速（相比c）的定制web服务器，或者增强 nginx 的功能。那么 lua 是如何嵌入到这些程序中的呢？lua 和 c 是如何互操作的呢？\n下文的相关环境和工具版本为：Lua 5.4.6; Mac OS 13.4.1 (Darwin Kernel Version 22.5.0) arm64 M2 pro; Apple clang version 14.0.3 (clang-1403.0.22.14.1)\nredis 中的 lua # 下面展示了一段 redis 中操作 lua API 的代码：\n这里出现了很多 lua_ 开头的函数，这些函数都是 lua 库中的函数，redis 通过这些函数来操作 lua 环境， 这里先不展开讲，后面会详细介绍。\n更多的代码，如 luaRegisterRedisAPI 就不展示了，有兴趣的可以去看源码。\n// redis-v7.2/src/eval.c#183 /* 初始化 lua 环境 * * redis 首次启动时调用，此时 setup 为 1, * 这个函数也会在 redis 的其他生命周期中被调用，此时 setup 为 0，但是被简化为 scriptingReset 调用。 */ void scriptingInit(int setup) { lua_State *lua = lua_open(); if (setup) { // 首次启动时，初始化 lua 环境 和 ldb (Lua debugger) 的一些数据结构 lctx.lua_client = NULL; server.script_disable_deny_script = 0; ldbInit(); } /* 初始化 lua 脚本字典，用于存储 sha1 -\u0026gt; lua 脚本的映射 * 用户使用 EVALSHA 命令时，从这个字典中查找对应的 lua 脚本。 */ lctx.lua_scripts = dictCreate(\u0026amp;shaScriptObjectDictType); lctx.lua_scripts_mem = 0; /* 注册 redis 的一些 api 到 lua 环境中 */ luaRegisterRedisAPI(lua); /* 注册调试命令 */ lua_getglobal(lua,\u0026#34;redis\u0026#34;); /* redis.breakpoint */ lua_pushstring(lua,\u0026#34;breakpoint\u0026#34;); lua_pushcfunction(lua,luaRedisBreakpointCommand); lua_settable(lua, -3); /* redis.debug */ lua_pushstring(lua,\u0026#34;debug\u0026#34;); lua_pushcfunction(lua,luaRedisDebugCommand); lua_settable(lua,-3); /* redis.replicate_commands */ lua_pushstring(lua, \u0026#34;replicate_commands\u0026#34;); lua_pushcfunction(lua, luaRedisReplicateCommandsCommand); lua_settable(lua, -3); lua_setglobal(lua,\u0026#34;redis\u0026#34;); /* 注册一个错误处理函数，用于在 lua 脚本执行出错时，打印出错信息。 * 需要注意的是，当错误发生在 C 函数中时，我们需要打印出错的 lua 脚本的信息， * 这样才能帮助用户调试 lua 脚本。 */ { char *errh_func = \u0026#34;local dbg = debug\\n\u0026#34; \u0026#34;debug = nil\\n\u0026#34; \u0026#34;function __redis__err__handler(err)\\n\u0026#34; \u0026#34; local i = dbg.getinfo(2,\u0026#39;nSl\u0026#39;)\\n\u0026#34; \u0026#34; if i and i.what == \u0026#39;C\u0026#39; then\\n\u0026#34; \u0026#34; i = dbg.getinfo(3,\u0026#39;nSl\u0026#39;)\\n\u0026#34; \u0026#34; end\\n\u0026#34; \u0026#34; if type(err) ~= \u0026#39;table\u0026#39; then\\n\u0026#34; \u0026#34; err = {err=\u0026#39;ERR \u0026#39; .. tostring(err)}\u0026#34; \u0026#34; end\u0026#34; \u0026#34; if i then\\n\u0026#34; \u0026#34; err[\u0026#39;source\u0026#39;] = i.source\\n\u0026#34; \u0026#34; err[\u0026#39;line\u0026#39;] = i.currentline\\n\u0026#34; \u0026#34; end\u0026#34; \u0026#34; return err\\n\u0026#34; \u0026#34;end\\n\u0026#34;; luaL_loadbuffer(lua,errh_func,strlen(errh_func),\u0026#34;@err_handler_def\u0026#34;); lua_pcall(lua,0,0,0); } /* 创建一个 lua client (没有网络连接)，用于在 lua 环境中执行 redis 命令。 * 这个客户端没必要在 scriptingReset() 调用时重新创建。 */ if (lctx.lua_client == NULL) { lctx.lua_client = createClient(NULL); lctx.lua_client-\u0026gt;flags |= CLIENT_SCRIPT; /* We do not want to allow blocking commands inside Lua */ lctx.lua_client-\u0026gt;flags |= CLIENT_DENY_BLOCKING; } /* Lock the global table from any changes */ lua_pushvalue(lua, LUA_GLOBALSINDEX); luaSetErrorMetatable(lua); /* Recursively lock all tables that can be reached from the global table */ luaSetTableProtectionRecursively(lua); lua_pop(lua, 1); lctx.lua = lua; } 通过这部分代码，应该对于 lua 的嵌入式使用有了一个大概的印象。这里可以回答以下的问题：\nlua 是如何嵌入到 redis 中的？\n通过包含 lua 的相关库，然后通过 API 来操作 lua 环境。主要分为以下几个步骤：\n初始化 lua 环境，也就是 lua_State 通过 lua 的 API 来操作 lua_State，比如注册 redis 的 api 到 lua 环境中 redis 中的 lua 可以使用那些 redis 的 api？\n这部分从 luaRegisterRedisAPI 函数中可以看到，redis 注册了以下的 api 到 lua 环境中：\nAPI 说明 pcall 执行函数，如果出错返回false和错误 (lua 内置，但是 redis 用了自己的实现) redis.call 执行 redis 命令 redis.pcall 执行 redis 命令，如果产生异常（如内存不足），返回错误信息 redis.error_reply 返回错误回复的辅助函数 { err = text } = redis.error_reply(text) redis.status_reply 返回简单字符串回复的辅助函数 { ok = text } = redis.status_reply(text) redis.sha1hex 返回其单个字符串参数的 SHA1 十六进制摘要 redis.log 打印日志 redis.log(redis.LOG_WARNING, \u0026lsquo;Something is terribly wrong\u0026rsquo;) redis.setresp(n) 允许执行脚本在 redis.call() 和 redis.pcall() 返回的回复的 Redis 序列化协议 (RESP) 版本之间切换，默认2 redis.set_repl 脚本执行过程中控制指令是否 同步/复制 redis.replicate_commands 将脚本的复制模式从 逐字复制(复制脚本) 切换为 效果复制(复制脚本生成的写入命令)。从 7.0 以后默认效果复制。 redis.breakpoint 使用 Redis Lua 调试器时，此函数会触发断点 redis.debug 该函数在 Redis Lua 调试器控制台中打印其参数 redis.register_function 此函数只能在 FUNCTION LOAD 命令的上下文中使用。调用时，它会向加载的库注册一个函数。 math.random 生成随机数 (lua 内置，但是 redis 用了自己的实现) math.randomseed 设置随机数种子 (lua 内置，但是 redis 用了自己的实现) 除此之外，还有一些调试的 api，这里就不展开了。可以参考 REDIS-LUA-API 了解更多。\nredis 中的 lua 可以使用那些变量？\n这部分从 luaRegisterRedisAPI 函数中可以看到，redis 注册了以下的变量到 lua 环境中：\n变量名 说明 redis redis 的命名空间 REPL_NONE REPL 模式，不复制命令 REPL_AOF REPL 模式，执行 lua 脚本，将命令复制到 AOF 文件中 REPL_SLAVE REPL 模式，执行 lua 脚本，将命令复制到从节点中 REPL_REPLICA REPL 模式，执行 lua 脚本，将命令复制到从节点中 REPL_ALL REPL 模式，执行 lua 脚本，将命令复制到 AOF 文件中和从节点中 REDIS_VERSION_NUM redis 版本号，比如 0x00070200 表示 7.2.0 REDIS_VERSION redis 版本号，比如 7.2.0 LOG_DEBUG 日志级别，debug LOG_VERBOSE 日志级别，verbose LOG_NOTICE 日志级别，notice LOG_WARNING 日志级别，warning lua-json 中的 C # lua-json 是一个 lua 的 json 库，这里只是用来展示 lua 中调用 C 的例子。更多信息请查阅 lua-cjson 的使用手册 https://www.kyne.com.au/~mark/software/lua-cjson-manual.html\n从它的帮助手册我们可以看到如下的代码：\n-- 模块实例化 local cjson = require \u0026#34;cjson\u0026#34; local cjson2 = cjson.new() local cjson_safe = require \u0026#34;cjson.safe\u0026#34; -- 在 lua 和 json 之间转换 text = cjson.encode(value) value = cjson.decode(text) -- 读取 lua json 中的值 setting = cjson.decode_invalid_numbers([setting]) setting = cjson.encode_invalid_numbers([setting]) keep = cjson.encode_keep_buffer([keep]) depth = cjson.encode_max_depth([depth]) depth = cjson.decode_max_depth([depth]) convert, ratio, safe = cjson.encode_sparse_array([convert[, ratio[, safe]]]) 同时在 make 安装 一节中, 指明了如果需要手动安装的话，需要执行以下的命令：\nmake cp cjson.so $LUA_MODULE_DIRECTORY 很明显，这里的 cjson.so 是一个共享库，也就是说在 lua 中如果想要调用 c 的扩展，那么共享库是一种方式。\n这里需要注意的是，lua 中通过 require 来引入的时候，需要保证 cjson.so 在 lua 的搜索路径中（package.cpath）。\n为什么说只是一种方式呢？因为 lua 还提供了一种方式，那就是通过 lua 的 API 让 c 代码编写的函数注册到 lua 环境中，然后在 c 中执行 lua 脚本，如下：\n#include \u0026lt;stdarg.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;lua.h\u0026gt; #include \u0026lt;lauxlib.h\u0026gt; #include \u0026lt;lualib.h\u0026gt; static int sumForLua(lua_State *lua) { // 如果给定虚拟栈中索引处的元素可以转换为数字，则返回转换后的数字，否则报错。 double a = luaL_checknumber(lua, 1); double b = luaL_checknumber(lua, 2); // push 计算结果到 lua 虚拟栈中 lua_pushnumber(lua, a+b); /* 返回值用于提示该C函数的返回值数量。 * * 这里可以看出，C可以返回给Lua多个结果， * 通过多次调用lua_push*()，之后return返回结果的数量。 */ return 1; } int main(void) { lua_State *lua = lua_open(); // 创建 Lua 环境 luaL_openlibs(lua); // 打开Lua状态机\u0026#34;L\u0026#34;中的所有Lua标准库。 // 注册 C函数到 lua 中 csum 函数 lua_register(lua, \u0026#34;csum\u0026#34;, sumForLua); // 执行 Lua 脚本 const char* script = \u0026#34;print(csum(3.14, 2.0))\u0026#34;; if(luaL_dostring(lua, script)) printf(\u0026#34;Failed to invoke.\\n\u0026#34;); lua_close(lua); return 0; } 我们还没有搞清楚 c 是如何给 lua 扩展的，再回到 lua-cjson 的源码中去：\n// https://github.com/mpx/lua-cjson/blob/master/lua_cjson.c#1416 CJSON_EXPORT int luaopen_cjson(lua_State *l) { // local cjson = require \u0026#34;cjson\u0026#34; lua_cjson_new(l); // 如果编译时定义了 ENABLE_CJSON_GLOBAL，注册 cjson (table) 到全局变量中 #ifdef ENABLE_CJSON_GLOBAL lua_pushvalue(l, -1); // #define CJSON_MODNAME \u0026#34;cjson\u0026#34; lua_setglobal(l, CJSON_MODNAME); #endif return 1; } // cjson 模块（table）的初始化 static int lua_cjson_new(lua_State *l) { // 函数注册表, 这些函数的函数原型都满足： // int (*lua_CFunction) (lua_State *L); return value: number of results // // 注册表结构体： // typedef struct luaL_Reg { // const char *name; // lua_CFunction func; // } luaL_Reg; // luaL_Reg reg[] = { { \u0026#34;encode\u0026#34;, json_encode }, { \u0026#34;decode\u0026#34;, json_decode }, { \u0026#34;encode_sparse_array\u0026#34;, json_cfg_encode_sparse_array }, { \u0026#34;encode_max_depth\u0026#34;, json_cfg_encode_max_depth }, { \u0026#34;decode_max_depth\u0026#34;, json_cfg_decode_max_depth }, { \u0026#34;encode_number_precision\u0026#34;, json_cfg_encode_number_precision }, { \u0026#34;encode_keep_buffer\u0026#34;, json_cfg_encode_keep_buffer }, { \u0026#34;encode_invalid_numbers\u0026#34;, json_cfg_encode_invalid_numbers }, { \u0026#34;decode_invalid_numbers\u0026#34;, json_cfg_decode_invalid_numbers }, { \u0026#34;new\u0026#34;, lua_cjson_new }, { NULL, NULL } /* 以 NULL, NULL 结束 */ }; // ... // 创建一个新的 table, 并且将其压入栈中 lua_newtable(l); // ... json_create_config(l); // 将 reg 中的函数注册到 table 中 // void luaL_setfuncs (lua_State *L, const luaL_Reg *l, int nup); // nup = 1 (不等于0) 表示将 nup 位置的值副本来初始化所有的函数，这些值会弹出栈。 // 因此这里是给 reg 中的所有函数都隐式的传入了一个 config 参数, 函数内部可以通过 // int lua_upvalueindex(int i); lua_to* 函数来获取这个参数。 luaL_setfuncs(l, reg, 1); // 设置 table.null = nil, userdata 代表 Lua中的 C 值 lua_pushlightuserdata(l, NULL); // void lua_setfield (lua_State *L, int index, const char *k); // table = stack[index]; table[k] = v (stack[-1]), 设置后 v会从栈中弹出 lua_setfield(l, -2, \u0026#34;null\u0026#34;); // 下面的类似，都是在设置 table 中的字段, 如: _NAME, _VERSION lua_pushliteral(l, CJSON_MODNAME); lua_setfield(l, -2, \u0026#34;_NAME\u0026#34;); lua_pushliteral(l, CJSON_VERSION); lua_setfield(l, -2, \u0026#34;_VERSION\u0026#34;); // 此时栈中还剩下 1 个 table return 1; } 我们来引入 lua-cjson 模块来看看：\n这里因为我使用的是 lua 5.4, 而 lua-cjson 貌似不兼容？\n这样我们也大概弄清楚了，如何为 lua 扩展 c 代码了:\n编写一个 luaopen_xxx 函数，该函数会在 lua 中通过 require \u0026ldquo;xxx\u0026rdquo; 时被调用； 在 luaopen_xxx 去注册 c 模块或者函数到 lua 环境中； 将 c 扩展编译为 so 文件，然后将 so 文件放到 lua 的搜索路径中； lua 和 C 互操作的基础 # 通过上述的例子，对于 lua 和 C 互操作有了一个大概的印象，那么我们看到的代码中那些 lua_ 和 luaL_ 开头的函数是什么呢？各种 lua_push* 和 lua_pop* 之类的函数又是做什么的呢？\n在 PIL 的 24.2 小节中解释在 Lua 和 C API 交互的过程中，会使用一抽象栈来传递数据，这个虚拟栈是一个 LIFO 栈，它由 Lua 来管理（方便 lua 的垃圾收集器工作）。该栈中的每个槽都可以保存任何 Lua 值。每当你想从 Lua 请求一个值（例如全局变量的值）时，你就调用 Lua，它将所需的值压入堆栈。每当你想向 Lua 传递一个值时，你首先将值压入堆栈，然后调用 Lua（这将弹出该值）。\n需要注意的是，虚拟栈并不是无限的，它的大小可以通过 lua.h 中的 LUA_MINSTACK 定义的。通常来说虚拟栈的大小是 20，一般情况足够使用了。如果不确实栈空间是否足够，可以通过使用 int lua_checkstack (lua_State *L, int sz); 来检查。\n有了这个基础我们就很容易理解，上面 redis 中的是如何去初始化 lua 环境的。\n重新解读 redis 中的 lua 初始化 # 为了更容易的解读，这里把一些 lua_ 函数做了简单的说明：\n函数名 说明 lua_State *lua_newstate (lua_Alloc f, void *ud); 创建一个新的 lua 环境, lua_open 的替代 int lua_getglobal (lua_State *L, const char *name); 把全局变量的值压入堆栈, 等价声明了一个 name 的全局变量 void lua_setglobal (lua_State *L, const char *name); 把堆栈顶部的值弹出并设置为 name 全局变量 const char *lua_pushstring (lua_State *L, const char *s); 把一个字符串压入堆栈，lua 会创建 s 的副本或者复用内部的副本 void lua_pushcfunction (lua_State *L, lua_CFunction f); 把一个 C 函数压入堆栈 void lua_settable (lua_State *L, int index); 给 table 的设定 kv, 等价于 table[k] = v, table = stack[index], v=stack[-1], k=stack[-2], 会从栈中弹出 k/v // redis-v7.2/src/eval.c#183 void scriptingInit(int setup) { // ... some codes ignored // 声明全局变量 redis, 同时压入栈中 lua_getglobal(lua, \u0026#34;redis\u0026#34;); // 往虚拟栈中压入 字符串 \u0026#34;breakpoint\u0026#34; lua_pushstring(lua, \u0026#34;breakpoint\u0026#34;); // 往虚拟栈中压入 C 函数 luaRedisBreakpointCommand lua_pushcfunction(lua, luaRedisBreakpointCommand); // 设置 redis[\u0026#34;breakpoint\u0026#34;] = luaRedisBreakpointCommand lua_settable(lua, -3); // 将 redis 弹出栈 lua_setglobal(lua, \u0026#34;redis\u0026#34;); // ... some codes ignored } 上面的片段等价于以下的 lua 代码：\nredis = {} redis[\u0026#34;breakpoint\u0026#34;] = luaRedisBreakpointCommand 这期间的栈空间变化可以用下面的图来表示：\n+----bottom----+ | {} | [1, -3] lua_getglobal(lua, \u0026#34;redis\u0026#34;); +--------------+ | \u0026#34;breakpoint\u0026#34; | [2, -2] lua_pushstring(lua,\u0026#34;breakpoint\u0026#34;); +--------------+ | function | [3, -1] lua_pushcfunction(lua,luaRedisBreakpointCommand); +------top-----+ +---------------------bottom--------------------------+ | {\u0026#34;breakpoint\u0026#34;: luaRedisBreakpointCommand} | [1, -1] lua_settable(lua, -3); +-----------------------top---------------------------+ 异常处理 # 虽然 C 语言没有提供异常处理机制，但是在 lua 中提供了 error 和 pcall 函数来抛出异常，处理异常。因此在 lua 和 C 交互的过程中，特意提供了一些函数来处理异常。如 lua_pcall，在调用期间如果没有异常，那么 lua_pcall 和 lua_call 行为一致，反之如果发生了异常，那么 lua_pcall 会捕获异常，并把错误对象（单个值）压入堆栈，并返回错误码。同时从堆栈中删除函数和参数。\n// 普通调用 void lua_call (lua_State *L, int nargs, int nresults); // 异常捕获调用 // msgh 用于是否使用自定义的错误处理函数，如果为 0，那么使用默认的错误处理函数 // 否则，msgh 用于说明：异常消息处理函数在堆栈中的位置 int lua_pcall (lua_State *L, int nargs, int nresults, int msgh); 应该还记得 redis 中的 lua 初始化代码中有这么一段：\n// redis-v7.2/src/eval.c#183 void scriptingInit(int setup) { // ... /* 注册一个错误处理函数，用于在 lua 脚本执行出错时，打印出错信息。 * 需要注意的是，当错误发生在 C 函数中时，我们需要打印出错的 lua 脚本的信息， * 这样才能帮助用户调试 lua 脚本。 */ { char *errh_func = \u0026#34;local dbg = debug\\n\u0026#34; \u0026#34;debug = nil\\n\u0026#34; \u0026#34;function __redis__err__handler(err)\\n\u0026#34; \u0026#34; local i = dbg.getinfo(2,\u0026#39;nSl\u0026#39;)\\n\u0026#34; \u0026#34; if i and i.what == \u0026#39;C\u0026#39; then\\n\u0026#34; \u0026#34; i = dbg.getinfo(3,\u0026#39;nSl\u0026#39;)\\n\u0026#34; \u0026#34; end\\n\u0026#34; \u0026#34; if type(err) ~= \u0026#39;table\u0026#39; then\\n\u0026#34; \u0026#34; err = {err=\u0026#39;ERR \u0026#39; .. tostring(err)}\u0026#34; \u0026#34; end\u0026#34; \u0026#34; if i then\\n\u0026#34; \u0026#34; err[\u0026#39;source\u0026#39;] = i.source\\n\u0026#34; \u0026#34; err[\u0026#39;line\u0026#39;] = i.currentline\\n\u0026#34; \u0026#34; end\u0026#34; \u0026#34; return err\\n\u0026#34; \u0026#34;end\\n\u0026#34;; // int luaL_loadbuffer (lua_State *L, const char *buff, size_t sz, const char *name); // 加载 lua 块，但是不执行，会将编译后的 lua 块压入栈中，如果出错会将错误信息压入栈中 luaL_loadbuffer(lua, errh_func, strlen(errh_func), \u0026#34;@err_handler_def\u0026#34;); // int lua_pcall (lua_State *L, int nargs, int nresults, int msgh); // 执行 lua 块，如果出错，pcall 会将错误信息压入栈中 lua_pcall(lua, 0, 0, 0); } // ... } 这里这么只是完成了定义错误处理函数，但是并没有使用，因为需要在 lua_pcall(lua_State *L, int nargs, int nresults, int msgh) 通过 msgh 来指定才能使用。那么再看看 redis 中的关于 __redis__err__handler 的使用：\n// redis-v7.2/src/eval.c#472 void evalGenericCommand(client *c, int evalsha) { // ... // 将 __redis__err__handler 压入栈中，已经在初始化过程中通过 lua 块定义成了全局变量 lua_getglobal(lua, \u0026#34;__redis__err__handler\u0026#34;); // ... // void luaCallFunction(scriptRunCtx* run_ctx, lua_State *lua, robj** keys, size_t nkeys, robj** args, size_t nargs, int debug_enabled) luaCallFunction(\u0026amp;rctx, lua, c-\u0026gt;argv+3, numkeys, c-\u0026gt;argv+3+numkeys, c-\u0026gt;argc-3-numkeys, ldb.active); // 从栈中移除 __redis__err__handler lua_pop(lua,1); // ... } // redis-v7.2/src/script_lua.c#L1615 void luaCallFunction(scriptRunCtx* run_ctx, lua_State *lua, robj** keys, size_t nkeys, robj** args, size_t nargs, int debug_enabled) { // ... // 创建一个数组，用于存储 keys luaCreateArray(lua,keys,nkeys); if (run_ctx-\u0026gt;flags \u0026amp; SCRIPT_EVAL_MODE){ // EVAL模式：KEYS 从栈中弹出成为全局变量 KEYS lua_enablereadonlytable(lua, LUA_GLOBALSINDEX, 0); lua_setglobal(lua,\u0026#34;KEYS\u0026#34;); lua_enablereadonlytable(lua, LUA_GLOBALSINDEX, 1); } // 下同，创建一个数组，用于存储 args luaCreateArray(lua,args,nargs); if (run_ctx-\u0026gt;flags \u0026amp; SCRIPT_EVAL_MODE){ lua_enablereadonlytable(lua, LUA_GLOBALSINDEX, 0); lua_setglobal(lua,\u0026#34;ARGV\u0026#34;); lua_enablereadonlytable(lua, LUA_GLOBALSINDEX, 1); } /* 执行 lua 脚本 * 如果是 EVAL 模式, 那么 lua 代码应该没有参数，然后接受一个返回值，此时 error_handler 在 -2 的位置。 * 如果是 FUNCTION 模式，那么 lua 代码应该有两个参数（keys 和 args 数组），然后接受一个返回值， * 此时 error_handler 在 -4 (stack: error_handler, callback, keys, args) 的位置。 */ int err; if (run_ctx-\u0026gt;flags \u0026amp; SCRIPT_EVAL_MODE) { err = lua_pcall(lua,0,1,-2); } else { err = lua_pcall(lua,2,1,-4); } // ... } 动手实践 # 使用 c 编写一个生成 snowflake ID 的库， 同时提供 lua 扩展，主要代码如下：\n完整代码可以在 yeqown/snowflake/contrib/lua 查看\nrequire \u0026#34;snowflake\u0026#34; print(\u0026#34; NAME: \u0026#34; .. snowflake._NAME) print(\u0026#34;VERSION: \u0026#34; .. snowflake._VERSION) print(\u0026#34; AUTHOR: \u0026#34; .. snowflake._AUTHOR) local worker = snowflake.new(1) local id = worker.next_id() print(\u0026#34;generating one id: \u0026#34; .. id .. \u0026#34;\\n\u0026#34;) print(\u0026#34;parsing snowflake id: \u0026#34; .. id) local state = snowflake.parse(id); print(\u0026#34;timestamp: \u0026#34; .. state.timestamp) print(\u0026#34;worker_id: \u0026#34; .. state.worker_id) print(\u0026#34; count: \u0026#34; .. state.count .. \u0026#34;\\n\u0026#34;) print(\u0026#34;generating 10 ids in a batch: \u0026#34;) local ids = worker.next_ids(10) for i, id in ipairs(ids) do print(id) end 运行效果如下：\n/* * Usage in lua: * worker = snowflake.new(1?) */ int l_snowflake_new(lua_State *L) { uint64_t worker_id = 0; // ... lua_newtable(L); snowflake_Worker *worker = snowflake_NewWorker(worker_id); // ... lua_pushlightuserdata(L, worker); luaL_Reg snowflake_worker_funcs[] = { {\u0026#34;next_id\u0026#34;, l_snowflake_worker_next_id}, {\u0026#34;next_ids\u0026#34;, l_snowflake_worker_next_ids}, {NULL, NULL}, }; luaL_setfuncs(L, snowflake_worker_funcs, 1); return 1; } /* * Usage in Lua: * snowflake.parse() */ int l_snowflake_parse(lua_State *L) { uint64_t id = (uint64_t) lua_tointegerx(L, -1, NULL); // ... lua_pop(L, -1); const snowflake_IDState *state = snowflake_ParseId((snowflake_ID) id); lua_newtable(L); lua_pushinteger(L, (long long) state-\u0026gt;timestamp); lua_setfield(L, -1, \u0026#34;timestamp\u0026#34;); lua_pushinteger(L, (long long) state-\u0026gt;worker_id); lua_setfield(L, -1, \u0026#34;worker_id\u0026#34;); lua_pushinteger(L, (long long) state-\u0026gt;count); lua_setfield(L, -1, \u0026#34;count\u0026#34;); return 1; } /* * Usage in lua: * local worker = snowflake.new(1) * local id = worker.next_id() */ int l_snowflake_worker_next_id(lua_State *L) { snowflake_Worker *worker = (snowflake_Worker *) lua_touserdata(L, lua_upvalueindex(1)); // ... snowflake_ID id = snowflake_NextId(worker, true); // ... lua_pushinteger(L, (long long) id); return 1; } /* * Usage in Lua: * local worker = snowflake.new(1) * local ids = worker.next_ids(10) */ int l_snowflake_worker_next_ids(lua_State *L) { // ... uint64_t count = (uint64_t) lua_tointegerx(L, -1, NULL); // ... lua_pop(L, -1); snowflake_Worker *worker = (snowflake_Worker *) lua_touserdata(L, lua_upvalueindex(1)); // ... snowflake_ID *ids = malloc(sizeof(snowflake_ID) * count); if (snowflake_NextIds(worker, count, ids, false) \u0026lt;= 0) { lua_pushstring(L, \u0026#34;failed to generate snowflake ids\u0026#34;); lua_error(L); } lua_newtable(L); for (uint64_t i = 0; i \u0026lt; count; i++) { lua_pushinteger(L, (long long) ids[i]); lua_rawseti(L, -2, i + 1); } free(ids); return 1; } int luaopen_snowflake(lua_State *L) { luaL_Reg snowflake_lib[] = { {\u0026#34;new\u0026#34;, l_snowflake_new}, {\u0026#34;parse\u0026#34;, l_snowflake_parse}, {NULL, NULL} }; luaL_newlib(L, snowflake_lib); lua_pushliteral(L, \u0026#34;_AUTHOR\u0026#34;); lua_setfield(L, -2, \u0026#34;yeqown\u0026#34;); lua_pushliteral(L, \u0026#34;lua-snowflake\u0026#34;); lua_setfield(L, -2, \u0026#34;_NAME\u0026#34;); lua_pushliteral(L, \u0026#34;0.1.0\u0026#34;); lua_setfield(L, -2, \u0026#34;_VERSION\u0026#34;); // set snowflake as global variable lua_pushvalue(L, -1); lua_setglobal(L, \u0026#34;snowflake\u0026#34;); return 1; } 这里就不再展开讲解了，里面的每一部分都是前面讲解的内容，这里只是做了一个简单的整合。当然实际扩展的时候，不会这么简单，可能需要处理复杂的数据结构，繁琐的流程，但这不在本文的讨论范围内，请善用工具和官方文档。\n总结 # lua 本身就是由 C 语言编写的，因此 lua 和 C 之间的交互是非常方便的。而他们交互的核心方式就是通过 lua 的虚拟栈来传递数据，然后通过 lua 的 API 来操作虚拟栈。\n参考 # lua.org manual v5.4 C-API 和 auxiliary library Programming in Lua Part IV - The C API The Application API "},{"id":16,"href":"/2023/08/17/tcp-%E9%95%BF%E8%BF%9E%E6%8E%A5%E6%9C%8D%E5%8A%A1%E4%BC%98%E9%9B%85%E9%87%8D%E5%90%AF%E7%9A%84%E7%A7%98%E5%AF%86/","title":"Tcp 长连接服务优雅重启的秘密","section":"Posts","content":"假设我们有一个长连接服务，我们想要对它进行升级，但是不想让客户端受到影响应该怎么做？这个问题其实是一个很常见的问题，比如我们的游戏服务器，我们的 IM 服务器，推送服务器等等，诸如此类使用tcp长连接的服务，都会遇到这个问题。那么我们应该怎么做呢？\n需求分析 # 我们可以先来看下这个场景下的需求：\n客户端必须要对这个操作没有感知，也就是说客户端不需要做任何的修改，在服务器升级的过程中不需要配合。 服务器在升级的过程中，不能丢失任何的连接，也就是说，如果有新的连接进来，那么这个连接必须要被接受，如果有旧的连接，那么客户端不能够触发重连。 基本思路 # 实现思路的讨论范围限制在 linux 服务器上\n为了实现上述的要求，首先在升级流程中我们需要做到以下几点：\n旧的服务器进程在处理完请求前不能退出，而且一旦升级开始就不能再接受新的连接。 旧的服务器进程在所有连接都处理完毕后才能退出。 新的服务器进程在启动时需要继承旧的服务器进程的所有连接，新的连接也应该被新的服务器进程接受。 新的服务器进程也必须监听旧的服务器进程的监听端口，否则新的连接无法被接受。 那么通过 Google 和 ChatGPT 的帮助，我们可以找到一些思路：\n新进程继承旧进程的（监听）套接字，而不是创建新的。\n为什么不创建新的（监听）套接字呢？在 linux 中内核会把处在不同握手阶段的TCP连接放在不同的队列中（半连接/全连接）。服务器的监听套接字会有自己的队列，如果创建新的套接字，那么旧的套接字队列中的连接就会丢失。为了做到客户端无感知，我们需要继承旧的套接字（主要是为了连接队列中的连接不丢失）。\n半连接队列：当客户端发送 SYN 包时，服务器会把这个连接放在半连接队列中，等待服务器的 ACK 包，这个时候连接处于半连接状态。当服务器发送 ACK 包时，这个连接就会从半连接队列中移除，放到全连接队列中，这个时候连接处于全连接状态。当服务器调用 accept 时，就会从全连接队列中取出一个连接，这个时候连接处于 ESTABLISHED 状态。\n实现方式 # 那么在 linux 中，我们可以通过以如下方式实现：\n通过 fork 创建子进程，子进程继承父进程的所有资源，包括监听套接字; 子进程通过 exec 加载最新的二进制程序执行，这样就实现了新进程继承旧进程的监听套接字。 新进程启动完成后，通知父进程退出。 父进程受到信号后，停止接受新的连接，等待所有的连接处理完毕后退出。 在 Go 里面，我们可以通过如下方式实现：\ntype gracefulTcpServer struct { listener *net.TCPListener shutdownChan chan struct{} conns map[net.Conn]struct{} servingConnCount atomic.Int32 serveRunning atomic.Bool } // 普通启动方式 func start(port int) (*gracefulTcpServer, error) { ln, err := net.Listen(\u0026#34;tcp\u0026#34;, fmt.Sprintf(\u0026#34;:%d\u0026#34;, port)) // handle error ignored s := \u0026amp;gracefulTcpServer{ listener: ln.(*net.TCPListener), shutdownChan: make(chan struct{}, 1), conns: make(map[net.Conn]struct{}, 16), servingConnCount: atomic.Int32{}, serveRunning: atomic.Bool{}, } return s, nil } // 优雅重启启动方式 func startFromFork() (*gracefulTcpServer, error) { // ... ignored code // 从环境变量中获取 父进程的处理的连接数，用来恢复连接 if nfdStr := os.Getenv(__GRACE_ENV_NFDS); nfdStr == \u0026#34;\u0026#34; { panic(\u0026#34;not nfds env\u0026#34;) } else if nfd, err = strconv.Atoi(nfdStr); err != nil { panic(err) } // restore conn fds, 0, 1, 2 has been used by os.Stdin, os.Stdout, os.Stderr lfd := os.NewFile(3, filepath.Join(tmpdir, \u0026#34;graceful\u0026#34;)) ln, err := net.FileListener(lfd) // handle error ignored s := \u0026amp;gracefulTcpServer{ listener: ln.(*net.TCPListener), shutdownChan: make(chan struct{}, 1), conns: make(map[net.Conn]struct{}, 16), servingConnCount: atomic.Int32{}, serveRunning: atomic.Bool{}, } // 从父进程继承的套接字中恢复连接 for i := 0; i \u0026lt; nfd; i++ { fd := os.NewFile(uintptr(4+i), filepath.Join(tmpdir, strconv.Itoa(4+i))) conn, err := net.FileConn(fd) // handle error ignored go s.handleConn(conn) } return s, nil } func (s *gracefulTcpServer) gracefulRestart() { _ = s.listener.SetDeadline(time.Now()) lfd, err := s.listener.File() // 给子进程设置 优雅重启 相关的环境变量 os.Setenv(__GRACE_ENV_FLAG, \u0026#34;true\u0026#34;) os.Setenv(__GRACE_ENV_NFDS, strconv.Itoa(len(s.conns))) // 将父进程的监听套接字传递给子进程 files := make([]uintptr, 4, 3+1+len(s.conns)) copy(files[:4], []uintptr{ os.Stdin.Fd(), os.Stdout.Fd(), os.Stderr.Fd(), lfd.Fd(), }) // 将父进程的套接字传递给子进程 for conn := range s.conns { connFd, _ := conn.(*net.TCPConn).File() files = append(files, connFd.Fd()) } procAttr := \u0026amp;syscall.ProcAttr{ Env: os.Environ(), Files: files, Sys: nil, } // 执行 fork + exec 调用 childPid, err := syscall.ForkExec(os.Args[0], os.Args, procAttr) } func main() { // ... // 根据环境变量判断是 fork 还是新启动 if v := os.Getenv(__GRACE_ENV_FLAG); v != \u0026#34;\u0026#34; { s, err = startFromFork() } else { s, err = start(*port) } go s.serve() // 处理信号，如果是 SIGHUP 信号，则执行 gracefulRestart 方法后再退出 s.waitForSignals() } 完整代码可以在 https://github.com/yeqown/playground/golang/tcp-graceful-restart 中找到。\nsyscall.ForkExec # 通过追踪 syscall.ForkExec 的源码，我们可以看到它的实现主体是 forkAndExecInChild, 它的实现如下：\nfunc forkAndExecInChild(argv0 *byte, argv, envv []*byte, chroot, dir *byte, attr *ProcAttr, sys *SysProcAttr, pipe int) (pid int, err Errno) { // ... some ignored codes fd := make([]int, len(attr.Files)) nextfd = len(attr.Files) for i, ufd := range attr.Files { if nextfd \u0026lt; int(ufd) { nextfd = int(ufd) } fd[i] = int(ufd) } nextfd++ // 调用 fork runtime_BeforeFork() r1, _, err1 = rawSyscall(abi.FuncPCABI0(libc_fork_trampoline), 0, 0, 0) if err1 != 0 { runtime_AfterFork() return 0, err1 } // fork 的函数原型: pid_t fork(void); // 调用成功后，父进程返回子进程的 pid，子进程返回 0。所以这里通过 r1 的值来判断是父进程还是子进程 // 如果是父进程，那么在这里就返回了，子进程会继续执行下面的代码。 if r1 != 0 { runtime_AfterFork() return int(r1), 0 } // 调用系统调用来设置 子进程 的各种属性： // 会话ID、进程组ID、用户ID、组ID、工作目录、是否前台运行等等。 // 找到 fd[i] \u0026lt; i 并且把他们移到 len(fd) 之后，这样在后面的 dup2 中就不会被 “踩踏”。 // 踩踏：一个变量或者资源被无意中（通常是意外地）覆盖或修改的情况。 for i = 0; i \u0026lt; len(fd); i++ { if fd[i] \u0026gt;= 0 \u0026amp;\u0026amp; fd[i] \u0026lt; i { if nextfd == pipe { // don\u0026#39;t stomp on pipe nextfd++ } if runtime.GOOS == \u0026#34;openbsd\u0026#34; { _, _, err1 = rawSyscall(dupTrampoline, uintptr(fd[i]), uintptr(nextfd), O_CLOEXEC) } else { _, _, err1 = rawSyscall(dupTrampoline, uintptr(fd[i]), uintptr(nextfd), 0) if err1 != 0 { goto childerror } _, _, err1 = rawSyscall(abi.FuncPCABI0(libc_fcntl_trampoline), uintptr(nextfd), F_SETFD, FD_CLOEXEC) } if err1 != 0 { goto childerror } fd[i] = nextfd nextfd++ } } // 调用 dup2 来复制父进程的文件描述符，也就是 gracefulTcpServer.gracefulRestart 中传递给子进程的文件描述符 for i = 0; i \u0026lt; len(fd); i++ { // ... some ignored codes // 基本只有 0，1，2 会触发这里的逻辑 if fd[i] == i { // dup2(i, i) won\u0026#39;t clear close-on-exec flag on Linux, // probably not elsewhere either. _, _, err1 = rawSyscall(abi.FuncPCABI0(libc_fcntl_trampoline), uintptr(fd[i]), F_SETFD, 0) if err1 != 0 { goto childerror } continue } // 其他的文件描述符，监听套接字 + 连接套接字采用 dup2 来复制 // 这样父进程退出后，子进程还可以继续使用这些套接字 _, _, err1 = rawSyscall(abi.FuncPCABI0(libc_dup2_trampoline), uintptr(fd[i]), uintptr(i), 0) if err1 != 0 { goto childerror } } // ... some ignored codes // 调用 execve 系统调用来执行新的程序 _, _, err1 = rawSyscall(abi.FuncPCABI0(libc_execve_trampoline), uintptr(unsafe.Pointer(argv0)), uintptr(unsafe.Pointer(\u0026amp;argv[0])), uintptr(unsafe.Pointer(\u0026amp;envv[0]))) childerror: // send error code on pipe rawSyscall(abi.FuncPCABI0(libc_write_trampoline), uintptr(pipe), uintptr(unsafe.Pointer(\u0026amp;err1)), unsafe.Sizeof(err1)) for { rawSyscall(abi.FuncPCABI0(libc_exit_trampoline), 253, 0, 0) } } 留个问题：这里为什么要对 fd[i] 数组进行处理呢？这些处理的效果是什么？\n后文有答案哦\n一些系统调用 # 在探究 go 底层 fork + exec 的实现时，我们发现了一些系统调用，那么每个系统调用的函数原型和使用场景是什么呢？\n系统调用 函数原型 说明 fork pid_t fork(void); 创建一个子进程，子进程会继承父进程的所有资源，包括文件描述符、内存、信号处理等等。 execve int execve(const char *filename, char *const argv[], char *const envp[]); 执行一个新的程序，这个程序会替换当前进程的内存空间，但是会继承父进程的文件描述符、信号处理等等。如果成功这个函数不会返回，否则返回错误标志 dup int dup(int oldfd); 复制文件描述符，oldfd 是文件描述符，dup 会把 oldfd 复制到当前进程中，返回新的文件描述符。 dup2 int dup2(int oldfd, int newfd); 复制文件描述符，oldfd 和 newfd 都是文件描述符，dup2 会把 oldfd 复制到 newfd，如果 newfd 已经打开，那么会先关闭 newfd。 fcntl int fcntl(int fd, int cmd, ... /* arg */ ); 对文件描述符进行各种操作，比如设置文件描述符的属性、获取文件描述符的属性等等。 close int close(int fd); 关闭文件描述符。 tableflip 源码分析 # cloudflare/tableflip 是一个 Go 的库，用来实现 tcp 长连接服务的优雅重启。它的源码可以在 https://github.com/cloudflare/tableflip 中找到。\n如下是 tableflip 作为一个库的使用方式：\nfunc main() { upg, _ := tableflip.New(tableflip.Options{}) defer upg.Stop() go func() { sig := make(chan os.Signal, 1) signal.Notify(sig, syscall.SIGHUP) for range sig { upg.Upgrade() } }() // Listen must be called before Ready ln, _ := upg.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;localhost:8080\u0026#34;) defer ln.Close() // 运行服务 go http.Serve(ln, nil) if err := upg.Ready(); err != nil { panic(err) } \u0026lt;-upg.Exit() } 结合之前提到的原理，我们可以看到 tableflip 的 API 跟我们之前实现的 demo 流程是类似的，因此这里我们只分析部分源码。\nListen Listen 是用于创建监听套接字的，tableflip 通过一个 Fds 的结构集中管理了子进程从父进程继承来的文件描述符，同时也是父进程向子进程复制文件描述符的数据结构：\ntype Upgrader struct { // ... // Upgrader.Listen 调用使用的嵌入字段 Fds 的方法 *Fds // ... } // Listen 返回从父进程继承来的文件描述符，如果没有继承来的文件描述符，那么就创建一个新的文件描述符。 func (f *Fds) Listen(network, addr string) (net.Listener, error) { return f.ListenWithCallback(network, addr, f.newListener) } func (f *Fds) ListenWithCallback(network, addr string, callback func(network, addr string) (net.Listener, error)) (net.Listener, error) { // 从父进程继承来的文件描述符尝试获取监听套接字 ln, err := f.listenerLocked(network, addr) if err != nil { return nil, err } // 如果找到那么就返回 if ln != nil { return ln, nil } // 调用回调函数 cllback(f.newListener) 来创建新的监听套接字 // 这里的 f.newListener = func(network, addr string) (net.Listener, error) { // f.lc 默认是 空的 net.ListenConfig, 等价于 net.Listen(network, addr) //\treturn f.lc.Listen(context.Background(), network, addr) // } ln, err = callback(network, addr) if err != nil { return nil, fmt.Errorf(\u0026#34;can\u0026#39;t create new listener: %s\u0026#34;, err) } if _, ok := ln.(Listener); !ok { ln.Close() return nil, fmt.Errorf(\u0026#34;%T doesn\u0026#39;t implement tableflip.Listener\u0026#34;, ln) } // 加入到文件描述符集合中 err = f.addListenerLocked(network, addr, ln.(Listener)) if err != nil { ln.Close() return nil, err } return ln, nil } Upgrade Upgrade 是触发优雅重启的入口，主要是发送一个升级信号，而 upgrade 信号的处理逻辑在 upgrader.run 中处理的：\nfunc (u *Upgrader) Upgrade() error { // 需要注意的是 upgradeC 是一个 chan chan error 类型的，也就是说它是一个 chan 的 chan response := make(chan error, 1) // 这里事先检查下当前进程是否处于 退出/升级 状态 select { case \u0026lt;-u.stopC: return errors.New(\u0026#34;terminating\u0026#34;) case \u0026lt;-u.exitC: return errors.New(\u0026#34;already upgraded\u0026#34;) case u.upgradeC \u0026lt;- response: } // 阻塞在这里，等待升级结果 return \u0026lt;-response } func (u *Upgrader) run() { // ... some ignored codes for { select { case \u0026lt;-parentExited: // ... case \u0026lt;-processReady: // ... case \u0026lt;-u.stopC: u.Fds.closeAndRemoveUsed() return case request := \u0026lt;-u.upgradeC: // 一些异常情况检测 // 执行升级 file, err := u.doUpgrade() request \u0026lt;- err if err == nil { // 告诉子进程，父进程已经退出 u.exitFd \u0026lt;- neverCloseThisFile{file} u.Fds.closeUsed() return } } } } func (u *Upgrader) doUpgrade() (*os.File, error) { // 启动子进程 // u.env 主要是对 startChild 需要的一些调用的抽象集合，为了适应不同的平台 // u.Fds.copy() 是复制一份父进程的文件描述符集合，这样子进程就可以直接使用父进程的文件描述符 child, err := startChild(u.env, u.Fds.copy()) if err != nil { return nil, fmt.Errorf(\u0026#34;can\u0026#39;t start child: %s\u0026#34;, err) } // 更多的信号处理，退出/升级 等等 } func startChild(env *env, passedFiles map[fileName]*file) (*child, error) { // 开启一个管道，用来通知父进程子进程已经准备好了 // 所以 readyR 是父进程用来读取的，readyW 是子进程用来写入的，因此 readyW 也需要传递给子进程 readyR, readyW, err := os.Pipe() if err != nil { return nil, fmt.Errorf(\u0026#34;pipe failed: %s\u0026#34;, err) } // 传递文件描述符的名字 namesR, namesW, err := os.Pipe() if err != nil { readyR.Close() readyW.Close() return nil, fmt.Errorf(\u0026#34;pipe failed: %s\u0026#34;, err) } // fds 和 fdNames 保证相同的顺序记录 passedFiles 中的文件描述符 fds := []*os.File{os.Stdin, os.Stdout, os.Stderr, readyW, namesR} var fdNames [][]string for name, file := range passedFiles { nameSlice := make([]string, len(name)) copy(nameSlice, name[:]) fdNames = append(fdNames, nameSlice) fds = append(fds, file.File) } // 传递环境变量，sentinelEnvVar=yes 也就是告诉新的子进程，这是一个升级的子进程 // 在 tableflip.New \u0026gt; newUpgrader \u0026gt; newParent 的开始处可以看到 // 这个变量决定了 Upgrader 中的 parent 字段是否有值，如果有值，那么就是一个优雅升级的子进程 sentinel := fmt.Sprintf(\u0026#34;%s=yes\u0026#34;, sentinelEnvVar) var environ []string for _, val := range env.environ() { if val != sentinel { environ = append(environ, val) } } environ = append(environ, sentinel) // 准备好新的子进程的 command, args 文件描述符，环境变量后就可以启动子进程了 // env.newProc 是一个抽象方法，用来创建子进程 // 默认使用 newOSProcess 其中核心还是调用 syscall.StartProcess 来创建子进程\t// syscall.StartProcess 与 syscall.ForkExec 是一样的 proc, err := env.newProc(os.Args[0], os.Args[1:], fds, environ) if err != nil { // fork/exec 执行失败，那么释放之前创建的资源 // ... return } // 后续，子进程启动成功后就不会执行到这里了，因此这里的代码都是在父进程中执行的 // 也就是说，父进程会想子进程传递一些数据（fdNames）然后等待了来自子进程的信号 exited := make(chan struct{}) result := make(chan error, 1) ready := make(chan *os.File, 1) c := \u0026amp;child{ // ... } go c.writeNames(fdNames) go c.waitExit(result, exited) go c.waitReady(ready) return c, nil } 通过这部分代码，我们可以看到 tableflip 的实现方式跟我们之前的 demo 是类似的，只不过它把一些细节封装了起来，同时也提供了一些额外的功能，比如：\n发送 fdNames 给子进程 父进程等待子进程的 ready 信号 子进程等待父进程的 exited 信号 等等 Ready Ready 使用来通知父进程子进程已经准备好了，这个方法是在子进程中调用的：\n需要注意的是，需要在进程真正的可以接受请求后才调用这个方法，否则可能会出现父进程退出后，新连接无法处理的情况。\nfunc (u *Upgrader) Ready() error { u.readyOnce.Do(func() { u.Fds.closeInherited() close(u.readyC) }) // some ignored codes // 如果没有父进程，那么就直接返回（parent == nil 意味着优雅升级进程来的） if u.parent == nil { return nil } // 通知父进程子进程已经准备好了, 父进程可以退出了 // 这里其实就是创建子进程时开辟的 ready 管道的 write 端写入特定的信号数据 (notifyReady) return u.parent.sendReady() } tableflip 的源码还有很多细节，这里就不一一分析了，有兴趣的可以自己看看，但我们可以发现底层实现的思路是一样的。这里留下一些问题：\ntableflip 在启动子进程时拷贝了 Fds, 但是纵观代码好像没有考虑已经建立的连接，那能否实现连接的继承呢？ 如果说进程中有一个嵌入式数据库，只允许单进程访问，这时候在 tableflip 中会出现什么情况？应该怎么解决呢？ syscall 的一些细节 # 通过前文的讲述，我们了解到了实现优雅重启过程中的一些细节，但更深入的了解还需要我们去看一下 fork 和 exec 的一些细节。下面就对 fork 和 exec 的简单地分析一下。\nfork(2) # fork(2) Linux manual page\n#include \u0026lt;unistd.h\u0026gt; pid_t fork(void); 通常我们使用以下的代码就可以快速的创建一个子进程：\n#include\u0026lt;sys/types.h\u0026gt; #include\u0026lt;unistd.h\u0026gt; #include\u0026lt;stdio.h\u0026gt; #include\u0026lt;stdlib.h\u0026gt; int main() { pid_t pid = fork(); // 创建子进程 if (pid \u0026lt; 0) { perror(\u0026#34;fork error\u0026#34;); exit(1); } if (pid != 0) { // 父进程 printf(\u0026#34;this is parent process, PID is %d.\\n\u0026#34;, getpid()); exit(0); } // 子进程 printf(\u0026#34;this is child process %d, PID is %d.\\n\u0026#34;, i, getpid()); } 我们知道在 linux 内核中，进程的核心结构体是 task_struct 如下：\n// linux-6.4/include/linux/sched.h#739 struct task_struct { //... /* Filesystem information: */ struct fs_struct\t*fs; /* 记录了进程打开的文件 */ struct files_struct\t*files; //... } struct files_struct { //... // 保存了进程打开的文件描述符, 是一个数组，数组的下标就是文件描述符 fd struct file __rcu * fd_array[NR_OPEN_DEFAULT]; } 在 fork 的过程中，通过调用 copy_process 来创建一个新的进程，这个函数的实现如下：\n// linux-6.4/kernel/fork.c#2246 __latent_entropy struct task_struct *copy_process( struct pid *pid, int trace, int node, struct kernel_clone_args *args) { struct task_struct *p; // 拷贝进程打开的文件描述符 retval = copy_files(clone_flags, p, args-\u0026gt;no_files); } 现在我们知道 fork 确实会复制父进程的文件描述符了，那么在子进程里该怎么使用呢？应该还记得我们是怎么恢复监听套接字的吧：\nlistenFile := os.NewFile(3, \u0026#34;/tmp/graceful\u0026#34;) ln, err := net.FileListener(listenFile) 其实这里有一个问题，我们知道对应的是 fd 也就是 files_struct 中 fd_array 的数组下标，fork 会复制父进程的 files_struct，但是为什么是3呢？其实通过断点调试我们可以知道，监听套接字的 fd 并不是 3 而是 10（我本地），那么为什么在新进程里面是 3 呢？这就需要回到 forkAndExecInChild 中对于 fd 数组的处理了，会重新排列 fd 的顺序，并且通过 dup2 系统调用使得 fd[i] = i。\n也就是说假如我传入的 fd 数组为：[0, 1, 2, 4, 5]，在经过处理后进程中的 fd_array 就会变成 [0, 1, 2, 3（dup 4）, 4(dup 5)]\nexecve(2) # execve(2) Linux manual page\n#include \u0026lt;unistd.h\u0026gt; int execve(const char *pathname, char *const _Nullable argv[], char *const _Nullable envp[]); execve 会执行一个新的程序，这个程序会替换当前进程的内存空间，但是会继承父进程的文件描述符、信号处理等等。如果成功这个函数不会返回，否则返回错误标志。\n代码就不贴了～ 跟本文的联系在于可以实现优雅升级：触发升级前，先替换老的二进制文件后，待子进程重新执行时，就实现升级了。\n小结 # tcp 长连接服务的优雅重启，其实就是通过 fork + exec 的方式来实现的。在启动子进程时，可以复制父进程的相关套接字，通过一些手段在子进程中恢复这些套接字，这样子进程就可以继续使用父进程的套接字，从而实现了客户端无感知的优雅重启。\n参考文献 # Graceful upgrades in Go "},{"id":17,"href":"/2023/08/15/cloudflare-tunnel-%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/","title":"Cloudflare Tunnel 使用笔记","section":"Posts","content":"Cloudflare 自不用多说，Tunnel 是 Cloudflare 提供的一项功能，可以将本地的服务通过 Cloudflare 的网络暴露到公网，这样就可以实现内网穿透，同时还可以通过 Cloudflare 的网络加速服务，提高访问速度。\n初识 Cloudflare Tunnel # 最开始接触 Cloudflare Tunnel 是在 Twitter 上看到一个项目cloudflare-tunnel-ingress-controller ，这个项目是一个 Kubernetes 的 Ingress Controller，可以将 Kubernetes 中的服务通过 Cloudflare Tunnel 暴露到公网，这样就可以实现内网穿透，也就是说局域网搭建的服务可以通过 Cloudflare 的网络暴露到公网。\n熟悉内网穿透的小伙伴，应该对这中东西很熟悉，也没什么好说的。\n前提 # 一个 Cloudflare 账号 一个域名 cloudfalred # Mac 上可以通过 brew install cloudflared 安装，安装完成后，可以通过 cloudflared -v 查看版本。\n$ cloudflared -v cloudflared version 2023.7.3 (built 2023-07-25T20:51:49Z) 参考 cloudflared 官方文档 安装。\n$ cloudflared login 使用 # 我的使用场景除了最开始提到的 Kubernetes Ingress Controller 之外，还有一个就是将局域网内的开发机通过 Cloudflare Tunnel 暴露到公网，方便远程开发。\n创建 Tunnel 在 Cloudflare 操作面板中，选择 Zero Trust (Dashboard) \u0026gt; Access \u0026gt; Tunnels 就可以开始创建 tunnel。根据流程一步步执行即可。\n创建期间会让你在本地开启一个 cloudflared 服务\nsudo cloudflared service install tunnel-run-token 远程登录 创建完 tunnel 后，这时候我们只有一个公网的地址, 如 ssh.example.com`` 这时候直接使用 ssh username@ssh.example.com` 是无法登录的。需要使用 cloudflared 作为 ssh 的代理。\nssh -o \u0026#34;ProxyCommand cloudflared access ssh --hostname %h\u0026#34; username@ssh.example.com 这样就可以登录到远程的机器了。\n文件传输 光是能够登录上机器还不满足日常的远程开发工作，往往我们还需要在本地和远程机器之间传输文件，这时候我们可以使用 scp 命令，但也不能直接使用，需要使用 cloudflared 作为代理。同样的，\n# 下载文件 scp -o \u0026#34;ProxyCommand cloudflared access ssh --hostname %h\u0026#34; username@ssh.example.com:/path/to/file . 小结 # cloudflared 的功能远不止此，这里仅仅涉及到 ssh 的部分，后面再有更多场景再补充。\n"},{"id":18,"href":"/2022/07/25/%E5%9C%A8parent-shell%E4%B8%AD%E6%89%A7%E8%A1%8C%E5%86%85%E7%BD%AE%E5%91%BD%E4%BB%A4%E7%9A%84%E6%96%B9%E6%B3%95/","title":"在Parent Shell中执行内置命令的方法","section":"Posts","content":" 背景 # 最近在为 大仓项目(Monorepo) 制作一个脚手架，其中构思了一个自动替用户切换工作路径的工具（代码是通过模板初始化的，在结构上基本一致，但是代码文件较深，想要在terminal去和文件交互时，只能使用 cd 命令，费时费力），因此我期望一个小工具，能比较方便的帮我跳转到目标路径。预期的使用效果如下：\n这个命令执行后，你可以从当前路径（任意路径）跳转到目标路径，那我就不用记我应该先跳转到根目录再前往目标目录了，少敲击很多次 tab。\nPS: 后续计划给这个命令扩展一下历史记录，可以通过筛选匹配历史快速补全命令，提高效率。\nchdir没作用？ # 这个小工具是通过Go来编写的，我从文档中看到 os.Chdir 这个调用, 因此用它来试试：\n// Chdir changes the current working directory to the named directory. // If there is an error, it will be of type *PathError. func Chdir(dir string) error 结果我发现没有效果，通过 os.Getwd() 却能看到当前路径已经被改变了。这个切换的需求还没解决，我却产生了几个新的疑问：\ncd 命令怎么实现的？ chdir 系统调用的使用和原理？ 通过执行如下的命令，会发现 cd 命令并不是通过一个独立的可执行文件来实现的，而是内置在 bash 等 shell 程序中，其次通过查阅资料可以发现 大部分Linux发行版都部分符合 POSIX 标准。如果再检索一下shell程序的原理，我们会得出如下的结论：\n$ which cd cd: shell built-in command shell 程序在执行内建命令时，是通过调用内部的函数的执行。而非内建的命令（如 git / gcc / gdb）都是通过 fork 一个进程来执行；cd 命令是通过 chdir [IEEE Std 1003.1-1988] 系统调用实现的；chdir 在 \u0026lt;uinstd.h\u0026gt; 头文件中定义；\n说人话：在另一个进程里执行 chdir 系统调用，只会影响当前进程而不是影响其他的进程（包括父进程）。\nPOSIX: is a family of standards specified by the IEEE Computer Society for maintaining compatibility between operating systems.POSIX defines both the system- and user-level application programming interfaces (API), along with command line shells and utility interfaces, for software compatibility (portability) with variants of Unix and other operating systems.\n解决方案 # 那么走编程语言提供的系统调用这条路已经不行了，那么应该咋办呢？在stackoverflow 上搜索良久，收集到如下的方法：\ngdb attach 到进程上，再执行 chdir 系统调用 通过 bash 脚本执行 source 命令 执行系统调用 ioctl 和 terminal-session 的 tty 文件交互 通过 alias 来实现: pcd=cd $(program command)) 其中各种方法尝试的过程就略去不说，最终采用了第三种非常好用，下面提供一个小 demo 以供使用：\n#include \u0026lt;unistd.h\u0026gt; #include \u0026lt;sys/ioctl.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; int injectCmd(int fd, char* cmd) { int i = 0; while (cmd[i] != \u0026#39;\\0\u0026#39;) { int ret = ioctl(fd, TIOSTI, \u0026amp;cmd[i++]) if (ret != 0) return ret; } return 0; } /* getty help get terminal session tty filename, you can open the fie * and get the file descriptor to call injectCmd. */ char* getty() { const int STDOUT = 1; return ttyname(STDOUT); } void main() { char * ttyfile = getty(); int fd = open(ttyfile, \u0026#34;w\u0026#34;); if (fd == -1) { perror(\u0026#34;open ttyfile failed\u0026#34;); return -1; } injectCmd(fd, \u0026#34;echo \u0026#39;hello world\u0026#39;\u0026#34;); } 在 go 里可以直接使用 syscall 包或者 cgo 来执行，原理是是一致的，不过多赘述。\n参考文献 # https://pubs.opengroup.org/onlinepubs/009696799/basedefs/unistd.h.html https://stackoverflow.com/questions/2375003/how-do-i-set-the-working-directory-of-the-parent-process/65406229#65406229 https://en.wikipedia.org/wiki/Process_isolation "},{"id":19,"href":"/2022/01/27/%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E7%B3%BB%E7%BB%9F/","title":"设计一个分布式定时任务系统","section":"Posts","content":" 需求和背景分析 # 一提到定时任务的使用场景，我们肯定能想到很多场景，比如：\n每天晚上12点执行一次清理数据的任务 每天凌晨1点给符合条件的用户发送推广邮件 每个月10号结算工资 每隔5分钟检查一次服务器的状态 每天根据用户的配置，给用户发送站内消息提醒 \u0026hellip; 从常见的场景中，我们可以提炼出一些定时任务的特点：\n序号 特点 说明 1 定时 执行时间有规则 2 可靠 可以延迟执行，但不能不执行；可以不执行，但是不能多执行 3 并发（可能） 某些场景下，可以运行多个 cron 进程来提高执行效率 4 可执行 这个有点废话了，但是这个关系到 cronjob 的设计，因此在这里还是提出来 但是作为一个系统来说，我们需要更多的功能来提升用户体验，保证平台的可靠和稳定。我们设想下以下的场景：\n定时任务已经触发了，但是有没有执行，执行结果是什么？ 如果一个定时任务长时间运行，那么它正常吗？ 一个定时任务还在运行，但是下一个触发时机又到来了，该怎么办？ 如果服务器资源已经处于高位，那么要被触发的任务还触发吗？ \u0026hellip; 最后，基本的访问控制，权限分配和API设计，这些都是系统功能的一部分，但不作为本文的重点考虑对象。\n一些概念 # 到这里我们可以提取一些概念，来帮助我们设计一个系统：\n序号 名词 解释 1 CronJob 定时任务实例，描述定时任务资源的一个实体 2 Job 执行中的定时任务 3 Scheduler 调度器，负责触发 CronJob 执行和监控 Job 的执行状态；或许还会存储一些 CronJob 的状态 4 JobRuntime job 运行时，准备 Job 运行时需要的资源; 可以参考 k8s CRI 这里 JobRuntime 是抽象化的概念，因为 Job 可以是k8s上的POD，也可以是物理机的进程，还可以是一个进程中的coroutine。\n这几个概念的关联关系如图：\n总结: 把定时任务平台分为了运行前和运行中两个阶段，通过调度器居中协调，达到两个目的：\nScheduler 不负责 CronJob 的运行，减轻 Scheduler 的职责。 JobRuntime 抽象化并独立设计，方便扩展。 关键点 # 再总结一下设计过程中的关键点，如下图。\n可以先忽略分布式相关的要点（large scale, reliable），先重点看下功能性的要点。\nCronJob Track「定时任务跟踪」\n感知 CronJob 的状态，并记录状态变化的历史记录，用于辅助 scheduler 实现一些策略调度。同时也能提供一些 CronJob 的运行数据，帮助用户调试和了解 CronJob 的健康状况。\nIdempotent「幂等保证」\n某些定时任务对执行次数是有要求的，作为提供服务的平台来说，需要尽最大努力保证任务执行。因此提出一些 “幂等级别” 供不同的定时任务使用。\n只执行一次。一个 \u0026ldquo;CronJob\u0026rdquo; 一定会触发一次，但是这个比较难保证。 最多一次。可以不触发，但是不能触发多次。 可以重复执行。可以触发多次。 Realtime「实时性」\nCronJob 对于执行的时机也有要求，如果 CronJob 要求1分钟内执行完毕，如果1分钟后再运行会导致时效性的问题，那么平台也应该考虑相关的设计。\nFault Tolerance「容错设计」\nCronJob 可能会因为一些外部因素 “执行失败” 或者 “超时”，这时候需要提供一些容错机制，比如：失败重试，超时关闭。\nCocurrency「并发控制」\n好比于，k8s 中 Deployment 作为一个无状态服务，可以运行多个副本以提高效率；但是相对的，Statefule Set 作为有状态的应用，POD不能随意的扩容，所以需要提供一些并发控制机制，严格的限制 CronJob 的并发数量。\nPre-Allocated「预分配」\n同样的，某些 CronJob 的优先级比较高，要保证这些任务一直稳定运行。那么需要提供一些预分配机制，可以避免再系统资源处于高位的时候，这些 CronJob 还能继续被调度运行。\n总结： 这些要点体现在设计上就是，Schuduler 对 CronJob 的调度控制，来自于 CronJob 的配置和运行状态。CronJob Configuration 包含一些辅助调度控制的配置，如：\n{ // ... \u0026#34;concurrency\u0026#34;: 1, // 1 表示这个 CronJob 只能有一个运行中的 Job 实例。 \u0026#34;preAllocated\u0026#34;: true, // true 表示这个 CronJob 可以被预分配，这样可以避免系统资源被占满，导致这个 CronJob 无法调度。 \u0026#34;idempotent\u0026#34;: \u0026#34;onceMost\u0026#34;, // [onceOnly, onceMost, unlimit] \u0026#34;faultTolerance\u0026#34;: { \u0026#34;maxStartupRetries\u0026#34;: 3, // 最大启动失败重试次数 \u0026#34;maxFailureRetries\u0026#34;: 3, // 最大执行失败重试次数 \u0026#34;maxFailureRetriesSeconds\u0026#34;: \u0026#34;60s\u0026#34;, // 最大执行失败重试间隔 \u0026#34;activeDeadlineSeconds\u0026#34;: \u0026#34;120s\u0026#34;, // 最大执行时间，超过这个时间就会被认为是失败 }, // 容错配置 // ... \u0026#34;resourceLimit\u0026#34;: { \u0026#34;cpu\u0026#34;: \u0026#34;0.1\u0026#34;, // CPU 限制 \u0026#34;memory\u0026#34;: \u0026#34;128Mi\u0026#34;, // 内存限制 }, // 资源限制 } 向分布式演进 # 到目前为止，这个定时任务系统的功能已经差不多了，那么是不是意味着这样的设计已经满足需求了？当然没有，比如：\n单个服务器要承担 交互 、调度 、执行 的压力，那么它能承载的定时任务数量势必会被限制。 同样，如果这一台机器出现问题，那么不是就导致所有的定时任务都无法正常运行，影响整个系统的运行。 同样，如果我们需求的是一个 PasS 平台，单机不可能 邻近 所有的用户，那么用户请求就很可能经过一些低俗和拥堵的网络链路。 \u0026hellip; 简单的说就是单点问题～，因此我们需要 分布式 设计。\n那么怎么把它演进成一个分布式系统呢？\n首先应用 CAP 理论，我们可以先确认这个系统应该是 CP 还是 AP 系统呢？对于定时任务系统，高可用不是最重要的，反而是可靠性非常重要。那么可靠性是怎么体现的呢？\n定时任务不会因为某些节点宕机而不执行 强调了最多运行一次，也不会被反复调度执行 因为系统异常而未被执行，可以被重新调度执行 \u0026hellip; 因此，我们不需要这个系统有非常高的可用性，偶尔不可用是可以接受的（这是由定时任务的性质决定的）。\n对一致性的思考 # 思考下，这里是否需要一致性？如果需要，那么是强一致还是最终一致呢？ 按照我目前的理解，分布式分为两种（无状态和有状态），针对有状态的服务，可能需要强一致性，也可能最终一致就能满足服务正常运行的需求了。\n这里我们分为几个问题来一步步理解：\n分布式设计之后，对定时任务系统带来的问题是什么？ 按照我们上面的抽象，怎么确定由哪个节点来承担 scheduler 的职责？\u0026ndash; 选主算法 如果 scheduler 宕机，应该由谁来接替它的工作？\u0026ndash; 选主算法 接替者如何“正确的继续”（崩溃时，被调度未结束的和调度中的任务如何处理）？\u0026ndash; 集群状态 如果集群整体崩溃，那么下次集群重启时，如何弥补宕机期间的任务执行？\u0026ndash; 集群状态 分布式集群有哪些 状态 ？ 任务队列（调度器） 调度中/执行中的任务（调度器） 集群中节点清单和节点状态 集群时间/上一次调度时间戳（调度器） \u0026hellip; 等等跟运行相关的数据 通过上面的QA，再来思考下分布式定时任务平台对于一致性的要求。如果你把整个系统的状态或者日志，放在外部的存储系统中，那么对于定时任务平台也没有特别的一致性要求，因为已经由外部存储系统来维护了。但是如果是保持在系统内部，那么就有要求了，如果说系统中各个节点对于集群的状态存储不一致，那么一旦执行调度任务的节点宕机，其他节点执行调度时上，这样就会导致定时任务的调度出现不一致，这就是一致性的问题。\n按照我们的分析，设计的分布式定时任务系统算是一个 CP 系统。因此在设计的时候（暂定为集群内部维护状态），我们通过引入一致性协议来解决一致性问题。如果你学习过分布式理论，亦或者学习过一些分布式软件，如 k8s / etcd / consul 等等，就应该知道 Raft 和 Paxos 这两个一致性协议。\nRaft 或者 Paxos 是通过 Leader-Follower 的方式来实现强一致性，那么我们引入 Raft 之后就有如下的架构：\n至此，整个分布式定时任务的框架已经成型，我们可以开始设计定时任务的调度器了。\n定时任务的调度器 # 思考下，调度器的职责是什么？\n调度器的职责说简单点就是，在确定在什么时间在哪个节点上运行某个定时任务。时间根据cron规则和cron任务配置的策略来确定，而节点是由调度器根据每个节点的资源负载同时配合cron任务配置的策略来确定。\n但是，只是这样而已吗？别忘了现在已经是个分布式集群了，调度器是由leader节点来运行的。如果leadership发生变化，那么接替者还应该检查上一任正在处理的事情，并且把它接着做。举个例子：\nA、B、C 三个节点，A作为第一任leader，A正在处理任务1且尚未分配到具体节点执行。这时A宕机了，B作为接替者，那么B成为leader后运行调度器，首先要做的是就是重建调度状态也就是集群状态，并根据集群状态来继续调度。那么怎么重建？\n加载所有的定时任务，重建时间轮或者定时器。 检查 “运行中队列” 检查 “待调度队列” 检查 “重试队列” 其他数据加载（调度时间） 队列为空，当然是最简单的状况，复杂的就是队列不为空情况下怎么处理？运行中不为空，那么恢复结果同步协程；待调度不为空，那么根据调度算法和任务配置的策略来调度；重试队列不为空，那么根据调度算法和任务配置的策略来调度。\n任务配置的策略，如不能并发；可以重试；最晚调度时间等等，都会参与调度算法。\n用代码来描述的话，就是：\ndef schedule(self): if leadership_changed: # cluster first run or leadership changed self.start_cron_timer() self.rebuild_wait_scheduled_queue() self.rebuild_retry_queue() self.rebuild_running_queue() for True: job = self.get_from_wait_schduled_or_retry_queue() if job is None: backoff_delay() continue node_id = self.match_node(job) # build cronjob\u0026#39;s RunContext, put it into running queue and # start a coroutine to synchroize the cronjob\u0026#39;s running result. self.schedule_to_node(node_id, job) 定时任务运行时 # 任务运行时是为了方便抽象和扩展而来的一个概念，它的职责是接受Leader任务调度请求，运行定时任务并且同步运行结果。但运行时不一定是要集群内的服务器来担任，也可以是外部的系统，如：k8s, containerd, docker, etc，内部只需要把外部系统包装成符合运行时要求的资源即可。\n这里，我们假定定时任务的资源类型为容器镜像，运行环境自然就是容器运行时。那么定时任务的运行时，就只需要包装一下k8s的API或者定义CRD即可。\ntype JobRuntime interface { // running job RunJobAsync(ctx context.Context, job *Job, resultCh chan\u0026lt;- *JobResult) (err error) // cancel job (timeout, etc) CancelJob(job *Job, reason Reason) (err error) // query job\u0026#39;s status (running, success, failed, etc) QueryJobResult(job *Job) (result *JobResult, err error) } 因为运行时也算一个无状态节点，所以也没有太多的设计要素。\nKubernetes CronJob # 如果对k8s稍微了解一点，肯定会好奇，为啥不直接使用k8s的cronjob来满足定时任务系统的使用场景呢？当然是可以的，只是这里的主要目的是从头梳理一个（分布式）定时任务系统的功能和设计。在一般场景下 k8s cronjob 已经能够满足使用了，只有个别场景不能直接支持：\n预分配（优先保证重要的定时任务资源） 幂等保证（并发/幂等精细控制） 实时性（超时退出，过时不启动） spec.startingDeadlineSeconds 异常告警 另外就是在使用上没有那么“方便”（运维可不一定会让开发直接接触k8s~）。\n关于 CronJob 的详细控制可以参见 https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/cron-job-v1/#CronJobSpec\n个人以为，中小型公司没必要花费大精力去开发分分布式定时任务系统，只需要在k8s的 CronJob/Job 的基础上稍微包装一下自己的控制模块就能满足要求。\n总结 # 定时任务系统是最常用的服务之一，实现方式可大可小，也可以使用linux crontab 服务。本文在不依赖任何服务的前提下，来设计一个分布式定时任务系统，从功能到架构一一梳理。当然，文中提到的功能清单不一定全，只是站在我的角度看到的必要提及的通用功能。架构也不是一成不变的，可以根据自己的场景和目的来设计，只要达到目的即可。\n水平有限，如有错误，欢迎勘误指正。\n参考文献 # https://xargin.com/google-cron-design/ https://queue.acm.org/detail.cfm?id=2745840 https://kubernetes.io/zh/docs/concepts/workloads/controllers/cron-jobs/ "},{"id":20,"href":"/2022/01/25/protoc-gen-fieldmask%E6%8F%92%E4%BB%B6/","title":"protoc-gen-fieldmask插件","section":"Posts","content":" 背景 # gRPC 作为服务端的常用框架，它通过 protocol-buffers 语言来定义服务，同时也约定了请求和响应的格式，这样在服务端和客户端之间就可以通过 protoc 生成的代码直接运行而不用考虑编码传输问题了。\n但是，可能会遇到这样的场景：\nRPC 响应中 无用的字段过多 , 浪费带宽和无效计算，如下图所示：\n这里的无用字段是指，在响应中，没有用到的字段，这些字段可以忽略掉，不会影响客户端的使用。\n或许 拆分接口 是一个好的办法，但是可能会因为这样那样的原因（信息粒度降低导致接口太多了，有些地方就是需要聚合信息；细粒度的API设计同时会导致代码重复增加），可能无法推动拆分改造。同时如果没有拆分标准，亦或团队内成员不能严格遵守标准，那么拆分也只是重复问题而已。\nRPC 增量更新时，如何判断零值字段是否需要更新？\n对于 unset 和 zero value 不好区分的语言中（比如：go），在提供服务的一方遇到 增量更新 的场景时就会遇到这样的情况：\n对于这种情况当然可以也有一些方法来解决，比如：使用指针来定义数据基本类型，那么在使用的时候如果判定为 nil 就说明没有设置，如果不为 nil 且为零值，那么就说明也是需要更新的。不过这样解决的缺点就是，nil refference panic 的概率又增加了，在使用时也稍微麻烦了一点。\n·\n解决方案 # 其实我们在思考上述两种场景的时候，把 客户端 和 服务端 的角色提取出来，就会发现这两个场景都是从 服务端 的视角遇到的问题，两个场景都是类似的：\n客户端需要哪些字段，服务端不知道 客户端更新了哪些字段，服务端也不知道 但是，其实客户端是知道的，因此让客户端把这部分信息传递给服务端就行了。因此我们可以用 FieldMask 字段，用来传递客户端需要的字段，服务端就只返回需要的字段；客户端的告诉服务端需要哪些字段，服务端就更新哪些字段。\n但是 FieldMask 只是一个定义，在具体的使用场景中还需要开发者自己编写一些辅助方法，来实现功能。那么是不是可以提供一个插件，让开发者可以只编写 proto 文件，便可以自动生成一些辅助方法呢？答案是肯定的，预览效果如下：\nmessage UserInfoRequest { string user_id = 1; google.protobuf.FieldMask field_mask = 2 [ (fieldmask.option.Option).in = {gen: true}, (fieldmask.option.Option).out = {gen: true, message:\u0026#34;UserInfoResponse\u0026#34;} ]; } message Address { string country = 1; string province = 2; } message UserInfoResponse { string user_id = 1; string name = 2; string email = 3;· Address address = 4; } message NonEmpty {} service UserInfo { rpc GetUserInfo(UserInfoRequest) returns (UserInfoResponse) {} rpc UpdateUserInfo(UserInfoRequest) returns (NonEmpty) {} } 生成的代码如下：\n因为篇幅有限，对代码进行了删减，如果需要查阅完整代码请参见 github.com/yeqown/protoc-gen-fieldmask/examples/pb/user.pb.fm.go\n// FieldMask 公共部分代码 // FieldMaskWithMode 根据不同的模式决定不同的使用姿势（Filter 和 Prune） func (x *UserInfoRequest) FieldMaskWithMode(mode pbfieldmask.MaskMode) *UserInfoRequest_FieldMask { fm := \u0026amp;UserInfoRequest_FieldMask{ maskMode: mode, maskMapping: make(map[string]struct{}, len(x.FieldMask.GetPaths())), } for _, path := range x.FieldMask.GetPaths() { fm.maskMapping[path] = struct{}{} } return fm } // Filter 模式下，包含在 FieldMask 中的字段代表 保留 / 更新。 func (x *UserInfoRequest) FieldMask_Filter() *UserInfoRequest_FieldMask { return x.FieldMaskWithMode(pbfieldmask.MaskMode_Filter) } // Prune 模式下，包含在 FieldMask 的字段代表需要 去除 / 不更新。 func (x *UserInfoRequest) FieldMask_Prune() *UserInfoRequest_FieldMask { return x.FieldMaskWithMode(pbfieldmask.MaskMode_Prune) } // UserInfoRequest_FieldMask provide provide helper functions to deal with FieldMask. type UserInfoRequest_FieldMask struct { maskMode pbfieldmask.MaskMode maskMapping map[string]struct{} } // 增量更新的服务部分代码 (fieldmask.option.Option).in = {gen: true} 控制生成 func (x *UserInfoRequest) MaskIn_UserId() *UserInfoRequest {} func (x *UserInfoRequest_FieldMask) MaskedIn_UserId() bool {} // 响应字段裁剪部分代码 (fieldmask.option.Option).out = {gen: true, message:\u0026#34;UserInfoResponse\u0026#34;} 控制生成 func (x *UserInfoRequest) MaskOut_UserId() *UserInfoRequest {} func (x *UserInfoRequest) MaskOut_Name() *UserInfoRequest {} func (x *UserInfoRequest_FieldMask) MaskedOut_UserId() bool {} func (x *UserInfoRequest_FieldMask) MaskedOut_Name() bool {} // Mask 根据 FieldMask 和 mode 来裁剪响应字段。 func (x *UserInfoRequest_FieldMask) Mask(m *UserInfoResponse) *UserInfoResponse { switch x.maskMode { case pbfieldmask.MaskMode_Filter: x.filter(m) case pbfieldmask.MaskMode_Prune: x.prune(m) } return m } func (x *UserInfoRequest_FieldMask) filter(m proto.Message) { if len(x.maskMapping) == 0 { return } pr := m.ProtoReflect() pr.Range(func(fd protoreflect.FieldDescriptor, _ protoreflect.Value) bool { _, ok := x.maskMapping[string(fd.Name())] if !ok { pr.Clear(fd) return true } return true }) } func (x *UserInfoRequest_FieldMask) prune(m proto.Message) { if len(x.maskMapping) == 0 { return } pr := m.ProtoReflect() pr.Range(func(fd protoreflect.FieldDescriptor, _ protoreflect.Value) bool { _, ok := x.maskMapping[string(fd.Name())] if !ok { return true } pr.Clear(fd) return true }) } 又了上述的辅助代码，因此我们就可以在客户端和服务端直接使用了：\n客户端侧：\nfunc main() { maskedReq := \u0026amp;pb.UserInfoRequest{ UserId: \u0026#34;1\u0026#34;, } maskedReq. MaskOut_Email(). // masking email field in response MaskOut_Address() // masking address field in response } 服务端侧：\n// 响应裁剪示例 func (u userServer) GetUserInfo( ctx context.Context, request *pb.UserInfoRequest, ) (*pb.UserInfoResponse, error) { // FieldMask_Filter means that the fields are included in the response, // otherwise they are omitted. // FieldMask_Prune means masked fields are not included in the response, // otherwise they are included. fm := request.FieldMask_Filter() // fm2 := request.FieldMask_Prune() fmt.Printf(\u0026#34;userServer.GetUserInfo is called: %v\\n\u0026#34;, request.String()) resp := \u0026amp;pb.UserInfoResponse{ UserId: \u0026#34;69781\u0026#34;, Name: \u0026#34;yeqown\u0026#34;, Email: \u0026#34;yeqown@gmail.com\u0026#34;, Address: nil, } // judge if the field masked or not, avoid unnecessary call or calculation. if fm.MaskedOut_Address() { // filter more, so the address field should be included. resp.Address = \u0026amp;pb.Address{ Country: \u0026#34;China\u0026#34;, Province: \u0026#34;Sichuan\u0026#34;, } } // filter the field masked out. _ = fm.Mask(resp) return resp, nil } // 增量更新示例 func (u userServer) UpdateUserInfo(ctx context.Context, request *pb.UserInfoRequest) (*pb.NonEmpty, error) { // FieldMask_Filter means that the fields are expected to update, // otherwise they are ignored. // FieldMask_Prune means masked fields are ignored to update, // otherwise they are expected to update. fm := request.FieldMask_Filter() // fm2 := request.FieldMask_Prune() fmt.Printf(\u0026#34;userServer.UpdateUserInfo is called: %v\\n\u0026#34;, request.String()) if fm.MaskedIn_UserId() { // userId want to be updated, so you should use request.UserId to update. fmt.Println(\u0026#34;userId want to be updated.\u0026#34;) } return new(pb.NonEmpty), nil } 关于 PG* 的一些总结 # PG* 就是 protoc-gen-star 的缩写，是一个高效开发 protoc 插件的 go 语言库。\nprotoc-gen-fieldmask 就是以它为基础开发的一个插件（PGV也是）。比较早以前就知道了这么个库的存在，但是但是并没有什么好的点子，也就一直没有实践过，这次就顺便学习了一下这个库的一些用法：\n基本原理\n所有的插件都是配合 protoc 一起工作的，protoc 会处理源代码然后生成一个 CodeGeneratorRequest 的请求，调用插件，插件处理这个请求，生成一个 CodeGeneratorResponse 的响应，protoc 会处理这个响应并生成文件。\n整个流程如下：\nfoo.proto → protoc → CodeGeneratorRequest → protoc-gen-myplugin → CodeGeneratorResponse → protoc → foo.pb.go\n基本用法\nprotoc-gen-star 抽象了一个 Module, 并提供一个一系列的辅助方法，因此你需要做的事就是实现这个 Module。那么如何实现呢？\n// Module describes the interface for a domain-specific code generation module // that can be registered with the PG* generator. type Module interface { // The Name of the Module, used when establishing the build context and used // as the base prefix for all debugger output. Name() string // InitContext is called on a Module with a pre-configured BuildContext that // should be stored and used by the Module. InitContext(c BuildContext) // Execute is called on the module with the target Files as well as all // loaded Packages from the gatherer. The module should return a slice of // Artifacts that it would like to be generated. Execute(targets map[string]File, packages map[string]Package) []Artifact } 重点就是 Execute 方法，在其中遍历所有文件的 AST，识别你的插件需要处理的特征，然后准备好一个 Artifact，然后返回这个 Artifact。\nArtifact 就是预期生成的 文件对象，在 protoc-gen-star 中，它可以是通过模板生成，也可以就是完整的文件内容。\n如何调试你的插件\n在开发过程中，调试/测试环节必不可少，那么对于依赖于 protoc 编译的插件该怎么调试呢？尤其是怎么断点调试呢？\n日志调试 protoc-gen-star 已经在 ModuleBase 中内置了日志记录，只需要调用 ModuleBase.Debugf 或者 ModuleBase.Logf 方法即可。\nIDE 断点调试 光有日志可不够，想要追踪某个变量的值，打日志可能会让你崩溃。我们得想办法，可以在IDE中调试自己的插件。想要断点调试，那么就不能依赖 protoc, 必须得让插件可以独立运行，这样才能调试。\n回顾一下前面的基本原理，我们的插件其实是依赖 CodeGeneratorRequest，因此如果我们可以构造一个 CodeGeneratorRequest，那么就可以构建一个测试用例，用来调试运行了。proto-gen-star 已经提供了一个 protoc-gen-debug 来生成 CodeGeneratorRequest。\nprotoc \\ -I=./examples/pb \\ -I=./proto \\ --plugin=protoc-gen-debug=/usr/local/bin/proto-gen-debug \\ --debug_out=\u0026#34;./internal/module/debugdata:.\u0026#34; \\ ./examples/pb/user.proto 再编写一个类似的测试用例，就可以在IDE中调试了自己的插件了。\nfunc (m *moduleTestSuite) Test_ForDebug() { // please look up the README at repository root directory to see how to // generate the `debugdata` and code_generator_request binary. req, err := os.Open(\u0026#34;./debugdata/code_generator_request.pb.bin\u0026#34;) m.NoError(err) fs := afero.NewMemMapFs() res := \u0026amp;bytes.Buffer{} pgs.Init( pgs.ProtocInput(req), // use the pre-generated request pgs.ProtocOutput(res), // capture CodeGeneratorResponse pgs.FileSystem(fs), // capture any custom files written directly to disk pgs.MutateParams(mutateLangParam(\u0026#34;go\u0026#34;)), // mutate params ). RegisterModule(m.module). Render() } 总结 # FieldMask 在屏蔽字段功能上类似 GraphQL 的功能，但是实现机理更加简单，更加灵活；最重要的是，gRPC 的使用范围更广，更多的被微服务场景中使用。\n水平有限，如有错误，欢迎勘误指正🙏。\n参考资料 # go.dev fieldmaskpb protoc-gen-fieldmask protoc-gen-star "},{"id":21,"href":"/2021/12/15/Sentry-OpenTelemetry%E5%89%8D%E5%90%8E%E7%AB%AF%E5%85%A8%E9%93%BE%E8%B7%AF%E6%89%93%E9%80%9A%E6%80%BB%E7%BB%93/","title":"Sentry+OpenTelemetry前后端全链路打通总结","section":"Posts","content":"自从微服务大行其道，容器化和k8s编排一统天下之后，\u0026ldquo;可观测性\u0026rdquo; 便被提出来。这个概念是指，对于应用或者容器的运行状况的掌控程度，其中分为了三个模块：Metrics、Tracing、Logging。Metrics 指应用采集的指标；Tracing 指应用的追踪；Logging 指应用的日志。\n日志自不用多说，这是最原始的调试和数据采集能力。Metrics 比较火的方案就是 Prometheus + Grafana，思路就是通过应用内埋入SDK，选择 Pull 或者 Push 的方式将数据收集到 prometheus 中，然后通过 Grafana 实现可视化，当然这不是本文的重点就此略过。\nTracing 也并不是可观测性提出后才诞生的概念，在微服务化的进程中就已经有Google的Dapper落地实践，并慢慢形成 OpenTracing 规范，这一规范又被多家第三方框架所支持，如 Jaeger、Zipkin 等。OpenTelemetry 就是结合了 OpenTracing + OpenCensus 规范，约定并提供完成的可观测性套件，只是目前（2021-12-15）稳定下来的只有 Tracing 这一部分而已。对 OpenTelemetry 发展历史感兴趣的可以自行了解。\n效果预览 # 链路总览，包含了前端页面的生命周期 + 整个了链路采集到的Span聚合。\n前端页面指标采集概览，包含了该页面生命周期内的动作和日志等。\n服务端链路细节，包含了服务端链路采集的标签和日志（事件）等信息。\npropagation兼容jaeger效果，保证jaeger侧链路完整，使用一致的 traceId检索。因为服务侧 sentry 是渐进更新的，因此没有接入的应用并不会展示在sentry侧， 等到完全更新后就会完整。\n背景 # 目前运行中的链路追踪组件是采用 opentracing + jaeger 实现，这套方案唯二的不足就是：\nopentracing 已经被 opentelemetry 兼容，且 opentelemetry 在可观测性上更全面，更灵活。 浏览器侧支持不完善（可以参见 https://github.com/jaegertracing/jaeger-client-javascript ）【恰恰我司有这方面的需求 🤪】。 前端采用 sentry 来采集前端页面数据（APP + WEB 都支持很好），因此才有了这么一个 前后端链路打通的需求。\n经过调研，我发现 sentry 也并不完美，详情参见 附录/sentry作为链路采集组件的优缺点\n最开始的需求目标是后端相关 Tracing SDK 全部使用sentry替换，但是结合上述对于sentry的调研，发现直接接入sentry并不是一个好的选择：\n相关的 Tracing 概念没有被我个人和社区接受，社区内主流的还是 OpenTracing 和 OpenTeletemetry 规范，而且这两个规范都是相互兼容的（个人认为：OpenTelemetry 大有一统可观测性的趋势，他们立项也是朝着这个方向前进）。 官方go-SDK活跃度低乎想象，参见 https://github.com/getsentry/sentry-go/issues/387 截止本文时（since 2021-10-22）仍然没有任何回复，进一步阻止我直接使用sentry。 调研 OpenTelemetry 在研究它的架构设计时，发现其设计中包含了一个 Collector（详细介绍参见 附录/OpenTelemetry中的概念） 。从示意图我们也可以看出，它的作用近似于 Jaeger Collector / Agent，但是相比于 Jaeger 它更开放，支持多种 Vendor (Jaeger / Zipkin 等等)，更加灵活。 如果替换了 sentry 加入是不是以后还可能更换新的采集组件，服务这么多再来换一遍还是费时费力 方案描述 # 那么基于以上的考虑，设计了如下的改造方案（也可以理解为新的实施方案，毕竟出了兼容也不会考虑以前怎么用的🤪）：\n在应用侧 Tracing SDK 全部替换为 otel SDK（后面经过考虑，还是自己做了一个防腐的 Tracing API封装）。 为了保证渐进更新服务端链路采集的同时，在Jaeger Dashborad中的链路不断，在 propagator 实现 otel 到 jaeger 的转换。 自己实现一个 Sentry Trace Exporter, 将客户端上报的链路，再投递到 sentry。 Collector 配置 exporter 时，同时上报到 sentry 和 jaeger 相应的实践，我已经放了一份在 github 上，请查看：https://github.com/yeqown/opentelemetry-quake\n期间遇到的问题 # 怎么调研，产出方案？\n刚开始接到需求，我第一时间在质疑需求的必要性，现在的链路采集好好的为啥非要换成另一种？也没有好的想法，毕竟对于 sentry 和 opentelemetry 认知都不够。通过查看官方文档，尤其是 opentelemetry 的设计，才找到了这么一个现阶段很满意的解决方案。\n调研 = 多看文档和别人的最佳实践。只有你了解够多，你才能站得更高，更容易考虑问题。\n方案 = 你需要的是什么，就用现成的工具来组合，如果实在没有那么是不是可以自己实现？\n怎么定制自己的 Trace Exporter？\n主要还是参考 https://github.com/open-telemetry/opentelemetry-collector-contrib 仓库中的实现。Collector 设计时已经考虑了用户自定义，所以按照官方的约定开发即可，至于 sentryexporter 在官方仓库已经存在了，所以我只是定制化了一下。（为了加深理解，我其实又从头撸了一遍 🤓）\n这里值得注意的是，如果你的collector需要使用特定的插件，那么需要使用官方的 github.com/open-telemetry/opentelemetry-collector/cmd/builder 来自己定制编译 collector, 这一点也可以在我提供的实践代码中找到。\nCollector 和 Agent 的差异？\n可以简单理解为：中心化和分布式部署的区别，在实现上并没有区别。至于你应该使用哪种部署方式，当然视你的集群规模而定，如果只有不到100个服务实例，个人觉得仅仅collector足以，直到不足以承载。反之，如果你集群中节点众多，服务实例也众多，那么最好一开始就上agent。\nk8s 中如何以 agent 方式部署 collector？\n官方也已经提供了样例代码，可以参考 github.com/open-telemetry/opentelemetry-collector/examples/k8s/otel-config.yaml。大致思路为：通过DaemonSet 在每个节点上运行一个 Collector 实例。这里需要注意的是，如果想要配置 NODE_IP:PORT 的方式让该节点上的服务直连 agent，那么需要在 otel-config.yaml 中添加如下配置：\napiVersion: apps/v1 # .... template: metadata: labels: app: opentelemetry component: otel-agent spec: hostNetwork: true # 增加这一行 dnsPolicy: ClusterFirstWithHostNet # 增加这一行 containers: # ... 当然不止这么一种方式，比如还可以使用 hostPort, 视你的k8s集群配置而定。\n附录 # sentry作为链路采集组件的优缺点 # 仅代表个人看法，可能对于 sentry 理解不到位，请自行按照自己的理解来看。\n缺点:\nsentry 的主要场景并不是在于链路采集，而是在于前端页面采集（页面加载/路径/日志），包括页面异常数据；主要根据在于它独有（或许）的链路采集概念。其中常见的如下：\n概念 定义 OpenTracing中的映射 存在的问题 Trace 可以通过 TraceId 标识的一条链路 Trace 前端 Trace/Page 与 Trace/Req 出入 Span 可以通过 SpanId 标识的一个单元 Span - Trasnaction Trace 在进程内 Span 的集合 - 独有的概念 Scope Trace 的上下文 (包含用户，Request等) 只能用 SpanContext 【并不准确】 相比 SpanContext 包含的信息过于太多了 Hub 应用侧 Tracer Tracer - 个人觉得，正是 sentry 自己的定位并不着重于后端服务的链路采集，才会如此设计。\n优点:\n已经支持前后端链路打通，集成展示前端页面上发生的一切行为。 单独对异常进行采集和展示。 OpenTelemetry中的概念 # 链路采集基本的概念就不多介绍了，了解过 OpenTracing 之后，会发现大同小异\n这里只是罗列了 Tracing 相关的概念：\n概念 定义 功能 TracerProvider Tracer 注册中心 可以用于区分不同 Lib Propagator 负责解析和注入 trace header 跨进程传递 otel SDK OpenTelemetry 客户端 SDK 语言相关的API，负责链路的采集和上报 otel Collector 链路采集后端组件 收集客户端上报的链路信息 otel Agent 与Collector相同，只是以Agent模式部署 k8s 中采用 DaemonSet 部署方式，增加部署容错能力，减少上报时延 Exporter (Span) SDK 处理 span 的组件 用于客户端自定义span数据加工处理（如客户端直接上报链路到Vendor） Exporter (Trace) Collector 中处理Trace的组件 与 Span Exporter 类似，但工作在 Collector 中，处理被 Collector 收集归纳的链路数据 Reciever Collector 中处理Trace的组件 与 Span Exporter 类似，但工作在 Collector 中，处理被 Collector 收集归纳的链路数据 前文提到的 OpenTelemetry 的开放能力，从 Collector 和 Exporter 也能看得出来。\n总结 # 对于 sentry 并没有过多的介绍，因为从后端的角度看过去，通过改造方案设计，sentry 就只是其中的一种 vendor 而已，并不会对我们的链路采集产生影响。就算以后前端想要试验另一种方案，那我们也只需要支持多一种 vendor 即可。强烈建议了解 OpenTelemetry 的相关概念，以后一定用得上。\n水平有限，如有错误，欢迎勘误指正🙏。\n参考 # https://opentelemetry.io/ https://sentry.io/ https://github.com/yeqown/opentelemetry-quake "},{"id":22,"href":"/2020/09/27/WebSocket-Implemention-With-Go/","title":"WebSocket Implemention With Go","section":"Posts","content":" 什么是WS协议 # The WebSocket Protocol enables two-way communication between a client running untrusted code in a controlled environment to a remote host that has opted-in to communications from that code. The security model used for this is the origin-based security model commonly used by web browsers. The protocol consists of an opening handshake followed by basic message framing, layered over TCP.\nThe goal of this technology is to provide a mechanism for browser-based applications that need two-way communication with servers that does not rely on opening multiple HTTP connections (e.g., using XMLHttpRequest or \u0026lt;iframe\u0026gt;s and long.polling). - 摘自 RFC6455 Abstract.\n用大白话就是： “基于TCP传输协议构建的全双工，用于Web浏览器和服务端进行通信的应用层协议”。那么同属于应用层协议，WebSocket和HTTP之间有何异同？\n特点 HTTP WebSocket OSI Layer 7 7 通信模式 单工/半双工/全双工，取决于HTTP版本 全双工 工作端口 80/443 80/443作为默认，但不限于 是否支持SSL 是 是 传输协议 TCP TCP 数据格式 JSON / TEXT / Binary Stream 等 JSON / TEXT / Binary Stream 等 协议Schema http/https ws/wss 最新版本 3 13 为什么要用WS协议 # 想一下，自己会在什么场景下考虑WebSocket而不是HTTP协议就明白了。在没有 WebSocket 之前，了实现即时通信有以下几种方案：\nShort Polling Long Polling Streaming SSE (Server Sent Event) 这部分的演进可以阅读 https://halfrost.com/websocket/ 本文也多有参考。\n实现 # 本文的主要目标是为了介绍如何使用Go实现WebSocket协议，因此最好是具备以下能力：\n会使用 Wireshark 或者 tcpdump 熟练使用 Go 语言开发 能读懂RFC6455的英语能力，当然你用翻译也可以，至少别因为翻译错误而无法继续～ 对TCP有一定理解 阅读RFC6455，阅读RFC6455，阅读RFC6455！！！ 这是一切的起点，没必要急于动手。\n1. 理解WebSocket的协议帧和工作流程 # 整体流程：\n时序图：\n协议帧：\n协议帧 是最重要的部分，理解了协议帧就相当于完成了50%，余下的就是撸代码了\u0026hellip; 协议帧主要包括了以下几个部分：\n头部 FIN [1bit] 指示该帧是不是消息分片的最后一帧。分片-传送门 RSV 保留位 [3bit] MASK 是否使用掩码 [1bit] PAYLOAD LEN 数据部分的长度 [7bit] 如果等于 126/127（7bit=128-1）则说明启用 PAYLOAD LEN EXT（1/2） 部分来表明数据部分的长度。 PAYLOAD LEN EXT 仅当 PAYLOAD LEN 无法表示数据部分的长度时启用 [16bit/64bit] MASKING KEY 掩码 [0/32bit] 取决是MASK是否设置。掩码-传送们 数据, 长度等于 Payload Len或者Payload Len Ext，请忽略图中一个bit一个字符\u0026hellip; 如果认真看图的话和看过RFC6455之后，你就会发现数据部分的真实长度，其实分为三种情况：\nPayload Len 的值，小于 126时，没有扩展的Payload Len Ext部分，值本身表达数据部分的长度 Payload Len 的值，等于 126时，Payload Len Ext1 启用，长度16bit Payload Len 的值，等于 127时，Payload Len Ext2 启用，长度64bit 2. 协议帧frame的基本处理 # 看过协议帧之后，接下来就是数据帧的处理，这部分需要的就是编码知识了。那么在Go里面如何去表达一个帧呢？一个帧在TCP眼里就是字节流，发送的时候是，接收的时候也是，因此帧的主要工作就是：将协议数据按照指定的格式塞到字节流里面去，亦或者是从字节流中解析到我们用于表达的数据结构中去。就像一个翻译官，按照特定的语法来回翻译。\n// Frame 这里使用uint16来承载，只是因为统一，方便计算移位，因为整个头部最重要的部分就是16bit的长度。 type Frame struct { Fin uint16 // 1 bit RSV1 uint16 // 1 bit, 0 RSV2 uint16 // 1 bit, 0 RSV3 uint16 // 1 bit, 0 OpCode OpCode // 4 bits Mask uint16 // 1 bit // Payload length: 7 bits, 7+16 bits, or 7+64 bits // // if PayloadLen = 0 - 125, actual_payload_length = PayloadLen // if PayloadLen = 126, actual_payload_length = PayloadExtendLen[:16] // if PayloadLen = 127, actual_payload_length = PayloadExtendLen[:] PayloadLen uint16 // 7 bits PayloadExtendLen uint64 // 64 bits MaskingKey uint32 // 32 bits Payload []byte // no limit by RFC6455 } 翻译的部分代码如下：\n这里对于 “大小端序” 没有概念的，可以先自行查阅资料。\n// 将帧翻译到字节流 func encodeFrameTo(frm *Frame) []byte { buf := make([]byte, 2, minFrameHeaderSize+8) var ( part1 uint16 // from FIN to PayloadLen ) part1 |= frm.Fin \u0026lt;\u0026lt; finOffset part1 |= frm.RSV1 \u0026lt;\u0026lt; rsv1Offset part1 |= frm.RSV2 \u0026lt;\u0026lt; rsv2Offset part1 |= frm.RSV3 \u0026lt;\u0026lt; rsv3Offset part1 |= uint16(frm.OpCode) \u0026lt;\u0026lt; opcodeOffset part1 |= frm.Mask \u0026lt;\u0026lt; maskOffset part1 |= frm.PayloadLen \u0026lt;\u0026lt; payloadLenOffset // start from 0th byte // fill part1 into 2 byte binary.BigEndian.PutUint16(buf[:2], part1) // FIXED: fill payloadExtendLen into 8 byte switch frm.PayloadLen { case 126: payloadExtendBuf := make([]byte, 2) binary.BigEndian.PutUint16(payloadExtendBuf[:2], uint16(frm.PayloadExtendLen)) buf = append(buf, payloadExtendBuf...) case 127: payloadExtendBuf := make([]byte, 8) binary.BigEndian.PutUint64(payloadExtendBuf[:8], frm.PayloadExtendLen) buf = append(buf, payloadExtendBuf...) } // FIXED: if not mask, then no set masking key if frm.Mask == 1 { // fill fmtMaskingKey into 4 byte maskingKeyBuf := make([]byte, 4) binary.BigEndian.PutUint32(maskingKeyBuf[:4], frm.MaskingKey) buf = append(buf, maskingKeyBuf...) } // header done, start writing body buf = append(buf, frm.Payload...) return buf } // 解析帧的头部（16bit）信息，因为WebSocket协议帧中：PayloadLen 是变长， // MaskingKey 也是有或者没有，但是都可以通过16bit的数据来获得准确的结果。 // 解析过程也就是写入的逆向操作。 func parseFrameHeader(header []byte) *Frame { var ( frm = new(Frame) part1 = binary.BigEndian.Uint16(header[:2]) ) frm.Fin = (part1 \u0026amp; finMask) \u0026gt;\u0026gt; finOffset frm.RSV1 = (part1 \u0026amp; rsv1Mask) \u0026gt;\u0026gt; rsv1Offset frm.RSV2 = (part1 \u0026amp; rsv2Mask) \u0026gt;\u0026gt; rsv2Offset frm.RSV3 = (part1 \u0026amp; rsv3Mask) \u0026gt;\u0026gt; rsv3Offset frm.OpCode = OpCode((part1 \u0026amp; opcodeMask) \u0026gt;\u0026gt; opcodeOffset) frm.Mask = (part1 \u0026amp; maskMask) \u0026gt;\u0026gt; maskOffset frm.PayloadLen = (part1 \u0026amp; payloadLenMask) \u0026gt;\u0026gt; payloadLenOffset return frm } 3. WebSocket链接的定义 # Conn 是对TCP链接的封装再配合上协议，来对客户端和服务端的提供功能。其中我个人觉得最重要的功能是：如何从TCP字节流中读到一个完整的WebSocket消息 Conn.ReadMessage()。\ntype Conn struct { conn net.Conn // 底层TCP链接 bufRD *bufio.Reader // 读 bufWR *bufio.Writer // 写 // 省略部分不是特别重要的字段 } // 构造websocket.Conn func newConn(netconn net.Conn, isServer bool) (*Conn, error) { c := Conn{ conn: netconn, bufRD: bufio.NewReaderSize(netconn, 65535), // 65535B = 64KB bufWR: bufio.NewWriter(netconn), } return \u0026amp;c, nil } // ReadMessage . it will block to read message func (c *Conn) ReadMessage() (mt MessageType, msg []byte, err error) { frm, err := c.readFrame() if err != nil { debugErrorf(\u0026#34;Conn.ReadMessage failed to c.readFrame, err=%v\u0026#34;, err) return NoFrame, nil, err } mt = MessageType(frm.OpCode) // 根据读到的帧判断是否还有后续的帧，如果有分片，那就读完将payload组装到一起。 buf := bytes.NewBuffer(nil) buf.Write(frm.Payload) for !frm.isFinal() { if frm, err = c.readFrame(); err != nil { debugErrorf(\u0026#34;Conn.ReadMessage failed to c.readFrame, err=%v\u0026#34;, err) return NoFrame, nil, err } buf.Write(frm.Payload) } msg = buf.Bytes() return } // 从缓冲区读取指定字节数量的数据 func (c *Conn) read(n int) ([]byte, error) { p, err := c.bufRD.Peek(n) if err == io.EOF { err = ErrUnexpectedEOF return nil, err } _, _ = c.bufRD.Discard(len(p)) return p, err } func (c *Conn) readFrame() (*Frame, error) { // 阻塞地读2Byte的数据 p, err := c.read(2) if err != nil { debugErrorf(\u0026#34;Conn.readFrame failed to c.read(header), err=%v\u0026#34;, err) return nil, err } // 解析WebSocket帧头部 frmWithoutPayload := parseFrameHeader(p) logger.Debugf(\u0026#34;Conn.readFrame got frmWithoutPayload=%+v\u0026#34;, frmWithoutPayload) var ( payloadExtendLen uint64 // this could be non exist remaining uint64 ) // 根据PayloadLen来读取不同字节数的扩展长度 // 126 -\u0026gt; 2B // 127 -\u0026gt; 8B switch frmWithoutPayload.PayloadLen { case 126: // has 16bit + 32bit = 6B p, err = c.read(2) if err != nil { debugErrorf(\u0026#34;Conn.readFrame failed to c.read(2) payloadlen with 16bit, err=%v\u0026#34;, err) return nil, err } payloadExtendLen = uint64(binary.BigEndian.Uint16(p[:2])) remaining = payloadExtendLen case 127: // has 64bit + 32bit = 12B p, err = c.read(8) if err != nil { debugErrorf(\u0026#34;Conn.readFrame failed to c.read(8) payloadlen with 16bit, err=%v\u0026#34;, err) return nil, err } payloadExtendLen = binary.BigEndian.Uint64(p[:8]) remaining = payloadExtendLen default: remaining = uint64(frmWithoutPayload.PayloadLen) } frmWithoutPayload.PayloadExtendLen = payloadExtendLen // get masking key if frmWithoutPayload.Mask == 1 { // only 32bit masking key to read p, err = c.read(4) if err != nil { debugErrorf(\u0026#34;Conn.readFrame failed to c.read(header), err=%v\u0026#34;, err) return nil, err } frmWithoutPayload.MaskingKey = binary.BigEndian.Uint32(p) } // 省略frame校验过程 // 读取payload数据并填充到frame中去 var ( payload = make([]byte, 0, remaining) ) logger.Debugf(\u0026#34;Conn.readFrame c.read(%d) into payload data\u0026#34;, remaining) for remaining \u0026gt; 65535 { // true: bufio.Reader can read 65535 byte as most at once p, err := c.read(65535) if err != nil { debugErrorf(\u0026#34;Conn.readFrame failed to c.read(payload), err=%v\u0026#34;, err) return nil, err } payload = append(payload, p...) remaining -= 65535 } // 读取剩余部分的payload p, err = c.read(int(remaining)) if err != nil { debugErrorf(\u0026#34;Conn.readFrame failed to c.read(payload), err=%v\u0026#34;, err) return nil, err } payload = append(payload, p...) frmWithoutPayload.setPayload(payload) // 处理ping pong close 帧 switch frmWithoutPayload.OpCode { case opCodeText, opCodeBinary, opCodeContinuation: // pass case opCodePing: err = c.replyPing(frmWithoutPayload) case opCodePong: err = c.replyPong(frmWithoutPayload) case opCodeClose: err = c.handleClose(frmWithoutPayload) } return frmWithoutPayload, err } 4. 服务端和客户端的定义 # 到这一步，已经把底层的工作都完成了：定义协议帧，协议帧翻译，Conn约定和封装等工作。现在可以开始设计和实现顶层API了。 服务端API，我参考了gorilla/websocket，定义了一个 Upgrader 来将HTTP升级到 Websocket。\n// Upgrader std.HTTP / fasthttp / gin etc type Upgrader struct { CheckOrigin func(req *http.Request) bool Timeout time.Duration } // 升级协议。如果遇到了hijack错误，可以看这里的连接，希望有所帮助 // https://stackoverflow.com/questions/32657603/why-do-i-get-the-error-message-http-response-write-on-hijacked-connection // func (ug Upgrader) Upgrade(w http.ResponseWriter, req *http.Request, fn func(conn *Conn)) error { // 设置超时上下文 ctx, cancel := context.WithTimeout(req.Context(), timeout) req = req.WithContext(ctx) defer cancel() // RFC6455 完成握手检查 // almost checking is about headers if err := ug.handshakeCheck(w, req); err != nil { debugErrorf(\u0026#34;Upgrader.Upgrade failed to ug.handshakeCheck, err=%v\u0026#34;, err) return ug.returnError(w, http.StatusBadRequest, err.Error()) } // !!! 重要，获取HTTP对应的底层TCP链接 h := w.(http.Hijacker) netconn, brw, err = .Hijack() if err != nil { debugErrorf(\u0026#34;Upgrader.Upgrade failed to h.Hijack, err=%v\u0026#34;, err) _ = ug.returnError(w, http.StatusInternalServerError, err.Error()) return nil } // 省略请求头处理 ... // 发送HTTP响应 if err = hackHandshakeResponse(brw.Writer, respHeaders, \u0026#34;101\u0026#34;); err != nil { return err } // 启动一个goroutine来处理该链接的消息，ConnPool / Reactor 的实现，可以在这里调整 conn, _ := newConn(netconn, true) go func() { defer func() { if err, ok := recover().(error); ok { logger.Errorf(\u0026#34;Upgrader.Upgrade fn panic: err=%v\u0026#34;, err) debug.PrintStack() } }() fn(conn) }() return nil } 客户端的话就更简单了，建立一个TCP连接，向服务端发起HTTP升级到Websocket的请求，等握手通过那么连接就建立完成了，就可以对websocket.Conn进行读写操作了。\n5. 建链和握手 # 在上一小节已经带过了这部分，这里重点想要介绍下http请求到websocket的升级过程。服务端例子如下：\nfunc main() { http.HandleFunc(\u0026#34;/echo\u0026#34;, echo) if err := http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil); err != nil { log.Fatal(err) } } func echo(w http.ResponseWriter, req *http.Request) { websocketHdl := func(conn *websocket.Conn) { for { // 读消息 mt, message, err := conn.ReadMessage() // 发送消息 err = conn.SendMessage(string(message)) } log.Info(\u0026#34;conn finished\u0026#34;) } // 调用 upgrader 升级，并调用连接处理方法 - goroutine 保持 err := upgrader.Upgrade(w, req, websocketHdl) if err != nil { log.Errorf(\u0026#34;upgrade error, err=%v\u0026#34;, err) return } log.Infof(\u0026#34;conn upgrade done\u0026#34;) } 由上述代码我们可以明白，所谓的升级过程是使用HTTP协议来完成，对于服务端更友好，很容易在现有的HTTP服务中加上一个WebSocket服务。握手 - 传送门。用大白话描述就是：\n客户端通过 schema://host:port/path/to/websocket 这样一个地址找到服务端 [建立TCP连接] 客户端发起一个HTTP请求（携带特殊的请求头）[客户端发起握手] 服务端通过验证后将该连接转换为websocket连接保持在服务端 [服务端握手检查和升级] 服务端握手完成后，同时向客户端发送一个HTTP响应（成功或失败）[服务端发送响应] 客户端根据服务端的响应处理该连接 [客户端处理响应] 握手完成 6. 完善细节 # 前面几步完成，WebSocket的框架就已经成型了，剩下的工作就是根据协议完善。比如对关闭帧的处理，Ping/Pong帧的处理等等。这里简单举例说明 Ping / Pong 的处理。\nfunc (c *Conn) readFrame() (*Frame, error) { // ignore cases // handle with close, ping, pong frame switch frmWithoutPayload.OpCode { // ignore cases ... case opCodePing: err = c.replyPing(frmWithoutPayload) case opCodePong: err = c.replyPong(frmWithoutPayload) // ignore some cases ... } return frmWithoutPayload, err } // Ping conn send a ping packet to another side. func (c *Conn) Ping() (err error) { return c.sendControlFrame(opCodePing, []byte(\u0026#34;ping\u0026#34;)) } // replyPing work for Conn to reply ping packet. frame MUST contains 125 Byte or- // less payload. func (c *Conn) replyPing(frm *Frame) (err error) { return c.pong(frm.Payload) } // pong . func (c *Conn) pong(pingPayload []byte) (err error) { return c.sendControlFrame(opCodePong, pingPayload) } // replyPong frame MUST contains same payload with PING frame payload func (c *Conn) replyPong(frm *Frame) (err error) { // if receive pong frame, try to call pongHandler if c.pongHandler != nil { c.pongHandler(string(frm.Payload)) } return nil } 总结 # 实现WebSocket协议并没有什么难点，只要你读完RFC6455就行了，动手去实现只是为了加深认识，尤其是对于网络和协议的认识。实现起来简单另外一个原因是因为毕竟是应用层协议，基于TCP可靠传输。传输层协议对我们屏蔽了大量的网络细节问题～，如果想要挑战自己，可以尝试实现TCP或者UDP🐶。\n水平有限，如有错误，欢迎勘误指正🙏。\n参考资料 # https://tools.ietf.org/html/rfc6455 https://halfrost.com/websocket/ https://github.com/yeqown/websocket "},{"id":23,"href":"/2020/09/22/Kubernetes%E4%B8%ADgRPC-Load-Balancing%E5%88%86%E6%9E%90%E5%92%8C%E8%A7%A3%E5%86%B3/","title":"Kubernetes中gRPC Load Balancing分析和解决","section":"Posts","content":" 背景 # 第一次，线上遇到大量接口RT超过10s触发了系统告警，运维反馈k8s集群无异常，负载无明显上升。将报警接口相关的服务重启一番后发现并无改善。但是开发人员使用链路追踪系统发现，比较慢的请求总是某个gRPC服务中的几个POD导致，由其他POD处理的请求并不会出现超时告警。\n第二次，同样遇到接口RT超过阈值触发告警，从k8s中查到某个gRPC服务（关键服务）重启次数异常，查看重启原因时发现是OOM Killed，OOM killed并不是负载不均衡直接导致的，但是也有一定的关系，这个后面再说。前两次由于监控不够完善（于我而言，运维的很多面板都没有权限，没办法排查）。期间利用pprof分析了该服务内存泄漏点，并修复上线观察。经过第二次问题并解决之后，线上超时告警恢复正常水平，但是该 deployment 下的几个POD占用资源（Mem / CPU / Network-IO），差距甚大。\n第二张图是运维第一次发现该服务OOM killed 之后调整了内存上限从 512MB =\u0026gt; 1G，然而只是让它死得慢一点而已。 从上面两张图能够石锤的是该服务一定存在内存泄漏。Go项目内存占用的分析，我总结了如下的排查步骤：\n1. 代码泄漏（pprof）（可能原因 goroutine泄漏；闭包） 2. Go Runtime + Linux 内核（RSS虚高导致OOM）https://github.com/golang/go/issues/23687 3. 采集指标不正常（container_memory_working_set_bytes） 2，3 是基于第1点能基本排除代码问题的后续步骤。 解决和排查手段： 1. pprof 通过heap + goroutine 是否异常，来定位泄漏点 运行`go tool pprof`命令时加上--nodefration=0.05参数，表示如果调用的子函数使用的CPU、memory不超过 5%，就忽略它。 2. 确认go版本和内核版本，确认是否开启了MADV_FREE，导致RSS下降不及时(1.12+ 和 linux内核版本大于 4.5)。 3. RSS + Cache 内存检查 \u0026gt; Cache 过大的原因 https://www.cnblogs.com/zh94/p/11922714.html // IO密集：手动释放或者定期重启 查看服务器内存使用情况： `free -g` 查看进程内存情况： `pidstat -rI -p 13744` 查看进程打开的文件： `lsof -p 13744` 查看容器内的PID： `docker inspect --format \u0026#34;{{ .State.Pid}}\u0026#34; 6e7efbb80a9d` 查看进程树，找到目标: `pstree -p 13744` 参考：https://eddycjy.com/posts/why-container-memory-exceed/ 通过上述步骤，我发现了该POD被OOM killed还有另一个元凶就是，日志文件占用。这里就不过多的详述了，搜索方向是 “一个运行中程序在内存中如何组织 + Cache内存是由哪些部分构成的”。这部分要达到的目标是：一个程序运行起来它为什么占用了这么些内存，而不是更多或者更少。\n问题到此并没有结束，OOM Killed 只是集群中发现的最显眼的问题。这里还有几个疑点：\n服务大量超时，而运维资源却没有异常，整体的请求量没有大幅度上涨？ 为什么重启不能让系统恢复？ OOM Killed是什么原因导致的？（go项目 / 内存限制 1GB / 近底层数据服务，主要和DB打交道）除内存泄漏还有什么会导致服务内存占用上升？ 同一个 Deployment 的 Pods 在一个 Service 下对外提供服务，为什么占用的资源并不是近乎相等的？ 猜想和验证 # 上面说了那么多和gRPC 和 k8s Service 有半毛钱关系吗？有半毛钱。\n其实发现gRPC负载不均衡很简单，从上面的疑或者看一看k8s资源监控面板，就能看的出来同一个service中的不同POD负载不一样，因为有负载才有压力，有负载才需要消耗资源。当然因为个别POD负载极大导致内存泄漏问题突出，日志文件上升快OOM killed频繁。\n知道上述的现象之后，那么就可以猜想了，为什么负载不均衡呢？现列举以下的关键点：\n该服务是一个基于gRPC-go编写的服务端。 gRPC使用的应用层协议是http2。 http2最大的特点是长链接。 该服务是基于了k8s Service来对内部应用提供服务。 k8s Service 自带L4负载均衡（轮询）。 k8s 的负载均衡是基于 iptables 实现的。 iptables 是通过修改 netfilter 规则来实现的（这里可以简单理解为：k8s的负载均衡是无法感知服务的负载压力的）。 说了上面这些，再来理一理整个系统的请求处理流程。\nSLB =\u0026gt; k8s 集群 (nginx) ==\u0026gt; API服务 (k8s Service) ... gRPC-Client == || gRPC Server \u0026lt;== gRPC-Client ... gRPC-Server (k8s Service) \u0026lt;==|| 到这里，结合最开始的背景中提到的“负载不均衡”，基本上可以得出结论：“一个请求确定了由某个API服务处理之后，后续调用的POD几乎是确定的”。那么这种确定性是从哪里来的，明明k8s有自己的L4负载均衡？\n不会吧，不会吧，你竟然才知道L4层负载均衡对长连接没有意义。\n因为L4负载均衡是让客户端和服务端直接连接，而不是通过自己转发。那连接上了之后又没有重新连接，那么自然该客户端的所有请求都会交给连接上的服务端处理，而不是其他的服务端了。因此，系统中的大多数客户端在启动时，如果被k8s的负载均衡分配到了几乎同一个服务端POD上，那么这个服务端POD自然会处理大部分请求，相比其他服务它的负载自然会高出很多。这也解释了 1 和 4 疑问。\n对于疑问 2 “为什么重启也不行呢？” 这个问题等价于 “为什么k8s会把客户端分配得不均匀呢？”\n这里说的重启不行是指盲目重启，没有策略和优先级。大多数人理解的重启，多半会是只重启有问题的服务。事实上也是这样操作的。既然知道是k8s让服务端POD负载不均衡，进而导致接口响应慢，那么重启的目的是让“请求尽可能的均匀一些”。那么首先重启服务端，服务端重启完成后再重启依赖该服务的客户端，依赖的最底端依次往上重启。\nk8s L4轮询负载均衡存在的问题是：服务没有完全部署或重启完成（滚动更新），这时候客户端通过service发现的只有其中的部分POD而不是全部，当然会让后加入的服务分配到较少的客户端连接。\n解决办法 # 知道了问题在哪儿，那么解决思路就很明确了。正如标题中说的一样 “gRPC LoadBalancing”，这里需要找到一种能够将gRPC连接负载均衡的手段。这里提出以下三种解决办法：\n序号 方案 描述 优缺点 1 集中LB 服务端LB 客户端无感知；存在单点问题 2 客户端LB gRPC resolver + LB 客户端自定义LB策略；需要注册中心或者对接k8s的API以获取服务列表;不好升级 3 Service Mesh linkerd之类的组件 服务端客户端无感知；增加延迟 这里推荐的只有两种（视体量的技术栈而定）： 客户端LB 和 Service Mesh。\nService Mesh自不用多说，因为改造简单无代码入侵，维护成本的话就见人见智了。 客户端LB最大的缺点就是“不好升级，代码入侵”，但从gRPC这一套方案出发，客户端服务发现和LB都已经被集成到了gRPC（Resolver + LB Policy）里，只需要提供自己的方案并注册到gRPC就行了，相比其他两种更加可控。\n这里使用了linkerd来尝试解决问题，下图是使用了linkerd之后的监控：\n因为还处于测试环境观察阶段，因此数据指标不是特别高，也没有那么明显。\n从上图可以发现，两个POD基本上做到的波峰波谷同步，而不是我已原地爆炸，你还快活逍遥。\n客户端LB我也在尝试，有两个方向：\n客户端使用k8s API获取服务列表甚至负载，以实现 服务发现 + 基于负载的LB策略。 使用额外的服务注册和发现中心，记录服务负载指标，客户端只需要实现LB策略即可。 水平有限，如有错误，欢迎勘误指正🙏\n参考文献 # https://github.com/yeqown/k8s-grpc-lb-solutions https://www.youtube.com/watch?v=F2znfxn_5Hg https://linkerd.io/2/overview/ https://github.com/grpc/grpc/blob/master/doc/load-balancing.md "},{"id":24,"href":"/2020/08/12/%E8%BF%91%E6%9C%9F%E4%BD%BF%E7%94%A8Docker%E6%89%93%E5%8C%85%E9%95%9C%E5%83%8F%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/","title":"近期使用Docker打包镜像遇到的问题总结","section":"Posts","content":" 背景 # 在 github.com/yeqown/goreportcard 项目中我改造了 goreportcard。 后续为了方便部署，我准备将其打包成为docker镜像并上传到 DockerHub。期间遇到了下面的问题，并一一解决，这里做一个记录帮助以后遇到类似的问题可以快速解决。\n初期的目标是：将goreportcard和golangci-lint编译好，尽可能较小镜像的体积。因此第一次尝试，我使用了分阶段编译，用golang:1.14.1编译，alpine来发布。\n基本 Dockerfile 如下：\n# building stage FROM golang:1.14-alpine3.11 as build WORKDIR /tmp/build COPY . . RUN export GOPROXY=\u0026#34;https://goproxy.cn,direct\u0026#34; \\ \u0026amp;\u0026amp; go mod download \\ \u0026amp;\u0026amp; go build -o app ./cmd/goreportcard-cli/ \\ \u0026amp;\u0026amp; go get github.com/golangci/golangci-lint \u0026amp;\u0026amp; go install github.com/golangci/golangci-lint/cmd/golangci-lint # release stage FROM golang:1.14-alpine3.11 as release WORKDIR /app/goreportcard COPY --from=build /tmp/build/app . COPY --from=build /tmp/build/tpl ./tpl COPY --from=build /tmp/build/assets ./assets # FIXED: 不能使用golangci-lint, `File not found` 错误 COPY --from=build /go/bin/golangci-lint /usr/local/bin EXPOSE 8000 ENTRYPOINT [\u0026#34;./app\u0026#34;, \u0026#34;start-web\u0026#34;, \u0026#34;\u0026amp;\u0026#34;] 问题清单和解决方案 # 由于并不是所有的问题都和Docker有关，因此我会使用 [分类] 在标题上注明。\n0. 如何调试Docker打包过程 [Docker] # 在Docker打包过程中会有如下字样，其中---\u0026gt; 5d5cac8457ad 字样就是一个可以运行的镜像，因此我们可以尝试运行这个镜像来获取我们想要的信息， 而不用在 Dockerfile 中通过 echo 来Debug🐶。\ndocker build -t goreportcard:v1.0.0 . Sending build context to Docker daemon 12.08MB Step 1/13 : FROM golang:1.14-alpine3.11 as build ---\u0026gt; 5d5cac8457ad Step 2/13 : WORKDIR /tmp/build ---\u0026gt; Using cache ---\u0026gt; c57b9255d5fb Step 3/13 : COPY . . 运行打包过程中的镜像\ndocker run -it c57b9255d5fb 1 golangci-lint 编译好后复制到 alpine中无法运行？[Go] # 遇到这个问题的时候，提示错误信息大致为File or Path not found，但同时 goreportcard 是可以运行，这两者是在相同地环境下编译出来的。 因此肯定是编译的时候存在差异，初步判断为CGO_ENABLED，GOOS，GOARCH三个变量，通过搜索和尝试最终统一增加这三个环境变量解决问题。\n\u0026amp;\u0026amp; export CGO_ENABLED=0 \\ \u0026amp;\u0026amp; export GOARCH=amd64 \\ \u0026amp;\u0026amp; export GOOS=linux \\ 2. golangci-lint 需要Go可执行文件? [Go] # // TODO:\n3. go get 需要 git / ssh 客户端？ [Go] # // TODO:\n4. 在 alpine 中如何安装预编译好的软件，如git？ [Docker] # 这个是在git的 linux安装帮助 里找到的😂，也不知道为啥，只能夸下git可真专业\napk add git 5. 在 alpine 中安装软件特别慢，如何解决？ [Docker] # sed -i \u0026#39;s/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g\u0026#39; /etc/apk/repositories 6. 镜像中 go get 怎么使用宿主的缓存？ [Go+Docker] # 把宿主机的 path/go/pkg 挂载到 /go/pkg，这里默认使用的是go的容器镜像。\n7. could not import C (no metadata for C) [Go] # https://github.com/golangci/golangci-lint/issues/602\n"},{"id":25,"href":"/2020/08/06/opentracing%E5%AE%9E%E6%88%98/","title":"Opentracing实战","section":"Posts","content":" 背景 # 在没有链路追踪系统的情况下，如果只要少数几个服务，或许还可以通过日志来排查定位问题。但是如果服务一旦超过10个，那么再想通过日志来定位分析问题将无比繁琐。 因为，你先要从大量的日志中删筛选出某次请求的日志数据，才能进行后续的定位分析。 倘若日志系统也不够完善，日志对于调试毫无帮助，那又得退回到最原始的方式，通过代码断点和增加日志，等待问题复现，或者通过肉眼检查代码。 不是说这种方式不行，而是大部分的程序员的业务需求比较紧张，这样的排查手段效率和收益远远达不到要求（如果你有时间，当我没说 🐶）。\n在实际场景中，我也遇到了这样的问题：\n日志系统里包含了过少的信息，对于调试几乎没有帮助 (几乎只有错误日志，缺少输出上下文的日志)。 服务调用复杂，一个请求失败，只能透过错误码和错误信息进行判断是否存在调用失败的情况。 调用链路复杂的情况下，想要对某个请求进行优化，无从下手。 这里只列举了跟trace相关的一些原始场景，当然从上面的描述中还能发现日志系统不够完善，对调试不友好，不过这里首要解决的问题是链路追踪问题。\n如果对路链路追踪没有概念，还望自行查阅资料，这里不会过多介绍～\nOpentracing # 注意：Opentracing 是一套标准接口，而不是具体实现。\n这里就实战opentracing + jaeger 的链路追踪方案。其中 opentracing 是一套标准接口，而jaeger是包含了 opentracing 的实现的一套工具。 Trace链路简单示例如下：\nTrace # 描述在分布式系统中的一次\u0026quot;事务\u0026quot;。\nSpan # 表示工作流的一部分的命名和定时操作。可以接受标签(Tag Key:Value)，以及附加到特定span实例的标注(Annotation)，如时间戳和结构化日志。\nSpanContext # 追踪伴随分布式事务的信息，包括它通过网络或通过消息总线将服务传递给服务的时间。span上下文包含TraceId、SpanId和追踪系统需要传播到下游服务的其他数据。\n实战 # 这里我准备的是 Go 项目，服务之间通过gRPC通信。链路如下：\n+-- process internal trace2 | +---\u0026gt; process internal trace1 | | +---\u0026gt; server-b trace(gRPC) entry(HTTP) ---\u0026gt; server-a trace--gRPC--| +---\u0026gt; server-c trace(gRPC) | +----\u0026gt; process internal trace3 从上图中可以明确，我们的目标是：实践跨服务调用和服务内部调用的链路追踪，配合jaeger我们还可以将链路信息可视化。\n我有了服务，怎么实施落地？ # 为了回答这个问题，我把这个问题结合opentracing的概念再分解一下：\nParentSpan 从哪儿来？ ChildSpan由ParentSpan创建，那么什么时候创建？ 链路Tracer从哪儿来？ Trace信息怎么传递？ 链路信息如何搜集？ Parent-Span 从哪儿来？链路从哪里开始。 # 在上述的实践目标中，我们只有一个入口服务，那么很明显每一次的\u0026quot;事务\u0026quot;都是在这个入口（http entry）开启的。我写了如下的中间件（基于gin）。\n// 这个定义是因为opentracing没有定义获取traceId 和spanId的方法，而我又实践了 zipkin 和 jaeger type getTraceID func(spCtx opentracing.SpanContext) string // get trace info from header, if not then create an new one func Opentracing(getTraceIdFromSpanContext getTraceID) gin.HandlerFunc { return func(c *gin.Context) { // prepare work ... // 这里首先尝试从客户端请求中获取到trace链路信息，如果获取到则创建child span. carrier := opentracing.HTTPHeadersCarrier(c.Request.Header) clientSpCtx, err := tracer.Extract(opentracing.HTTPHeaders, carrier) if err != nil { log.Printf(\u0026#34;could not extract trace data from http header, err=%v\\n\u0026#34;, err) } // derive a span or create an root span sp = tracer.StartSpan( c.Request.RequestURI, opentracing.ChildOf(clientSpCtx), ) defer sp.Finish() // do some work // ... // 将含有span的 context.Context 设置到 gin.Context 用于传递span ctx = opentracing.ContextWithSpan(c.Request.Context(), sp) c.Set(_traceContextKey, ctx) traceId := getTraceIdFromSpanContext(sp.Context()) c.Header(\u0026#34;X-Trace-Id\u0026#34;, traceId) // continue process request c.Next() // do some work // ... } } Child-Span什么时候创建？ # 其实在上述的中间件里已经有了体现（跨服务调用时）。这里主要有两种场景：跨服务调用，服务内部调，这两种的区别在于是否是同一个进程。 针对跨服务调用，我们使用了gRPC来通信，那么创建的时机就在于gRPC客户端发起调用时，需要创建一个childSpan并传递给服务端，服务端需要解析到该span并在当次请求中使用。 而对于服务内部的调用，相较而言会更简单一点，直接使用该span创建childSpan就好了。\n这里需要注意的是，一般来说在Go开发过程中推荐使用Context作为函数调用的第一个参数，opentracing也考虑了这一点，如下： 官方提供了对应的方法，来帮助使用者把span和context.Context一起使用。\n// ContextWithSpan returns a new `context.Context` that holds a reference to // the span. If span is nil, a new context without an active span is returned. func ContextWithSpan(ctx context.Context, span Span) context.Context { if span != nil { if tracerWithHook, ok := span.Tracer().(TracerContextWithSpanExtension); ok { ctx = tracerWithHook.ContextWithSpanHook(ctx, span) } } return context.WithValue(ctx, activeSpanKey, span) } // SpanFromContext returns the `Span` previously associated with `ctx`, or // `nil` if no such `Span` could be found. // // NOTE: context.Context != SpanContext: the former is Go\u0026#39;s intra-process // context propagation mechanism, and the latter houses OpenTracing\u0026#39;s per-Span // identity and baggage information. func SpanFromContext(ctx context.Context) Span { val := ctx.Value(activeSpanKey) if sp, ok := val.(Span); ok { return sp } return nil } 在中间件那里，我们已经把 context.Context 设置到了 gin.Context 中去了，因此在后续的使用中，我们需要把它从 gin.Context 取出并传递。\n// traceHdl is a trace handler from HTTP request func traceHdl(c *gin.Context) { // get root Context from request // TODO: try to use c.Request.WithContext() to set context ctx, ok := c.Get(x.GetTraceContextKey()) if !ok { panic(\u0026#34;impossible\u0026#34;) } // ctx在服务内部传递，ctx含有span信息 if err := clientCall(ctx.(context.Context)); err != nil { c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;message\u0026#34;: err.Error()}) return } // response to client c.JSON(http.StatusOK, gin.H{\u0026#34;message\u0026#34;: \u0026#34;traceHdl done\u0026#34;}) } func clientCall(ctx context.Context) error { // 注意：这里使用了gRPC来做跨服务的调用，对于ctx的处理，是通过interceptor实现的。 // 这部分代码会在后面贴上，但是逻辑也跟类似，只是多了需要适配gRPC数据传递的规则。 _, err := serverAConn.PingA(ctx, \u0026amp;pb.PingAReq{ Now: time.Now().Unix(), From: \u0026#34;client\u0026#34;, }) if err != nil { return err } // 这里演示进程内链路 return processInternalTrace1(ctx) } // internal process trace example 1 func processInternalTrace1(ctx context.Context) error { ctx2, sp := x.StartSpanFromContext(ctx) defer sp.Finish() println(\u0026#34;processInternalTrace1 called\u0026#34;) // do some ops time.Sleep(10 * time.Millisecond) return processInternalTrace2(ctx2) } gRPC interceptor 实现如下：服务端工作内容相似，只是从inject操作变成了extract。\n// client interceptor func OpenTracingClientInterceptor(tracer opentracing.Tracer, optFuncs ...Option) grpc.UnaryClientInterceptor { otgrpcOpts := newOptions() otgrpcOpts.apply(optFuncs...) return func( ctx context.Context,m ethod string, req, resp interface{}, cc *grpc.ClientConn, invoker grpc.UnaryInvoker, opts ...grpc.CallOption, ) error { var err error var parentCtx opentracing.SpanContext if parent := opentracing.SpanFromContext(ctx); parent != nil { parentCtx = parent.Context() } // ... clientSpan := tracer.StartSpan( method, opentracing.ChildOf(parentCtx), ext.SpanKindRPCClient, gRPCComponentTag, ) defer clientSpan.Finish() // 注入 ctx = injectSpanContext(ctx, tracer, clientSpan) // ... // 调用 err = invoker(ctx, method, req, resp, cc, opts...) // ...\treturn err } } func injectSpanContext(ctx context.Context, tracer opentracing.Tracer, clientSpan opentracing.Span) context.Context { md, ok := metadata.FromOutgoingContext(ctx) if !ok { md = metadata.New(nil) } else { md = md.Copy() } mdWriter := metadataReaderWriter{md} err := tracer.Inject(clientSpan.Context(), opentracing.HTTPHeaders, mdWriter) // We have no better place to record an error than the Span itself :-/ if err != nil { clientSpan.LogFields(log.String(\u0026#34;event\u0026#34;, \u0026#34;Tracer.Inject() failed\u0026#34;), log.Error(err)) } return metadata.NewOutgoingContext(ctx, md) } 至此我们就介绍完了，服务间（gRPC）调用和服务内（context）调用的childSpan的创建和传递。\n链路Tracer从哪儿来？ # 先说Tracer有什么用，Tracer是用来管理Span的统筹者，负责创建span和传播span。\n// Tracer负责创建span和传播span type Tracer interface { // Create, start, and return a new Span with the given `operationName` and // incorporate the given StartSpanOption `opts`. (Note that `opts` borrows // from the \u0026#34;functional options\u0026#34; pattern, per // http://dave.cheney.net/2014/10/17/functional-options-for-friendly-apis) // // A Span with no SpanReference options (e.g., opentracing.ChildOf() or // opentracing.FollowsFrom()) becomes the root of its own trace. // // Examples: // // var tracer opentracing.Tracer = ... // // // The root-span case: // sp := tracer.StartSpan(\u0026#34;GetFeed\u0026#34;) // // // The vanilla child span case: // sp := tracer.StartSpan( // \u0026#34;GetFeed\u0026#34;, // opentracing.ChildOf(parentSpan.Context())) // // // All the bells and whistles: // sp := tracer.StartSpan( // \u0026#34;GetFeed\u0026#34;, // opentracing.ChildOf(parentSpan.Context()), // opentracing.Tag{\u0026#34;user_agent\u0026#34;, loggedReq.UserAgent}, // opentracing.StartTime(loggedReq.Timestamp), // ) // StartSpan(operationName string, opts ...StartSpanOption) Span // Inject() takes the `sm` SpanContext instance and injects it for // propagation within `carrier`. The actual type of `carrier` depends on // the value of `format`. // // OpenTracing defines a common set of `format` values (see BuiltinFormat), // and each has an expected carrier type. // // Other packages may declare their own `format` values, much like the keys // used by `context.Context` (see https://godoc.org/context#WithValue). // // Example usage (sans error handling): // // carrier := opentracing.HTTPHeadersCarrier(httpReq.Header) // err := tracer.Inject( // span.Context(), // opentracing.HTTPHeaders, // carrier) // // NOTE: All opentracing.Tracer implementations MUST support all // BuiltinFormats. // // Implementations may return opentracing.ErrUnsupportedFormat if `format` // is not supported by (or not known by) the implementation. // // Implementations may return opentracing.ErrInvalidCarrier or any other // implementation-specific error if the format is supported but injection // fails anyway. // // See Tracer.Extract(). Inject(sm SpanContext, format interface{}, carrier interface{}) error // Extract() returns a SpanContext instance given `format` and `carrier`. // // OpenTracing defines a common set of `format` values (see BuiltinFormat), // and each has an expected carrier type. // // Other packages may declare their own `format` values, much like the keys // used by `context.Context` (see // https://godoc.org/golang.org/x/net/context#WithValue). // // Example usage (with StartSpan): // // // carrier := opentracing.HTTPHeadersCarrier(httpReq.Header) // clientContext, err := tracer.Extract(opentracing.HTTPHeaders, carrier) // // // ... assuming the ultimate goal here is to resume the trace with a // // server-side Span: // var serverSpan opentracing.Span // if err == nil { // span = tracer.StartSpan( // rpcMethodName, ext.RPCServerOption(clientContext)) // } else { // span = tracer.StartSpan(rpcMethodName) // } // // // NOTE: All opentracing.Tracer implementations MUST support all // BuiltinFormats. // // Return values: // - A successful Extract returns a SpanContext instance and a nil error // - If there was simply no SpanContext to extract in `carrier`, Extract() // returns (nil, opentracing.ErrSpanContextNotFound) // - If `format` is unsupported or unrecognized, Extract() returns (nil, // opentracing.ErrUnsupportedFormat) // - If there are more fundamental problems with the `carrier` object, // Extract() may return opentracing.ErrInvalidCarrier, // opentracing.ErrSpanContextCorrupted, or implementation-specific // errors. // // See Tracer.Inject(). Extract(format interface{}, carrier interface{}) (SpanContext, error) } 之前已经提到了了opentracing是一套接口，那么具体的实现是在其他的工具中完成的，如jaeger。创建的时候就需要jaeger的支持了，如下：\n// 使用jaeger来创建一个tracer，并注入到opentracing全局中去 func BootTracerWrapper(localServiceName string, hostPort string) error { tracer, err := xjaeger.BootJaegerTracer(localServiceName, hostPort) if err != nil { return errors.Wrap(err, \u0026#34;BootTracerWrapper.BootZipkinTracer\u0026#34;) } // 注入到全局后，就可以通过 opentracing.GlobalTracer() 来使用 tracer了 opentracing.SetGlobalTracer(tracer) return nil } func BootJaegerTracer(localServiceName, hostPort string) (opentracing.Tracer, error) { cfg := \u0026amp;config.Configuration{ Sampler: \u0026amp;config.SamplerConfig{ Type: jaeger.SamplerTypeConst, Param: 1, }, ServiceName: localServiceName, // 服务名 Reporter: \u0026amp;config.ReporterConfig{ LogSpans: true, CollectorEndpoint: _jaegerRecorderEndpoint, }, // 链路搜集配置 } tracer, _, err := cfg.NewTracer( config.Logger(jaegerlog.StdLogger), config.ZipkinSharedRPCSpan(true), ) if err != nil { return nil, errors.Wrap(err, \u0026#34;BootJaegerTracer\u0026#34;) } return tracer, nil } Trace信息怎么传递？ # 在 Child-Span什么时候创建？ 中提到了 Inject, Extract 方法，这两个方法就是用来辅助Trace信息传递的方法， 具体的实现也是在jaeger中实现的，有兴趣的可以自行查阅代码。\n链路信息如何搜集？ # 在 #链路Tracer从哪儿来 已经提到了jaeger在创建tracer时可以指定搜集器的配置， 因此上报动作会在jaeger中完成，但是要注意的是，显式调用 sp.Finish() 才会触发上报动作。\n总结 # 最终实战结果截图如下： 图中包含了调用链路层级关系，每个环节的耗时情况，请求的入参和结果数据（异常信息，如有）等。同时还支持自定义（Tag 和 Annotation 功能）， 如果还想要更多的信息，那么推荐使用这两个功能组合来满足需求。\n再多的案例和文档，都没有自己上手实践的效果好，建议实际运行并查阅opentracing源码。所有的代码均可在 https://github.com/yeqown/opentracing-practice 中找到。\n水平有限，如有错误，欢迎勘误指正🙏。\n参考文献 # https://opentracing.io/docs/overview/ https://github.com/opentracing/opentracing-go https://github.com/uber/jaeger-client-go https://github.com/yeqown/opentracing-practice "},{"id":26,"href":"/2020/04/13/channel-in-Go%E5%B0%8F%E7%BB%93/","title":"Channel in Go小结","section":"Posts","content":"在其他编程语言中，如果想要在线程中通信，最常用的手段是共享内存。然而考虑到线程冲突问题，不得不考虑加锁，以保证并发安全，加锁也一定会带来额外的开销，对性能产生影响。\nCSP模型 # 在 Go 语言中也能使用共享内存加互斥锁进行通信，但是 Go 语言提供了一种不同的并发模型，也就是通信顺序进程（Communicating sequential processes，CSP）1。Goroutine 和 Channel 分别对应 CSP 中的实体和传递信息的媒介，Go 语言中的 Goroutine 会通过 Channel 传递数据。\n使用示例 # 在使用之前，需要对channel有个整体的印象：\nFIFO (First In First Out) 分为有缓冲和无缓冲两种 在使用过程中会阻塞（无缓冲时，只操作读或写；有缓冲已满时，只操作读或者写） 接受者和发送者都是goroutine 参考下图：\nfunc main() { // Q: 有缓冲和无缓冲在使用上有什么区别？ // ch := make(chan int) // 无缓冲 ch := make(chan int, 1) // 有缓冲，大小为1 // 发送 ch \u0026lt;- 1 fmt.Println(\u0026lt;-ch) } 注意事项 # 在使用channel时，需要注意一下事项：\n操作\\CH状态 ch为空 ch已关闭 ch正常 发送 ch \u0026lt;- 死锁 panic 成功或阻塞 接收 \u0026lt;-ch 死锁 成功或空值 成功或阻塞 关闭 close(ch) panic panic 成功 Q: 这里考虑下如何优雅的关闭channel (避免panic)?\n// A: 代码上从发送端控制channel的关闭，同时为了避免重复关闭，使用sync.Once来协助。 // create ch := make(chan int, 233) // sending ch \u0026lt;- 1 // close from sender once.Do(close(ch)) Make 详解 # channel通过make关键字创建，并分配缓冲区大小。\n// runtime/chan.go#makechan func makechan(t *chantype, size int) *hchan { elem := t.elem // ... // elem.size * size 是否溢出 mem, overflow := math.MulUintptr(elem.size, uintptr(size)) if overflow || mem \u0026gt; maxAlloc-hchanSize || size \u0026lt; 0 { panic(plainError(\u0026#34;makechan: size out of range\u0026#34;)) } // Hchan does not contain pointers interesting for GC when elements stored in buf do not contain pointers. // buf points into the same allocation, elemtype is persistent. // SudoG\u0026#39;s are referenced from their owning thread so they can\u0026#39;t be collected. // TODO(dvyukov,rlh): Rethink when collector can move allocated objects. // 大意是是说，当存储在buf的元素中不包含指针时，gc对此不感兴趣？（留坑）因此才会将buf和hchan分配在同一片内存空间。 var c *hchan switch { case mem == 0: // 无缓冲（只分配hchan） c = (*hchan)(mallocgc(hchanSize, nil, true)) c.buf = c.raceaddr() case elem.ptrdata == 0: // 不含指针（hchan和缓冲区在一起） c = (*hchan)(mallocgc(hchanSize+mem, nil, true)) // buf的起始地址 = 基地址 + hchan结构体大小偏移量 c.buf = add(unsafe.Pointer(c), hchanSize) default: // 有缓冲，含指针（hchan和缓冲区不在一起） c = new(hchan) c.buf = mallocgc(mem, elem, true) } c.elemsize = uint16(elem.size) c.elemtype = elem c.dataqsiz = uint(size) // ... return c } Close 详解 # close(ch) 可以关闭channel。已经关闭的channel不可以发送，也不能再被关闭，但是还能接收。如果缓冲数据被处理完那么会接受到空值。\n// runtime/chan.go#closechan func closechan(c *hchan) { if c == nil { // 如果channel为空，则会触发panic panic(plainError(\u0026#34;close of nil channel\u0026#34;)) } lock(\u0026amp;c.lock) if c.closed != 0 { // 关闭已经关闭的channel，也会触发panic unlock(\u0026amp;c.lock) panic(plainError(\u0026#34;close of closed channel\u0026#34;)) } // ... // 设置已关闭标志位 c.closed = 1 var glist gList // 释放所有的recvq中的g for { sg := c.recvq.dequeue() if sg == nil { break } // 清空数据 if sg.elem != nil { typedmemclr(c.elemtype, sg.elem) sg.elem = nil } if sg.releasetime != 0 { sg.releasetime = cputicks() } gp := sg.g gp.param = nil // ... glist.push(gp) } // 释放所有的sendq中的g for { // ... 与接受者处理类似 } unlock(\u0026amp;c.lock) // 让所有的读/写g就绪 for !glist.empty() { gp := glist.pop() gp.schedlink = 0 goready(gp, 3) } } Send 详解 # 向channel发送最常见的就是如下2种方式：\n// case 1 ch \u0026lt;- val // case 2 select { case ch \u0026lt;- 2: // foo default: // bar } 在编译时都会被变成成调用func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool的函数，但是两者在对block的赋值上并不相同，如下：\n// case 1 func chansend1(c *hchan, elem unsafe.Pointer) { chansend(c, elem, true, getcallerpc()) } // case 2 func selectnbsend(c *hchan, elem unsafe.Pointer) (selected bool) { return chansend(c, elem, false, getcallerpc()) } 也就是说在case2中，channel发送并不会阻塞当前的goroutine，就算是无缓冲，或者缓冲已满的情况。\n发送期间处理流程，如下图：\nRecv 详解 # 从channel中接收数据，也有两种方式 (普通用法和非阻塞用法)：\n// case 1 val := \u0026lt;- ch // case 2 select { case val := \u0026lt;- ch: // foo default: // bar } // 此外接收，还有另外一个参数，ok表示已接收 val, ok := \u0026lt;- ch 在编译时都会被变成成调用func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool)的函数，同样的两者在对block的赋值上并不相同，如下：\n// case 1 func chanrecv1(c *hchan, elem unsafe.Pointer) { chanrecv(c, elem, true) } // 多返回值 func chanrecv2(c *hchan, elem unsafe.Pointer) (received bool) { _, received = chanrecv(c, elem, true) return } // case 2 func selectnbrecv(elem unsafe.Pointer, c *hchan) (selected bool) { selected, _ = chanrecv(c, elem, false) return } // 多返回值 func selectnbrecv2(elem unsafe.Pointer, received *bool, c *hchan) (selected bool) { // TODO(khr): just return 2 values from this function, now that it is in Go. selected, *received = chanrecv(c, elem, false) return } 相较于send场景，recv场景稍微复杂一些，但是大体流程上保持一致，如下图：\n简单总结的说，就是recv为了实现在发送者阻塞的场景下的FIFO，做了特殊处理。\n使用场景和模式 # 下面列举了一些比较常用的channel使用模式及相关场景。这里主要参考了advanced-go-programming-book。\n如有遗漏，敬请补充\nSynchronous # 一般来说在需要同步goroutine的场景下，会使用sync.Mutex或者sync.WaitGroup来同步。这里用channel来实现一下goroutine同步。\nfunc main() { done := make(chan int, 1) go func(){ println(\u0026#34;i\u0026#39;m running\u0026#34;) done \u0026lt;- 1 }() // 如果有多个goroutine, 可以对done接收计数，或者检测goroutine关闭来判断同步完成 \u0026lt;-done println(\u0026#34;main goroutine done\u0026#34;) } Producer-Consumer # func Producer(ch chan int) { for { ch \u0026lt;- 1 } } func Consumer(ch chan int) { for { v \u0026lt;- ch println(v) } } func main() { ch := make(chan int, 2) go Producer(ch) go Producer(ch) go Consumer(ch) go Consumer(ch) select{} } Pub-Sub # 发布订阅与生产者消费者类似，唯一区别在于发布订阅模型中，发布者不关心有多少个订阅者即每个订阅者都应该收到消息，而在生产者消费者模型中，消费者是集群消费（集群消费是说每一条消息只有一个消费者可以消费）。使用channel也可以很简单的实现Pub/Sub这种模型:\ntype topic struct{ chPub chan int chSub []chan int bufsize int } func newTopic(bufsize int) *topic { return \u0026amp;topic{ bufsize: bufsize, chPub: make(chan int, bufsize), chSub: make([]chan int, 0, 5), } } // 这里不是并发安全的！！！ func (t *topic) Sub() chan int { ch := make(chan int, t.bufsize) t.chSub = append(t.chSub, ch) return ch } func (t *topic) Pub(v int) { t.chPub \u0026lt;- v // 分发 go func() { v2 := \u0026lt;- t.chPub for idx := range t.chSub { t.chSub[idx] \u0026lt;- v2 } }() } // Q：如何实现topic的关闭方法 func main() { topic := newTopic(topic) // 开启5个订阅 for i := 0; i \u0026lt; 5; i++ { go func() { ch := topic.Sub() for v := range ch { println(\u0026#34;recv topic\u0026#34;, v) } }() } // 发布消息 for v := 10; v \u0026lt; 100; v++{ topic.Pub(v) time.Sleep(100 *time.Millisecond) } } Concurrent Pool # 并发控制也是channel的重要应用场景，利用channel会阻塞的特性，得以控制最大goroutine数。在Web应用中可以用于实现限流。下面实现的方式，类似与令牌桶，另外推荐一个chanpool的实现以供参考。\ntype obj int func factory() obj { return \u0026#34;newobj\u0026#34; } type pool struct { ch chan obj } func newpool() pool { p := pool{ ch: make(chan obj, 10), } for i := 0; i \u0026lt; 10; i++ { p.put(obj(i)) } } // 需要增加并发安全！！！ func (p pool) put(obj obj) { p.ch \u0026lt;- obj } // 需要增加并发安全！！！ func (p pool) get() obj { v := \u0026lt;- p.ch return v } func main() { pool := newpool() go func() { for { println(\u0026#34;num of running g:\u0026#34;, runtime.NumGoroutine()) time.Sleep(100 * time.Millisecond) } }() for i := 0; i \u0026lt; 100; i++ { obj := pool.get() go func() { defer pool.put(obj) // do some work time.Sleep(100 *time.Millisecond)\t}() } } // num of running g: 12 // num of running g: 12 // num of running g: 12 // num of running g: 12 // num of running g: 12 // num of running g: 12 // num of running g: 12 // num of running g: 12 // num of running g: 12 // num of running g: 12 总结 # 掌握channel的原理，能帮助我们在实际应用中快速定位和分析并发带来的问题。也能在高并发场景下，合理利用channel。\n水平有限，如有错误，欢迎勘误指正🙏。\n参考 # https://draveness.me/golang/docs/part3-runtime/ch06-concurrency/golang-channel/ https://github.com/golang/go/tree/release-branch.go1.14 https://github.com/eapache/channels https://colobu.com/2018/03/26/channel-patterns/ https://juejin.im/post/5decff136fb9a016544bce67 https://chai2010.cn/advanced-go-programming-book/ch1-basic/ch1-06-goroutine.html "},{"id":27,"href":"/2020/04/02/%E6%B6%88%E6%81%AF%E6%8E%A8%E9%80%81%E6%9E%B6%E6%9E%84-based-GOIM/","title":"消息推送架构-Based-GOIM","section":"Posts","content":" 本文的重点，主要梳理了GOIM的架构，消息流转和消息处理。本文没有提到Comet的具体逻辑，套接字编程和RingBuffer等，但是Comet的复杂度远高于其他两个网元，因此强烈建议阅读Comet源码，应该会对Go网络编程有更多认识。\nGOIM 是Go实现的消息推送的分布式服务，易于扩容伸缩，使用了bilibili/discovery来支持服务发现。相较于我之前用Socket.IO做的信令服务，优点在于更优雅的扩容，将连接层和逻辑层分离，职责更清晰。当然缺点也有（没有和具体实现解耦，如MQ的选型，导致不够灵活；客户端非全双工通信，TCP利用率偏低，这点并不全是缺点，好处是：消息流转清晰，职责非常明确），这部分可以自己做定制（最后的参考文献2中讲很多）。\n架构图 # 总的来说，整个应用的架构如下\n这里省略了其中用于服务发现的“bilibili/discovery”。在整个GOIM中用到服务发现的主要是gRPC和消息推送关系查找。\n如上图：\nComet负责建立和维持客户端的长连接； Job负责消息的分发； Logic提供三种纬度的消息（全局，ROOM，用户）投递，还包括业务逻辑，Session管理。 消息流转 # 从上述的架构图中可以知道，消息是通过HTTP调用Logic产生的，然后用MQ来中转（存储，削峰）；每个Job成员都从给队列中消费消息，投递给一个或者多个Comet，由Comet将消息发送给客户端。\n生成消息 # 目前在Github上的GOIM版本，消息（除鉴权/心跳等基础数据包外）生成都是由Logic完成第一手处理，Logic提供了HTTP接口以支持消息发送能力，主要有三个纬度：用户，房间，全应用广播，如下：\ncurl -d \u0026#39;mid message\u0026#39; http://api.goim.io:3111/goim/push/mids?operation=1000\u0026amp;mids=123 curl -d \u0026#39;room message\u0026#39; http://api.goim.io:3111/goim/push/room?operation=1000\u0026amp;type=live\u0026amp;room=1000 curl -d \u0026#39;broadcast message\u0026#39; http://api.goim.io:3111/goim/push/all?operation=1000 在Logic服务中会通过处理，将消息处理成**附#消息格式#任务队列消息**的格式，然后投递到MQ中。其中三种纬度的消息处理稍有不同：\n用户\n// goim/internal/logic/push.go // mid =\u0026gt; []PushMsg{op, server, keys, msg} func (l *Logic) PushMids(c context.Context, op int32, mids []int64, msg []byte) (err error) { // 根据用户ID获取所有的 key:server 对应关系；在redis中是一个hash keyServers, _, err := l.dao.KeysByMids(c, mids) // ... keys := make(map[string][]string) for key, server := range keyServers { // ... keys[server] = append(keys[server], key) } for server, keys := range keys { // 通过DAO组装PushMsg投递给MQ if err = l.dao.PushMsg(c, op, server, keys, msg); err != nil { return } } return } 房间 没什么特别的处理\n// goim/internal/logic/push.go func (l *Logic) PushRoom(c context.Context, op int32, typ, room string, msg []byte) (err error) { return l.dao.BroadcastRoomMsg(c, op, model.EncodeRoomKey(typ, room), msg) } // // goim/internal/logic/dao func (d *Dao) BroadcastRoomMsg(c context.Context, op int32, room string, msg []byte) (err error) { pushMsg := \u0026amp;pb.PushMsg{ Type: pb.PushMsg_ROOM, Operation: op, Room: room, Msg: msg, } b, err := proto.Marshal(pushMsg) // ... if err := d.nsqProducer.Publish(d.c.Nsq.Topic, b); err != nil { log.Errorf(\u0026#34;PushMsg.send(push pushMsg:%v) error(%v)\u0026#34;, pushMsg, err) } return } 广播 没什么特别的处理\n// goim/internal/logic/push.go func (l *Logic) PushAll(c context.Context, op, speed int32, msg []byte) (err error) { return l.dao.BroadcastMsg(c, op, speed, msg) } // goim/internal/logic/dao func (d *Dao) BroadcastMsg(c context.Context, op, speed int32, msg []byte) (err error) { pushMsg := \u0026amp;pb.PushMsg{ Type: pb.PushMsg_BROADCAST, Operation: op, Speed: speed, // 这里需要去到Job才知道speed的具体功效 Msg: msg, } b, err := proto.Marshal(pushMsg) if err != nil { return } if err := d.nsqProducer.Publish(d.c.Nsq.Topic, b); err != nil { log.Errorf(\u0026#34;PushMsg.send(push pushMsg:%v) error(%v)\u0026#34;, pushMsg, err) } return } 小结：\n针对用户单发时，会获取到具体的sever和keys组装到PushMsg 房间消息，没有server和keys, 但是多一个room是通过typ和roomID组装而成的 “live://1000” 广播消息，除了消息体之外，另外有一个speed字段 传输消息 # 由Logic处理好的消息会放在MQ中，Job任务会自动消费消息，然后通过gRPC调用Comet单元。相比其他两个网元，Job就简单多了。从MQ中消费到消息后会调用c.job.push(ctx, pushMsg)。\n// job 发送消息（普通消息，房间消息，广播） func (j *Job) push(ctx context.Context, pushMsg *pb.PushMsg) (err error) { switch pushMsg.Type { case pb.PushMsg_PUSH: err = j.pushKeys(pushMsg.Operation, pushMsg.Server, pushMsg.Keys, pushMsg.Msg) case pb.PushMsg_ROOM: // 获取一个job中的Room缓存，用于房间内“定时，定量”发送消息，减少请求次数 // 这里调用的Push并不会立即发送，而是放在Room.proto这个channel中 // 实际放松是由Room.pushproc来定时 err = j.getRoom(pushMsg.Room).Push(pushMsg.Operation, pushMsg.Msg) case pb.PushMsg_BROADCAST: err = j.broadcast(pushMsg.Operation, pushMsg.Msg, pushMsg.Speed) default: err = fmt.Errorf(\u0026#34;no match push type: %s\u0026#34;, pushMsg.Type) } return } // 根据serverID发送给特定的Comet服务，避免广播 // cometServers 是由discovery服务发现维护的comet列表。 func (j *Job) pushKeys(operation int32, serverID string, subKeys []string, body []byte) (err error) { buf := bytes.NewWriterSize(len(body) + 64) p := \u0026amp;comet.Proto{ Ver: 1, Op: operation, Body: body, } p.WriteTo(buf) p.Body = buf.Buffer() p.Op = comet.OpRaw var args = comet.PushMsgReq{ Keys: subKeys, ProtoOp: operation, Proto: p, } if c, ok := j.cometServers[serverID]; ok { if err = c.Push(\u0026amp;args); err != nil { log.Errorf(\u0026#34;c.Push(%v) serverID:%s error(%v)\u0026#34;, args, serverID, err) } log.Infof(\u0026#34;pushKey:%s comets:%d\u0026#34;, serverID, len(j.cometServers)) } return } // 处理成一个BroadcastReq,并广播给所有的Comet func (j *Job) broadcast(operation int32, body []byte, speed int32) (err error) { // ... 与pushKeys一致，生成一个p comets := j.cometServers // 如 speed = 64, len(comets) = 2, speed = 32 speed /= int32(len(comets)) var args = comet.BroadcastReq{ ProtoOp: operation, Proto: p, Speed: speed, // 是被传递给Comet处理，继续跟踪 } for serverID, c := range comets { if err = c.Broadcast(\u0026amp;args); err != nil { log.Errorf(\u0026#34;c.Broadcast(%v) serverID:%s error(%v)\u0026#34;, args, serverID, err) } } log.Infof(\u0026#34;broadcast comets:%d\u0026#34;, len(comets)) return } 房间消息处理：\ngetRoom(roomID) -\u0026gt; room.Push() -\u0026gt; p -\u0026gt; room.proto | |---\u0026gt; NewRoom(batch, duration) | |---\u0026gt; go room.pushproc() -\u0026gt; p \u0026lt;- room.proto // goim/internal/job/room.go type Room struct { c *conf.Room // 关于房间的配置 job *Job // 绑定job，为了追溯Room所属的Job id string // 房间ID proto chan *comet.Proto // 有缓冲channel } // pushproc merge proto and push msgs in batch. // 默认batch = 20, sigTime = 1s func (r *Room) pushproc(batch int, sigTime time.Duration) { var ( n int last time.Time p *comet.Proto buf = bytes.NewWriterSize(int(comet.MaxBodySize)) // 4096B = 4KB ) // 设置了一个定时器,在一定时间后往room.proto放送一个roomReadyProto信号。 td := time.AfterFunc(sigTime, func() { select { case r.proto \u0026lt;- roomReadyProto: default: } }) defer td.Stop() for { if p = \u0026lt;-r.proto; p == nil { // 如果创建了room，但是读到空包 break // exit } else if p != roomReadyProto { // 读取room.proto 如果是正常的数据包，则合并到buf中去,如果满了怎么办？ p.WriteTo(buf) // 如果是第一个数据包，则重置定时器，并继续读取后续数据包 if n++; n == 1 { last = time.Now() td.Reset(sigTime) continue } else if n \u0026lt; batch { // 后续的数据包，不会重置定时器，但是如果时间仍在第一个数据包的 sigTime 时间间隔内 // 简单说，定时器还没到时间 if sigTime \u0026gt; time.Since(last) { continue } } // 累计的数据包数量已经超过了batch, 执行发送动作 } else { // 定时器到读到了roomReadyProto // 如果buf已经被重置了，则跳出循环执行清理动作；否则执行发送消息\tif n == 0 { break } } // 发送房间内的消息 _ = r.job.broadcastRoomRawBytes(r.id, buf.Buffer())\tbuf = bytes.NewWriterSize(buf.Size()) n = 0 // 如果配置了房间最大闲置时间，则重新设定定时器 // 也就是说，如果房间被创建后，处理完了该房间的消息，并不是直接跳出循环清理房间 // 而是，会阻塞等待下一次的消息再来，如果在 “1m / r.c.Idle” 时间内没有来，则会跳出循环清理掉该房间 // 如果在 “1m / r.c.Idle” 内有消息，则会重新设定定时器为sigTime，并为proto计数 if r.c.Idle != 0 { td.Reset(time.Duration(r.c.Idle)) // 默认15分钟 } else { td.Reset(time.Minute) } } // 清理动作 r.job.delRoom(r.id) } 总结如下图：\n小结：\n针对用户单发时，会直接发送到对应的comet服务，根据key再去给特定的channel发送消息 房间消息，这个会特殊一些，Job持有一个特殊的Room结构，用于合并发送到该房间的消息，定时定量发送房间消息（好处是，减少了gRPC调用次数降低系统负载，缺点增加时消息延迟） 广播消息，将消息封装到BroadcastPushReq中，然后直接发送给所有的Comet 投递消息 # Comet接收到Job单元的gRPC调用之后，会将消息通过Websocket套接字按照GOIM约定的协议格式发送给指定客户端。从Job那边传输过来的消息，依旧是分为用户消息，房间消息，全局消息。这里得先说明下Comet是如何管理用户端的长连接，如下图：\nBucket是在一个管理Channel和Room的数据结构，它的作用在于使用了hash来将Channel做分片管理，相较于集中管理，这样channel分布在不同的bucket中而不是一个map，可以降低冲突，减小锁的粒度。\n有了上述结构，那么消息发送在忽略传输层的情况下：\n针对用户单发\n调用链路为：comet.Bucket(key).Channel(key).Push(proto)，这里Push也只是将proto放在了channel的队列中（10缓冲），消息的下发在goim/internal/comet/server_websocket.go#dispatchWebsocket。\n房间消息\n在Comet内部遍历Buckets并调用Bucket.BroadcastRoom()，但是这里也只是把消息放到了“某处”，并没有直接发送。实际发送代码在goim/internal/comet/bucket.go#roomproc。\n// BroadcastRoom broadcast a message to specified room func (b *Bucket) BroadcastRoom(arg *grpc.BroadcastRoomReq) { // 这里取模选中一个goroutine来执行任务 num := atomic.AddUint64(\u0026amp;b.routinesNum, 1) % b.c.RoutineAmount // b.routines 是 b.c.RoutineAmount 数量的 有 b.c.RoutineSize 缓冲大小的 chan *grpc.BroadcastRoomReq b.routines[num] \u0026lt;- arg } // 在创建Bucket时，便开启了goroutine来处理 func (b *Bucket) roomproc(c chan *grpc.BroadcastRoomReq) { for { arg := \u0026lt;-c if room := b.Room(arg.RoomID); room != nil { room.Push(arg.Proto) } } } // 遍历房间内的channel的链表，将消息放到channel的发送队列中，又回到了channel消息单发的逻辑。 func (r *Room) Push(p *grpc.Proto) { r.rLock.RLock() for ch := r.next; ch != nil; ch = ch.Next { _ = ch.Push(p) } r.rLock.RUnlock() } 广播消息\n在Comet内部遍历Buckets并向Bucket中的所有Channel发送消息。这里终于用到了speed，上文提到过，如果设定speed = 64， len(comets) = 2, 那么这里用到的 speed = 32。\n// Broadcast broadcast msg to all user. func (s *server) Broadcast(ctx context.Context, req *pb.BroadcastReq) (*pb.BroadcastReply, error) { if req.Proto == nil { return nil, errors.ErrBroadCastArg } go func() { for _, bucket := range s.srv.Buckets() { bucket.Broadcast(req.GetProto(), req.ProtoOp) if req.Speed \u0026gt; 0 { //该bucket // 有0个channel时，t = 0 / 32 = 0 // 有2个channel时, t = 2 / 32 = 0.0625 // 有32个channel时, t = 32 / 32 = 1 // 有64个channel时，t = 64 / 32 = 2 // 由此可得，（comet）speed 的含义是 每个bucket每秒最多发送的消息数量 t := bucket.ChannelCount() / int(req.Speed) time.Sleep(time.Duration(t) * time.Second) } } }() return \u0026amp;pb.BroadcastReply{}, nil } // 广播，直接从bucket.chs中遍历 func (b *Bucket) Broadcast(p *grpc.Proto, op int32) { var ch *Channel b.cLock.RLock() for _, ch = range b.chs { // 如果不在该channel的监听队列中，那么该消息不会发给该客户端 // 这个监听队列，是在建立连接是发送的 “accepts” 字段中取得的 // 譬如accpets = [1000, 1001, 1002], 但是op = 1003， 那么就不会发送 // // 值得注意的是，这个op是从BroadcastReq.ProtoOp取得，BroadcastReq.ProtoOp又是从pushMsg.Operation取得 // 也就是说 op = grpc.BroadcastReq.ProtoOp = proto.Op = PushMsg.Operation = 从发送消息接口产生。 // if !ch.NeedPush(op) { continue } _ = ch.Push(p) } b.cLock.RUnlock() } 小结：\n针对用户单发时，直接利用key定位到Bucket和Channel，将消息放到队列中。 房间消息，将消息分配到房间协程之一的队列中，在该协程中会持续不断的消费消费消息并处理，处理动作是将消息分发到Channel的消息队列（buffered chan）上。 广播消息，直接使用了bucket的chs遍历，来为每一个Channel推送一条消息到消息队列上。 附 # 这里会展示GOIM中必要的数据结构，帮助理解GOIM中的数据流转过程。 这里会出现几个名词：\nserver: comet服务的hostname (string) mid: 用户在业务中的ID (int64) key: 用户在GOIM中的唯一ID (string) Session结构 # Session由Redis管理，维持了客户端MID，Server，Key的关系，这部分是在Logic中gRPC服务的Connect方法中设置。如下图所示：\n// goim/internal/logic/conn.go func (l *Logic) Connect(c context.Context, server, cookie string, token []byte) (mid int64, key, roomID string, accepts []int32, hb int64, err error) { var params struct { Mid int64 `json:\u0026#34;mid\u0026#34;` // 用户ID Key string `json:\u0026#34;key\u0026#34;` // 客户端标识别，如果为空则自动生成UUID RoomID string `json:\u0026#34;room_id\u0026#34;` // 客户端加入房间 Platform string `json:\u0026#34;platform\u0026#34;`// 客户端所在平台 Accepts []int32 `json:\u0026#34;accepts\u0026#34;` // 监听房间 } if err = json.Unmarshal(token, \u0026amp;params); err != nil { log.Errorf(\u0026#34;json.Unmarshal(%s) error(%v)\u0026#34;, token, err) return } mid = params.Mid roomID = params.RoomID accepts = params.Accepts hb = int64(l.c.Node.Heartbeat) * int64(l.c.Node.HeartbeatMax) if key = params.Key; key == \u0026#34;\u0026#34; { key = uuid.New().String() } if err = l.dao.AddMapping(c, mid, key, server); err != nil { log.Errorf(\u0026#34;l.dao.AddMapping(%d,%s,%s) error(%v)\u0026#34;, mid, key, server, err) } log.Infof(\u0026#34;conn connected key:%s server:%s mid:%d token:%s\u0026#34;, key, server, mid, token) return } // goim/internal/logic/dao/redis.go // func (d *Dao) AddMapping(c context.Context, mid int64, key, server string) (err error) { // ... if mid \u0026gt; 0 { if err = conn.Send(\u0026#34;HSET\u0026#34;, keyMidServer(mid), key, server); err != nil { log.Errorf(\u0026#34;conn.Send(HSET %d,%s,%s) error(%v)\u0026#34;, mid, server, key, err) return } if err = conn.Send(\u0026#34;EXPIRE\u0026#34;, keyMidServer(mid), d.redisExpire); err != nil { log.Errorf(\u0026#34;conn.Send(EXPIRE %d,%s,%s) error(%v)\u0026#34;, mid, key, server, err) return } // ... } if err = conn.Send(\u0026#34;SET\u0026#34;, keyKeyServer(key), server); err != nil { log.Errorf(\u0026#34;conn.Send(HSET %d,%s,%s) error(%v)\u0026#34;, mid, server, key, err) return } if err = conn.Send(\u0026#34;EXPIRE\u0026#34;, keyKeyServer(key), d.redisExpire); err != nil { log.Errorf(\u0026#34;conn.Send(EXPIRE %d,%s,%s) error(%v)\u0026#34;, mid, key, server, err) return } // ... } 从AddMapping方法中，总结下得到：\n如果(mid=1, key=69dafe8b58066478aea48f3d0f384820,server=comet.001) mid_1 = {69dafe8b58066478aea48f3d0f384820: comet.001} key_69dafe8b58066478aea48f3d0f384820 = \u0026quot;comet.001\u0026quot; 也就是说，同一个用户可以在多个地方同时连入系统；同时也能看出来，Session管理并不包括用户所在的房间，用户需要接受哪些房间的消息，这部分是在是在Logic.Connect处理好了之后通过gRPC响应，交给Comet处理的。\n// goim/internal/comet/server_websocket.go func (s *Server) ServeWebsocket(conn net.Conn, rp, wp *bytes.Pool, tr *xtime.Timer) { // ... if p, err = ch.CliProto.Set(); err == nil { if ch.Mid, ch.Key, rid, accepts, hb, err = s.authWebsocket(ctx, ws, p, req.Header.Get(\u0026#34;Cookie\u0026#34;)); err == nil { ch.Watch(accepts...) // 监听房间列表 b = s.Bucket(ch.Key) // 根据用户key选择一个bucket (对key做cityhash再取模) err = b.Put(rid, ch) // 将用户ID和连接Channel维护到Bucket中 if conf.Conf.Debug { log.Infof(\u0026#34;websocket connnected key:%s mid:%d proto:%+v\u0026#34;, ch.Key, ch.Mid, p) } } } // ... } // auth for goim handshake with client, use rsa \u0026amp; aes. func (s *Server) authWebsocket(ctx context.Context, ws *websocket.Conn, p *grpc.Proto, cookie string) (mid int64, key, rid string, accepts []int32, hb time.Duration, err error) { for { if err = p.ReadWebsocket(ws); err != nil { return } if p.Op == grpc.OpAuth { break } else { log.Errorf(\u0026#34;ws request operation(%d) not auth\u0026#34;, p.Op) } } // rid roomID if mid, key, rid, accepts, hb, err = s.Connect(ctx, p, cookie); err != nil { return } p.Op = grpc.OpAuthReply p.Body = nil if err = p.WriteWebsocket(ws); err != nil { return } err = ws.Flush() return } 消息格式 # 1 - 任务队列消息：\n不管是个人消息，还是房间消息和广播消息，都是用的如下结构；其中Op和Type可以帮助Job单元可以针对消息上做差异化的处理。\ntype PushMsg struct { Type PushMsg_Type // 消息类型，个人，房间广播，广播 Operation int32 // 指令 goim/api/comet/grpc/operation.go Speed int32 // 广播时用 TODO: Server string // Comet的Hostname, 个人消息时指定 Room string // 房间号 Keys []string // bucket key Msg []byte // 消息体 } 2 - GOIM消息协议：\n区别于任务队列消息，这个条消息是客户端实际收到的消息（对比可以发现，其中只有Op和Body是从Logic单元传递过来的，其他字段很大一部分用于分流（定位Comet/Bucket/Room/Channel），或者系统字段用于差异化处理消息）：\ntype Proto struct { Ver int32 // 版本号 Op int32 // 消息类型，如Ping，Pong, Text Seq int32 // 序列号 TODO: Body []byte // 消息体 等于 PushMsg.Msg } 服务发现 # 服务发现可以帮助整个应用发现Comet单元和Logic单元，而Job单元并不需要注册自己（不需要被发现）。当然可以没有服务发现功能，直接在代码和配置中配置好（Comet/Logic）服务地址，但是也就失去了动态扩容的能力。另外，如果是K8S部署，这里的服务发现功能就有点冗余了，因此需要做一些调整再用K8S部署，调整包括（服务注册和发现抽象即于discovery结耦，可选开启；对于Comet的gRPC调用，在针对用户单发消息时，需要从定向单播变成广播）。\n总结 # GOIM将整个应用职责，分配给三个单个独立网元来承担相应的工作，让流程更清晰，应用也易于扩展（动态）。 在用户和长连接的映射上，使用了Key来区别应用用户和业务用户，得以支持单个用户同时登陆（多平台）的场景；另外key也作为了Comet网元定位用户的唯一标识；利用了bucket + cityhash来降低竞争，加速用户定位并发送消息。 在房间消息的处理上，出于消息频繁和业务场景的考虑：在Job上为房间消息增加了数据包合并机制；在Comet层为每一个Bucket都创建了一定数量的goroutine来持续处理房间消息。这两个动作，都能提高整个应用对于房间消息处理能力，提升吞吐量。 从Job端将消息分发到Comet时，除了单个用户的消息能够指定Comet以外，其他的消息都只能广播给所有的Comet处理。 水平有限，如有错误，欢迎勘误指正🙏。\n参考文档 # https://github.com/Terry-Mao/goim https://juejin.im/post/5cd12fa16fb9a0320b40ec32 "},{"id":28,"href":"/2020/03/29/redis%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/","title":"Redis主从复制","section":"Posts","content":"redis主从复制是高可用方案中的一部分，那主从复制是如何进行的？又是如何实现的？怎么支撑了redis的高可用性？在主从模式下Master和Slave节点分别做了哪些事情？\nredis高可用方案是什么？ # 我理解的redis高可用的特点有：\n高QPS，主从 =\u0026gt; 读写分离 高容量，集群分片 =\u0026gt; 高容量 故障转移，sentinel =\u0026gt; 故障转移 故障恢复，数据持久 =\u0026gt; 故障恢复 ～ 这里我简单的理解（RDB + AOF）= 故障恢复 主从复制 # redis 主从复制有两个版本：旧版（Ver2.8-），新版（Ver2.8+，增加PSYNC命令来解决旧版中的问题）\n讨论复制时都需要考虑两种场景：\n场景1：从节点刚刚上线，需要去同步主节点时，这部分可以理解为 全量复制。 场景2：从节点掉线，恢复上线后需要同步数据，使自己和主节点达到一致状态。这部分在旧版复制里等价于全量复制，在新版里可以理解为增量复制。 当然你肯定会想到如果主节点掉线，这时候会怎么样？这个场景当然也在redis高可用方案中，之时不是本文的重点，属于Sentinel机制的内容了。\n旧版主从复制 # 前文说过了，旧版主从复制只有全量复制用于应付上述两个场景，因此下面的流程也只有一份：\n从服务器向主服务器发送sync命令。 主服务器在收到sync命令之后，调用bgsave命令生成最新的rdb文件，将这个文件同步给从服务器，这样从服务器载入这个rdb文件之后，状态就会和主服务器执行bgsave命令时候的一致。 主服务器将保存在命令缓冲区中的写命令同步给从服务器，从服务器执行这些命令，这样从服务器的状态就跟主服务器当前状态一致了。 如果你不知道redis中还有个缓冲区的话，建议系统的了解下redis中缓冲区的设计。这里缓冲区特指命令缓冲区，后面还会讲到复制缓冲区。\n但是这样的实现在 场景2 下的缺点很明显：如果说从节点断线后迅速上线，这段时间内的产生的写命令很少，却要全量复制主库的数据，传输了大量重复数据。\nSYNC命令产生的消耗： 1. 主节点生成RDB，需要消耗大量的CPU，内存和磁盘IO 2. 网络传输大量字节数据，需要消耗主从服务器的网络资源 3. 从节点需要从RDB文件恢复，会造成阻塞无法接受客户端请求 优点就是：简单暴力。个人看来在redis架构中不合适的用法，不代表说实际场景中也一定不合适，简单暴力也是一个很大的优点。\n新版主从复制 # 新版的主从复制跟旧版的区别就在于：对场景2的优化。\n场景2的缺点上文已经提到过了，那么优化的方向就是**“尽量不使用全量复制；增加增量复制(PSYNC)的功能”**。为此还要解决下列问题：\n如果某个从节点断线了，重新上线该从节点如何知道自己是否应该全量还是增量复制呢？ 该从节点断线恢复后，又怎么知道自己缺失了哪些数据呢？ 主节点又如何补偿该从节点在断线期间丢失的那部分数据呢？旧版的复制除了RDB，还有从命令缓冲区中的写命令来保持数据一致。 为此新版中使用了以下概念：\n运行ID - runid # 每个redis服务器都有其runid，runid由服务器在启动时自动生成，主服务器会将自己的runid发送给从服务器，而从服务器会将主服务器的runid保存起来。从服务器redis断线重连之后进行同步时，就是根据runid来判断同步的进度：\n如果前后两次主服务器runid一致，则认为这一次断线重连还是之前复制的主服务器，主服务器可以继续尝试部分同步操作。 如果前后两次主服务器runid不相同，则全同步流程。 复制偏移量 - offset # 主从节点，分别会维护一个复制偏移量： 主服务器每次向从服务器同步了N字节数据之后，将修改自己的复制偏移量+N。从服务器每次从主服务器同步了N字节数据之后，将修改自己的复制偏移量+N。通过对比主从节点的偏移量很容易就可以发现，主从节点是否处于一致状态。\n复制（积压）缓冲区 - copybuffer # 一个固定长度（可配置）的FIFO队列，默认大小 = 1MB；预测值 = second * write_size_per_second。当从节点重新连上主节点时候，从节点会通过PSYNC命令将自己的复制偏移量（offset）发送给主服务器，主节点会根据偏移量会判断该执行何种同步：\n如果从节点offset之后的数据仍然存在复制缓冲区中，就执行部分重同步。 反之，如果不存在，那么执行完全重同步。 因为复制缓冲区不可能无限大，因此为了尽可能多的利用部分重同步，需要针对真实场景估算出最合适的复制缓冲区大小。\n至此，redis新版PSYNC通过上述概念和流程，解决了场景2下旧版复制中的资源浪费问题，流程图和示例图见下文。\n示例图如下，ABCD四个从节点，其中A执行部分中同步，D执行了完整重同步。\n总结 # 水平有限，如有错误，欢迎勘误指正🙏。\n参考文献 # 《redis设计与实现》 https://zhuanlan.zhihu.com/p/65712373 "},{"id":29,"href":"/2020/01/21/gin%E6%BA%90%E7%A0%81%E7%AE%80%E8%A6%81%E5%88%86%E6%9E%90/","title":"Gin源码简要分析","section":"Posts","content":" 概述 # 通过日常对gin场景出发，深入源码，总结介绍gin的核心设计。包含：Engine / HandlerFunc / RouterGroup(Router) / Context。在日常使用中常见的就以上概念，汇总如下：\n概念 解释 应用意义 Engine 引擎 web server的基础支持，也是服务的入口 和 根级数据结构 RouterGroup(Router) 路由 用于支持gin,REST路由绑定和路由匹配的基础，源于radix-tree数据结构支持 HandlerFunc 处理函数 逻辑处理器和中间件实现的函数签名 Context 上下文 封装了请求和响应的操作，为HandlerFunc的定义和中间件模式提供支持 从DEMO开始 # type barForm struct { Foo string `form:\u0026#34;foo\u0026#34; binding:\u0026#34;required\u0026#34;` Bar int `form:\u0026#34;bar\u0026#34; binding:\u0026#34;required\u0026#34;` } func (fooHdl FooHdl) Bar(c *gin.Context) { var bform = new(barForm) if err := c.ShouldBind(bform); err != nil { // true: parse form error return } // handle biz logic and generate response structure // c (gin.Context) methods could called to support process-controling c.JSON(http.StatusOK, resp) // c.String() alse repsonse to client } // mountRouters . func mountRouters(engi *gin.Engine) { // use middlewares engi.Use(gin.Logger()) engi.Use(gin.Recovery()) // mount routers group := engi.Group(\u0026#34;/v1\u0026#34;) { fooHdl := demohtp.New() group.GET(\u0026#34;/foo\u0026#34;, fooHdl.Bar) group.GET(\u0026#34;/echo\u0026#34;, fooHdl.Echo) // subGroup := group.Group(\u0026#34;/subg\u0026#34;) // subGroup.GET(\u0026#34;/hdl1\u0026#34;, fooHdl.SubGroupHdl1) // 最终路由：\u0026#34;targetURI = /v1/subg/hdl1\u0026#34; } } func main() { engi := gin.New() mountRouters(engi) if err := engi.Run(\u0026#34;:8080\u0026#34;); err != nil { log.Fatalf(\u0026#34;engi exit with err=%v\u0026#34;, err) } } 通过上述的代码就简单开启了一个gin server，其中就包括了常见的：路由注册，中间件注册，路由分组，服务启动。核心概念也就是刚刚在上文提到的那四个概念。概览流程如下图：\n可以参照DEMO中的方法名在图中检索改流程。\nEngine # // Engine is the framework\u0026#39;s instance, it contains the muxer, middleware and configuration settings. type Engine struct { // 路由管理 RouterGroup // 省略非关心属性 // ... // context poo，支持context复用，减少对象创建提高性能。 pool sync.Pool // []methodTree方法树根节点集合 这部分在路由部分会详细介绍 trees methodTrees } Engine是根入口，它把RouterGroup结构图体嵌入自身，以获得了Group，GET，POST等路由管理方法。从 Run方法的代码：\n因为直接复制代码太多了，推荐下载源码来跳转，更便捷高效。\nfunc (engine *Engine) Run(addr ...string) (err error) { defer func() { debugPrintError(err) }() address := resolveAddress(addr) debugPrint(\u0026#34;Listening and serving HTTP on %s\\n\u0026#34;, address) // 使用的是标准库的http服务启动方法 // 如果看过http.ListenAndServe函数签名，就知道engine实现了http.Handler方法， // 也就是它一定有一个ServeHTTP方法 err = http.ListenAndServe(address, engine) return } 通过层层跳转我们可以找到图中淡紫色部分中的handlerHTTPRequest方法，在ServeHTTP函数时，engine会从pool中取得一个Context对象，并准备好Request和ResponseWriter的相关数据设置到Context中去，提供给方法链调用。\n// 调用中间件和请求逻辑函数 func (engine *Engine) handleHTTPRequest(c *Context) { httpMethod := c.Request.Method rPath := c.Request.URL.Path unescape := false if engine.UseRawPath \u0026amp;\u0026amp; len(c.Request.URL.RawPath) \u0026gt; 0 { rPath = c.Request.URL.RawPath unescape = engine.UnescapePathValues } if engine.RemoveExtraSlash { rPath = cleanPath(rPath) } // 这里必须知道gin使用的是 “基数树（Radix Tree）” 用于存储路由 // https://michaelyou.github.io/2018/02/10/%E8%B7%AF%E7%94%B1%E6%9F%A5%E6%89%BE%E4%B9%8BRadix-Tree/ t := engine.trees for i, tl := 0, len(t); i \u0026lt; tl; i++ { if t[i].method != httpMethod { // 从这里可以知道，engine中的methodTrees []methodTree是请求方法分组的路由匹配树。 continue } root := t[i].root // root数据结构（radix-tree）？如何查找？ value := root.getValue(rPath, c.Params, unescape) // 找到路由对应的处理函数 if value.handlers != nil { c.handlers = value.handlers // 复制节点的所有handlers c.Params = value.params // 路由参数 c.fullPath = value.fullPath // 全路径 c.Next() // 依次调用HandlerChain中的函数 c.writermem.WriteHeaderNow() return } // 省略，这部分流程控制也自行阅读 } // 省略... } 从上述流程我们可以总结得到图中紫色部分调用链：\nengine.Run -\u0026gt; http.ListenAndServe -\u0026gt; engine.handleHTTPRequest -\u0026gt; c.Next\n但是其中还有不清楚的问题：\n路由是通过方法确定到该方法树的根节点，再查询到路由对应的节点，那么在radix-tree中是如何进行路由匹配的呢？ c.handlers 直接从匹配到的路由节点的handlers复制得到，那么中间件是如何挂在到其中的呢？ 获取到c.handlers之后，只调用了一次c.Next，那么该链路是如何继续调用下去的？ Path Param是如何获取到？root.getValue RouterGroup \u0026amp; MethodTree # 这部分是除了Context概念之外理解gin的第二核心部分，提供路由注册，调用链路函数链处理，路由分组，路由匹配功能。在开始之前，必须知道gin的路由是由httprouter提供，而httprouter包是使用了radix-tree来实现路由管理功能。\n// IRouter defines all router handle interface includes single and group router. type IRouter interface { IRoutes Group(string, ...HandlerFunc) *RouterGroup } // IRoutes defines all router handle interface. type IRoutes interface { Use(...HandlerFunc) IRoutes Handle(string, string, ...HandlerFunc) IRoutes Any(string, ...HandlerFunc) IRoutes GET(string, ...HandlerFunc) IRoutes POST(string, ...HandlerFunc) IRoutes DELETE(string, ...HandlerFunc) IRoutes PATCH(string, ...HandlerFunc) IRoutes PUT(string, ...HandlerFunc) IRoutes OPTIONS(string, ...HandlerFunc) IRoutes HEAD(string, ...HandlerFunc) IRoutes StaticFile(string, string) IRoutes Static(string, string) IRoutes StaticFS(string, http.FileSystem) IRoutes } // RouterGroup is used internally to configure router, a RouterGroup is associated with // a prefix and an array of handlers (middleware). type RouterGroup struct { Handlers HandlersChain // 中间件 basePath string // 路由前缀 engine *Engine // 指向engine入口的指针 root bool // 是否是root } .路由注册 # 通过任意一个路由注册方法，都可以进入到 func (group *RouterGroup) handle(httpMethod, relativePath string, handlers HandlersChain) IRoutes 这个方法。\n// 挂载路由的实际处理函数 func (group *RouterGroup) handle(httpMethod, relativePath string, handlers HandlersChain) IRoutes { // 计算最终的绝对路径 base + relativePath // 注意base也是绝对路径 absolutePath := group.calculateAbsolutePath(relativePath) // 合并处理函数，将中间件和逻辑函数结合在一起 // 一般来说这里传入的handlder是逻辑函数 len(handlers) = 1 // 只有少数的handler会有自己的中间件处理函数 len(handlers) \u0026gt; 1 handlers = group.combineHandlers(handlers) // 将处理好的 HandlersChain 加载到Radix Tree中去 // 这也表明，这里的RouterGroup只会载处理路由时发挥作用 group.engine.addRoute(httpMethod, absolutePath, handlers) return group.returnObj() } engine在初始化的时候会设置RouterGroup.basePath = \u0026ldquo;/\u0026rdquo; engine是将相同请求方法挂载到同一棵树下\n看到这里回到engine.addRoute中去，通过深入发现如下调用链：\nengine.GET -\u0026gt; routergroup.GET -\u0026gt; routergroup.handle -\u0026gt; engine.addRoute -\u0026gt; methodTree.addRoute -\u0026gt; node(radix-tree\u0026rsquo;s node).insertChild\n.路由分组 # RouterGroup通过Group函数来衍生下一级别的RouterGroup，会拷贝父级RouterGroup的中间件，重新计算basePath。\nfunc (group *RouterGroup) Group(relativePath string, handlers ...HandlerFunc) *RouterGroup { return \u0026amp;RouterGroup{ Handlers: group.combineHandlers(handlers), // 初始化时，拷贝父亲节点的中间件 basePath: group.calculateAbsolutePath(relativePath), // 初始化时，计算孩子group的绝对路径 engine: group.engine, } } .中间件挂载 # 有两个地方提供给中间件挂载，一个是RouterGroup.Use集中挂载中间件；另一个地方是挂载特定路由的时候(RouterGroup.GET / POST 等等)，将中间件和逻辑处理函数一起挂载。第二种方式在路由注册的时候已经见过了，这里再说下Use方法，RouterGroup.Handlers = append(RouterGroup.Handlers, middlewareHandlers)，这里只是简单的将后来的中间件复制到RouterGroup的中间件上，没有其他的逻辑。\n.路由匹配 # 这部分全都是交由radix-tree来负责的部分。\nHandlerFunc # type HandlerFunc func(*Context) // 处理函数签名 type HandlersChain []HandlerFunc // 处理函数链 这部分没什么好说的，就是定义了函数签名，指定Context作为上下文传递的关键数据。\nContext # Context是上下文传递的核心，它包括了请求处理，响应处理，表单解析等重要工作。\n// Context is the most important part of gin. It allows us to pass variables between middleware, // manage the flow, validate the JSON of a request and render a JSON response for example. type Context struct { writermem responseWriter // 实现了http.ResponseWriter 和 gin.ResponseWriter Request *http.Request // http.Request, 暴露给handler // gin.ResponseWriter 包含了： // http.ResponseWriter，http.Hijacker，http.Flusher，http.CloseNotifier和额外方法 // 暴露给handler，是writermm的复制 Writer ResponseWriter Params Params // 路径参数 handlers HandlersChain // 调用链 index int8 // 当前handler的索引 fullPath string engine *Engine // 对engine的引用 Keys map[string]interface{} // c.GET / c.SET 的支持，常用于session传递。 // Errors is a list of errors attached to all the handlers/middlewares who used this context. Errors errorMsgs // Accepted defines a list of manually accepted formats for content negotiation. Accepted []string // query 参数缓存 queryCache url.Values // 表单参数缓存, 跟queryCache作用类似 formCache url.Values } .调用链流转和控制 # 从Engine部分已经知道，当路由被匹配到之后会执行一次c.Next，Next逻辑非常简单如下：\nfunc (c *Context) Next() { // 从c.reset 可以知道 c.index == -1 c.index++ for c.index \u0026lt; int8(len(c.handlers)) { c.handlers[c.index](c) // 发生调用 c.index++ } } // 63，这意味这如果最大链路超过了63，那么通过index的流程控制就出问题了。 const abortIndex int8 = math.MaxInt8 / 2 func (c *Context) Abort() { c.index = abortIndex } 要注意的是，当你在调用链中的某一个handler中调用了c.Abort之类的函数，调用链会直接退出也是通过c.index来控制的。除了调用链流转，在Context这一部分还比较重要的是，参数的解析，响应处理。\n.参数解析 # 参数传递一共有一下几种方式：\n序号 参数类型 解释 Context支持 1 path param 在URI中将参数作为路径的一部分 c.Param(\u0026ldquo;key\u0026rdquo;) string 2 query param 在URI中以\u0026quot;?\u0026ldquo;开始的，\u0026ldquo;key=value\u0026quot;形式的部分 c.Query(\u0026ldquo;key\u0026rdquo;) string 3 body [form; json; xml等等] 根据请求头Content-Type判定或指定 c.Bind类似函数 // Content-Type MIME of the most common data formats. const ( MIMEJSON = \u0026#34;application/json\u0026#34; MIMEHTML = \u0026#34;text/html\u0026#34; MIMEXML = \u0026#34;application/xml\u0026#34; MIMEXML2 = \u0026#34;text/xml\u0026#34; MIMEPlain = \u0026#34;text/plain\u0026#34; MIMEPOSTForm = \u0026#34;application/x-www-form-urlencoded\u0026#34; MIMEMultipartPOSTForm = \u0026#34;multipart/form-data\u0026#34; MIMEPROTOBUF = \u0026#34;application/x-protobuf\u0026#34; MIMEMSGPACK = \u0026#34;application/x-msgpack\u0026#34; MIMEMSGPACK2 = \u0026#34;application/msgpack\u0026#34; MIMEYAML = \u0026#34;application/x-yaml\u0026#34; ) 关于第三种参数解析是最常用的，也是最复杂的部分包含的内容比较多，这里就不展开了。别忘了，binding的同时还有参数校验是基于“github.com/go-playground/validator”实现的。\n.响应处理 # 常用的响应方法有c.String(http.Status, string), c.JSON(http.Status, interface{})\nc.Render(code http.Status, r gin.Render) -\u0026gt; r.Render\n// Render interface is to be implemented by JSON, XML, HTML, YAML and so on. type Render interface { // Render writes data with custom ContentType. Render(http.ResponseWriter) error // WriteContentType writes custom ContentType. WriteContentType(w http.ResponseWriter) } 这里以gin.render.JSON为例：\n这里json包并不是直接使用的标准库json，而是经过了一层包装用于支持jsoniter替换json。 这一支持是基于golang选择性编译实现的。\n// Render (JSON) writes data with custom ContentType. func (r JSON) Render(w http.ResponseWriter) (err error) { if err = WriteJSON(w, r.Data); err != nil { panic(err) } return } // WriteContentType (JSON) writes JSON ContentType. func (r JSON) WriteContentType(w http.ResponseWriter) { writeContentType(w, jsonContentType) } // WriteJSON marshals the given interface object and writes it with custom ContentType. func WriteJSON(w http.ResponseWriter, obj interface{}) error { writeContentType(w, jsonContentType) encoder := json.NewEncoder(w) err := encoder.Encode(\u0026amp;obj) return err } 总结 # 这里知识讲了把gin作为API Server开发时常用到的主要流程和数据结构，其中更多的流程控制部分被省略了，可以自行结合代码和源码阅读。这里主要解析了gin中函数调用链路和handlers流程控制，RadixTree的部分都被省略了，算法菜鸡不敢乱说，掌握后再结合gin的源码重读。\n文中代码偏多，最好是结合源码和图片食用。\n水平有限，如有错误，欢迎勘误指正🙏。\n参考文献 # https://michaelyou.github.io/2018/02/10/%E8%B7%AF%E7%94%B1%E6%9F%A5%E6%89%BE%E4%B9%8BRadix-Tree/ "},{"id":30,"href":"/2020/01/17/%E8%AE%B0%E4%B8%80%E6%AC%A1%E6%8E%92%E6%9F%A5gRPC%E4%BD%BF%E7%94%A8%E4%B8%8D%E5%BD%93%E5%AF%BC%E8%87%B4%E7%9A%84goroutine%E6%B3%84%E6%BC%8F/","title":"一次gRPC使用不当导致goroutine泄漏排查记录","section":"Posts","content":" 由于保留必要的“罪证”，因此某些异常只能通过文字来描述了～\n背景 # 昨晚上10点左右，前端童鞋反映开发环境接口响应超时，但过了几分钟后又恢复了，于是有了这一篇文章。\n其实很久以前就出现了内存占用异常的情况～，只是占用并不高也就是50MB左右，加上当时还忙着写业务需求就没有急着加上pprof来检查。\n首先通过运维平台(k8s based)直观发现了该pod数量从1变成了2, 再结合新增pod的启动时间，我发现该时间正好是前端童鞋反映状况的时间节点，稍后我检查了下该服务的资源限制如下图：\n那么前端童鞋反映的问题就很明显了，由于某种原因导致了pod内存超限，触发了运维平台对于内存超限的“容忍机制”。表现为: 新增一个pod用于缓解服务压力，老服务由于无法申请更多内存会导致崩溃或其他异常（无法响应客户端请求），这与反映的情况一致。\npprof排查 # 知道了服务内存异常，想要具体定位的话，这时候就需要pprof上场了。\n如果你需要重启服务才能开启pprof的话，那么只能等待复现了。这里我在开发环境和测试环境一直开启了pprof，因此可以直接检查。个人觉得，这样还可以帮助开发和测试，完成最初的性能分析😼。\n内存检查 # go tool pprof --http=:8080 https://service.host.com/debug/pprof/heap 这个命令是在本地打开一个web服务，直接可视化该服务的内存占用情况。也可以使用:\ngo tool pprof https://service.host.com/debug/pprof/heap 使用交互模式来分析。通过这个步骤定位到了 grpc相关的包内存占用异常分为两个部分：\n50MB+ google.golang.org/grpc/internal/transport.newBufWriter 50MB+ bufio.NewReaderSize http2 相关库的占用也比较多 这一切都指向了我们使用的gRPC，可是为啥使用gRPC会用到这么“多”内存呢？接着分析\ngoroutine检查 # 打开一看 https://service.host.com/debug/pprof/一看，goroutine和heap居“高”(4000+)不下，虽然对于动辄10W+的别人家的服务来说，这点根本不算事，但在我们这种小作坊里可就算异常了。点开看goroutine查看详情，有四个部分的goroutine分别有900个左右，这里就算初步定位了“gRPC客户端使用了较多的goroutine，但是却没有正确的结束掉”，如下图（这是解决后的截的图）：\npprof总结 # 服务中使用的gRPC客户端出了某些故障，导致了goroutine泄漏，引发了OOM（Out Of Memory）。如下图：\n代码排查 # 上一步已经定位到是gRPC客户端的问题，那么就可以直接从代码上手了。我心里已经有一个“嫌疑犯”了，如下：\nvar ( defaultHandler *handler timeout = 5 * time.Second // _ pb.UserServiceClient = \u0026amp;handler{} ) // Init of usersvc.handler func Init(rpcAddr string) error { // ... 略去不表 } type handler struct { // rpc configs rpcAddr string client pb.UserServiceClient lastGrpcReqError error } func (h *handler) connectRPC() { if h.client != nil \u0026amp;\u0026amp; h.lastGrpcReqError != nil { // 这里判断的本意是：如果客户端初始化失败， // 或者期间因为异常情况，导致客户端与服务端连接中断的情况下尝试重连。 // // 但是忽略了gRPC实现中，对于客户端的处理： // 1. grpc.Dail 是异步的 // 2. grpc 有自己的重连机制 // // 这一部分我还没有看完，就不乱发表看法了。 conn, err := grpc.Dial(h.rpcAddr, grpc.WithInsecure()) if err != nil { logger.Std.Errorf(\u0026#34;could not dial: %s with err: %v\u0026#34;, h.rpcAddr, err) return } logger.Std.Infof(\u0026#34;usersvc.client.connectRPC called\u0026#34;) h.client = pb.NewUserServiceClient(conn) } } // QueryBasicInfoByID based default Handler . func QueryBasicInfoByID(in *pb.ByIDForm) (*pb.BasicInfoResponse, error) { defaultHandler.connectRPC() ctx, cancel := context.WithTimeout(context.Background(), timeout) defer cancel() resp, err := defaultHandler.client.QueryBasicInfoByID(ctx, in) defaultHandler.lastGrpcReqError = err return resp, err } 抛开本意不谈，这样的写法也是不OK的。。。因为\nif h.client != nil \u0026amp;\u0026amp; h.lastGrpcReqError != nil { // 经过初始化后的client始终不等于nil, 那么控制grpc.Dail的就只剩下了err, // 而err的取值是所有的方法的报错，而不只是连接异常。 // } 那么修复工作只需要 去除掉这“简陋破烂”的重连 就行了：注释掉所有跟connectRPC和lastGrpcReqError相关的代码行。\n这里我还做了一个简单的测试，如下：\nconst N = 100 func Test_clientErr(t *testing.T) { profileWriter, _ = os.OpenFile(\u0026#34;./profile.normal.out\u0026#34;, os.O_CREATE|os.O_WRONLY, 0644) heapWriter, _ = os.OpenFile(\u0026#34;./memprofile.normal.out\u0026#34;, os.O_CREATE|os.O_WRONLY, 0644) pprof.StartCPUProfile(profileWriter) defer pprof.StopCPUProfile() for i := 0; i \u0026lt; N; i++ { // Normal err = nil resp, err := QueryBasicInfoByID(\u0026amp;usrpb.ByIDForm{UserId: 1}) // 会触发 err = NotFound // resp, err := QueryBasicInfoByID(\u0026amp;usrpb.ByIDForm{UserId: 11}) t.Log(resp, err) } runtime.GC() if err := pprof.WriteHeapProfile(heapWriter); err != nil { t.Error(err) } } // 未修复前，100个NotFound错误 // Showing nodes accounting for 8040.19kB, 100% of 8040.19kB total // Showing top 10 nodes out of 26 // flat flat% sum% cum cum% // 5446.66kB 67.74% 67.74% 5446.66kB 67.74% google.golang.org/grpc/internal/transport.newBufWriter // 1056.33kB 13.14% 80.88% 1056.33kB 13.14% bufio.NewReaderSize // 513kB 6.38% 87.26% 513kB 6.38% golang.org/x/net/http2/hpack.newInternalNode // 512.19kB 6.37% 93.63% 512.19kB 6.37% runtime.malg // 512.01kB 6.37% 100% 1025.01kB 12.75% golang.org/x/net/http2/hpack.addDecoderNode // 0 0% 100% 1025.01kB 12.75% golang.org/x/net/http2.(*Framer).ReadFrame // 0 0% 100% 1025.01kB 12.75% golang.org/x/net/http2.(*Framer).readMetaFrame // 0 0% 100% 1025.01kB 12.75% golang.org/x/net/http2/hpack.(*Decoder).Write // 0 0% 100% 1025.01kB 12.75% golang.org/x/net/http2/hpack.(*Decoder).parseFieldLiteral // 0 0% 100% 1025.01kB 12.75% golang.org/x/net/http2/hpack.(*Decoder).parseHeaderFieldRepr // 未修复前 100 normal // Showing nodes accounting for 2208.36kB, 100% of 2208.36kB total // Showing top 10 nodes out of 19 // flat flat% sum% cum cum% // 1184.27kB 53.63% 53.63% 1184.27kB 53.63% runtime/pprof.StartCPUProfile // 512.08kB 23.19% 76.82% 512.08kB 23.19% golang.org/x/net/http2.init // 512.01kB 23.18% 100% 512.01kB 23.18% golang.org/x/net/http2/hpack.addDecoderNode // 0 0% 100% 1184.27kB 53.63% git.class100.com/backend/protobuf/benchmark_test.Test_CallUsergRPC_Normal // 0 0% 100% 512.01kB 23.18% golang.org/x/net/http2.(*Framer).ReadFrame // 0 0% 100% 512.01kB 23.18% golang.org/x/net/http2.(*Framer).readMetaFrame // 0 0% 100% 512.01kB 23.18% golang.org/x/net/http2/hpack.(*Decoder).Write // 0 0% 100% 512.01kB 23.18% golang.org/x/net/http2/hpack.(*Decoder).parseFieldLiteral // 0 0% 100% 512.01kB 23.18% golang.org/x/net/http2/hpack.(*Decoder).parseHeaderFieldRepr // 0 0% 100% 512.01kB 23.18% golang.org/x/net/http2/hpack.(*Decoder).readString // 修复后，100个NotFound错误 // Showing nodes accounting for 1537.02kB, 100% of 1537.02kB total // Showing top 10 nodes out of 14 // flat flat% sum% cum cum% // 1024.02kB 66.62% 66.62% 1537.02kB 100% golang.org/x/net/http2/hpack.addDecoderNode // 513kB 33.38% 100% 513kB 33.38% golang.org/x/net/http2/hpack.newInternalNode // 0 0% 100% 1537.02kB 100% golang.org/x/net/http2.(*Framer).ReadFrame // 0 0% 100% 1537.02kB 100% golang.org/x/net/http2.(*Framer).readMetaFrame // 0 0% 100% 1537.02kB 100% golang.org/x/net/http2/hpack.(*Decoder).Write // 0 0% 100% 1537.02kB 100% golang.org/x/net/http2/hpack.(*Decoder).parseFieldLiteral // 0 0% 100% 1537.02kB 100% golang.org/x/net/http2/hpack.(*Decoder).parseHeaderFieldRepr // 0 0% 100% 1537.02kB 100% golang.org/x/net/http2/hpack.(*Decoder).readString // 0 0% 100% 1537.02kB 100% golang.org/x/net/http2/hpack.buildRootHuffmanNode // 0 0% 100% 1537.02kB 100% golang.org/x/net/http2/hpack.getRootHuffmanNode 只是100个错误的触发就能看见内存占用有明显的上升，修复版本的内存占用也显著下降了。重新部署服务后，通过检查pprof，如下图：goroutine数量也正常(4000+ =\u0026gt; 68)了。\n从内存占用上看，也一直处于正常的范围（当然还需要长期观察）：\n代码分析总结 # 该问题一直存在该服务中，甚至影响到了其上游服务，但是由于种种原因：\n1. 迭代较快，服务更新频繁 2. lastGrpcReqError触发并不频繁 3. 内存超限设置较高，没有触发报警 4. 业务驱动。。。ORZ 一直得过且过，没有影响业务也就无所谓了，想起来一句话，“我们这体量，不要过早优化浪费时间，先把需求实现咯”。不过我觉的排查这种问题还是很有趣的，至少比写业务有趣。\n总结 # 😂pprof我用得也不够熟练，这里姑且算是一次实践了 😂保留更多的罪证，方便水一篇文章，避免“口说无凭”（那我也想不到，我会写这种文章啊） 😂监控是个好东西，一定要监控好你的服务，并及时告警 😂pprof 也是个好东西，早点埋进你的服务里，避免到了要分析的时候却没有数据可用于分析 😂7个月太长了，以至于我都不想承认这是我写的 😂这里没有去分析gRPC的实现细节，因为我还没有开始看。。。先留个坑吧 "},{"id":31,"href":"/2020/01/08/ssh-tunnel%E5%B0%8F%E5%B7%A5%E5%85%B7/","title":"SSH Tunnel小工具","section":"Posts","content":" 背景 # 生产环境数据库不允许直接访问，但是又经常有需要直接操作数据库的需求😂。先不说合不合理，背景就是这个背景，因此只能通过跳板机来连接数据库，一（就）般（我）来（而）说（言）会使用ssh隧道，就轻松能解决这个问题，然鹅，事情并不简单。这里陈述一下：\n生产环境数据库不让直接访问； 跳板机上没有公钥，没有权限； 我一次可能需要开3+个隧道才能启动服务【敲重点】 解决 # 本着“我不造轮子，谁来造轮子”的想法，这里就造一个小轮子：用Go来实现SSH隧道多开，并支持配置。成果预览： 原理简要分析 # 如果代理原理有点了解，这里的原理差不多是一样的：Local \u0026lt;-\u0026gt; SSH tunnel \u0026lt;-\u0026gt; Remote Server，对于隧道来说把Local的请求传给Remote, 把Remote的响应告诉Local。直接上代码：\n// Start . // TODO: support random port by using localhost:0 func (tunnel *SSHTunnel) Start() error { listener, err := net.Listen(\u0026#34;tcp\u0026#34;, tunnel.LocalAddr) if err != nil { return err } defer listener.Close() // tunnel.Local.Port = listener.Addr().(*net.TCPAddr).Port for { conn, err := listener.Accept() if err != nil { return err } logger.Infof(tunnel.name() + \u0026#34; accepted connection\u0026#34;) go tunnel.forward(conn) } } // 创建隧道并传递消息，分别有两个端点，一个是本地隧道口，另一个是远程服务器上的隧道口 func (tunnel *SSHTunnel) forward(localConn net.Conn) { // 创建本地到跳板机的SSH连接 serverSSHClient, err := ssh.Dial(\u0026#34;tcp\u0026#34;, tunnel.ServerAddr, tunnel.SSHConfig) if err != nil { logger.Infof(tunnel.name()+\u0026#34; server dial error: %s\u0026#34;, err) return } logger.Infof(tunnel.name()+\u0026#34; connected to server=%s (1 of 2)\u0026#34;, tunnel.ServerAddr) // 创建跳板机到远程服务器的连接 remoteConn, err := serverSSHClient.Dial(\u0026#34;tcp\u0026#34;, tunnel.RemoteAddr) if err != nil { logger.Infof(tunnel.name()+\u0026#34; remote dial error: %s\u0026#34;, err) return } logger.Infof(tunnel.name()+\u0026#34; connected to remote=%s (2 of 2)\u0026#34;, tunnel.RemoteAddr) copyConn := func(writer, reader net.Conn) { _, err := io.Copy(writer, reader) if err != nil { logger.Infof(tunnel.name()+\u0026#34; io.Copy error: %s\u0026#34;, err) } } // local(w) =\u0026gt; 远程(r) go copyConn(localConn, remoteConn) // 远程(w) =\u0026gt; 本地(r) go copyConn(remoteConn, localConn) } 代码中需要注意的是：\nfunc forward() { // ... some code ignored // 下面的代码实现了消息传递，问题：为什么`io.Copy + net.Conn`就可以实现数据的持续传递呢？ copyConn := func(writer, reader net.Conn) { _, err := io.Copy(writer, reader) if err != nil { logger.Infof(tunnel.name()+\u0026#34; io.Copy error: %s\u0026#34;, err) } } go copyConn(localConn, remoteConn) go copyConn(remoteConn, localConn) } 总结 # 上述场景也可以通过写脚本的方式来解决，但毕竟每个平台对shell支持并不一致，还需要安装ssh工具，因此还是选择了造轮子。\nio.Copy会一直复制直到读到EOF；TCP协议只有在处理FIN报文才会写入EOF，因此io.Copy会持续不断的在local和remote之间复制数据。\n参考 # https://medium.com/@elliotchance/how-to-create-an-ssh-tunnel-in-go-b63722d682aa https://golang.org/x/crypto/ssh 如何在shell脚本中实现交互，请自行google 所有的代码均在：https://github.com/yeqown/infrastructure/tree/master/cmd/tunnel-helper "},{"id":32,"href":"/2019/12/12/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8Bhashtable/","title":"数据结构 - hashtable","section":"Posts","content":" 背景 # 最近一直在看《redis设计与实现》，其中讲了redis中使用到的数据结构如：sds, ziplist, skiplist, hashtable, intset, linkedlist等。读完第一部分之后，再结合github上的源码redis，本着好记性不如烂笔头的理念，便准备动手撸一遍。\nredis中hashtable的特点 # 链地址法解决hash冲突（除此以外，常见的冲突解决办法还有：再散列法/再哈希法/建立公共溢出区） 使用了murmur2哈希函数。 渐进式rehash，rehash过程并不是一步到位，而是在get/set/del 等操作中，穿插着完成。 自动扩容和自动收缩，通过阀值来控制扩容和收缩。 有2个bucket，其中0号bucket是最常用的，而1号只会在rehash过程中使用，一旦rehash完成，便不再使用。 解析和实现 # hashtable：是根据Key直接访问在内存存储位置的数据结构。如何根据key得到内存中的位置，就需要使用hash函数来从旁协助了。\nhash函数：是一种从任何一种数据中创建小的数字“指纹”的方法。简单的说：hash(input) = 1122334455。\n这里选择了golang来实现；murmur3 hash算法；\n数据结构 # 一图以蔽之：\n// 对外暴露的hashtable type LinkedDict struct { // 2个存储桶，0号正常使用，1号在rehash过程中使用；rehash完成之后，1号赋值给0号然后重置1号。 ht [2]*hashtable // 初始值 -1，表示没有在rehash rehashIdx int } // 存储桶 type hashtable struct { // 底层“数组” table []*dictEntry size int sizemask int used int } // hashtable 元素定义 type dictEntry struct { key string value interface{} next *dictEntry } 方法集 # hashtable常用的方法有 GET/SET/DELETE/ITER，接下来会在SET和DEL中介绍rehash的详细过程。\nSET # func (d *LinkedDict) Set(key string, value interface{}) { if d.ht[0].table == nil { d.ht[0].init(InitTableSize) } // isRehashing 判定：`rehashIdx != -1` // needRehash 判定: 装载因子 used / size \u0026gt; 1.0 时触发扩容rehash if !d.isRehashing() \u0026amp;\u0026amp; d.needRehash() { // rehashGrowup 表示本次rehash是需要扩容，在配置ht[1].table时会扩展为当前的2倍 // 反之则会缩减内存空间 // startrehash 会设置 rehashIdx = 0, 标志rehash的进度 d.startrehash(rehashGrowup) } if d.isRehashing() { // 如果在rehash过程中，则完成一部分任务: // 根据rehash的进度rehashIdx，选择搬移 d.ht[0].table[rehashIdx]部分的数据，添加到d.ht[1]中 d.steprehash() } // 上述工作完成之后，就可以考虑新增数据了 hashkey := d.hashkey(key) if d.isRehashing() { // 如果在rehash过程中，毋庸考虑，直接新增到d.ht[1]中 d.ht[1].insert(hashkey, newDictEntry(key, value, nil)) return } // 反之，不在rehash过程中，则直接新增到d.ht[0]中 d.ht[0].insert(hashkey, newDictEntry(key, value, nil)) return } // 渐进式rehash，根据rehashIdx确定，要搬移那一部分的数据。 func (d *LinkedDict) steprehash() { entry := d.ht[0].table[d.rehashIdx] // 如果rehashIdx指向的侧链为空，则rehashIdx自增，直到找到有数据的侧链或者数据均搬移完成 for entry == nil { d.rehashIdx++ if d.rehashIdx \u0026gt; d.ht[0].sizemask { d.finishrehash() return } entry = d.ht[0].table[d.rehashIdx] } // 开始搬移动作 // 遍历链表，将所有数据，新增到d.ht[1]中 next := entry.next for entry != nil { entry.next = nil d.ht[1].insert(d.hashkey(entry.key), entry) if next == nil { break } entry = next next = next.next } // 释放d.ht[0].table[rehashIdx]链表：避免干扰查询；释放内存 d.ht[0].table[d.rehashIdx] = nil d.rehashIdx++ if d.rehashIdx \u0026gt; d.ht[0].sizemask { // 是否已经结束，如果结束则： // d.ht[0] = d.ht[1] // d.ht[1] = newHashTable() // rehashIdx = -1 d.finishrehash() } } // 新增一个元素到到存储桶： // 根据hash函数的结果(hashkey)对存储桶大小(size)取模得到结果(pos)；ht.table[pos]完成对链表的新增工作。 func (ht *hashtable) insert(hashkey uint64, item *dictEntry) { pos := hashkey % uint64(ht.size) entry := ht.table[pos] last := entry if entry == nil { ht.used++ ht.table[pos] = item return } for entry != nil { if ht.keyCompare(entry.key, item.key) { // 如果key已经存在则覆盖旧值 entry.value = item.value return } last = entry entry = entry.next } ht.used++ last.next = item } 总结：\nrehashIdx 不仅用于标示hashtable是否在rehash过程中，也标示了rehash的进度； rehash过程中，新增元素直接新增到1号bucket中； 非rehash状态，则新增到0号bucket中； 侧链新增元素过程，需比较key值是否存在，如果存在则更新并返回； rehash过程中，rehashIdx不是只会增加1单位，而是根据侧链情况来更新； GET # func (d *LinkedDict) Get(key string) (v interface{}, ok bool) { // 同上SET，不过多赘述 if d.isRehashing() { d.steprehash() } hashkey := d.hashkey(key) v, ok = d.ht[0].lookup(hashkey, key) if !d.isRehashing() { // 如果不在rehash过程中 d.ht[0]中检索的结果便是最终结果 return } else if ok { // 如果在rehash过程中且命中，也返回结果 return v, ok } // 反之 rehash过程中，但在d.ht[0]中没找到，却不代表该key真的不存在, 还需要在d.ht[1]中确定 v, ok = d.ht[1].lookup(hashkey, key) return } 总结：\n渐进rehash过程与SET中一致 查询动作也需要根据rehash状而定：在rehash中则需要检查ht[0]和ht[1]；反之只需要检查rehash[0]即可； 这里省略了lookup部分的代码，是因为查询和新增在原理上是一致的：定位 -\u0026gt; 遍历检查 -\u0026gt; 比较key -\u0026gt; 动作。 DEL # // Del to delete an item in hashtable. func (d *LinkedDict) Del(key string) { if d.ht[0].used == 0 \u0026amp;\u0026amp; d.ht[1].used == 0 { return } // 这里相比Set，区别在于：判定的内容不是是否需要扩容而是缩容 // 缩容判定：d.ht[0]的内存空间大于初始值4且“填充率”少于 10% // d.ht[0].size \u0026gt; 4 \u0026amp;\u0026amp; (d.ht[0].used*100/d.ht[0].size) \u0026lt; 10 if !d.isRehashing() \u0026amp;\u0026amp; d.needShrink() { d.startrehash(rehashShrink) } if d.isRehashing() { d.steprehash() } hashkey := d.hashkey(key) d.ht[0].del(hashkey, key) if d.isRehashing() { d.ht[1].del(hashkey, key) } } 总结：\n万变不离其宗，不管是SET，GET，DEL 都是先定位，再确定元素位置，再执行相应的动作； 缩容判定中，填充率也等价于装载因子； 代码中有个取巧：当使用率为0时则直接返回，避免了后续调用～ ITER # 此部分代码略去； 遍历操作也需要视rehash情况而定； 测试 # 这里我使用了golang内置的Map做了对比测试，结果如下：\nbuiltinMap_1000 cost: 0ms builtinLinkedDict_1000 cost: 0ms getMap_1000 cost: 0ms getLinkedDict_1000 cost: 0ms builtinMap_10000 cost: 4ms builtinLinkedDict_10000 cost: 6ms getMap_10000 cost: 2ms getLinkedDict_10000 cost: 5ms builtinMap_100000 cost: 76ms builtinLinkedDict_100000 cost: 108ms getMap_100000 cost: 56ms getLinkedDict_100000 cost: 131ms builtinMap_1000000 cost: 1053ms builtinLinkedDict_1000000 cost: 1230ms getMap_1000000 cost: 581ms getLinkedDict_1000000 cost: 915ms builtinMap_10000000 cost: 13520ms builtinLinkedDict_10000000 cost: 17137ms getMap_10000000 cost: 8663ms getLinkedDict_10000000 cost: 14271ms 可见差距还是非常大的，这里大胆分析下导致这些差距的原因：\nKey类型，通过pprof分析，在hashtable.keyCompare上花费了较多时间，虽然已经通过strings.Compare来加速orz；相比下golang内置的Map使用了unsafe.Pointer**pointer to unsafe.ArbitraryType (int)**作为key，并针对不同的key类型来设计哈希算法。 bucket（数据结构）的使用：在我的实现中只使用了2个bucket，常用的只有1个bucket，在定位上：hash后的结果使用取模的方法定位；相比之下，map采用了多个bucket，每个bucket只存放8个元素，在定位上：hash后用low bits \u0026amp; bucketMask定位buckets和high 8bits找到对应的位置，效率更高； 总结 # 实现一个hashtable并不难，难点在于：hash算法的选用（均匀分布）；如何降低hash冲突（rehash时机）； 当完成上述工作的时候，我再去阅读go内置的map的实现会容易很多orz，仅相似部分；map比上述hashtable的实现要复杂得多； 文中所有代码均在 https://github.com/yeqown/hashtable/blob/master/hashtable.go； 如果只是想要了解原理，参考资料中的推荐文档足以； 已经实现的版本还可以继续优化，并考虑并发安全问题～ 参考资料 # https://en.wikipedia.org/wiki/Hash_table http://redisbook.com/ https://redisbook.readthedocs.io/en/latest/internal-datastruct/dict.html 推荐 https://github.com/cch123/golang-notes/blob/master/map.md 推荐 https://github.com/yeqown/hashtable "},{"id":33,"href":"/2019/12/04/%E5%9F%BA%E4%BA%8Esocket.io%E6%9E%84%E5%BB%BA%E5%8D%B3%E6%97%B6%E9%80%9A%E8%AE%AF%E7%B3%BB%E7%BB%9F/","title":"基于socket.io构建即时通讯系统","section":"Posts","content":" 本意在总结实现socket.io-app过程中的一些知识。\n背景 # 现需要替换公司的即时通信框架（之前用的是阿里云的微消息队列，优点在于：简单易接入，问题在于：对于人数和客户端状态感知不够准确，原因后面细说）。在框架选型的时候，基于应用场景（客户端有：小程序/nodeJS/浏览器），有三种方案：\n替换MQTT的架子，针对现有场景下的问题，选用一款更加可控的MQTT服务，如EMQX。 基于现有的技术栈，选择一款golang开发的开源框架，在此基础上进行开发，如goim。 一个大众且稳定的开源框架，语言不限，如socket.io。 综合一系列因素（技术熟悉程度/star数/时间成本/金钱成本/运维成本）选择了socket.io / JS🐶～\n这里想推荐一下goim给golang程序猿。个人看法：goim对团队技术栈更友好；对分布式更友好；架构合理易于扩展；如有兴趣可以去看下：https://juejin.im/post/5cbb9e68e51d456e51614aab @tsingson 的解析，结合源码更容易理解上手。\n关于socket.io的介绍 # 文档在此socket.io\n通信机制及特点 # 特点 摘要 可用性 提供 long-polling 和 WebSocket 两种方式，可以自动升级，基于engine.io 自动重连 客户端会一直重连，直到再次链接上服务器 断线检测 客户端和服务端通过心跳维持长链接 二进制消息 任意被序列化的数据结构都可以传输 多路复用 对不同的Namespace,复用底层链接 内置Room概念 理解为聊天室和即可 基于websocket，但不互通 https://github.com/socketio/socket.io-protocol 总结一下：socket.io 已经提供了即时通讯必要的基础，不用关心任何通讯相关的细节，开箱即用。\n分布式 # 确认socket.io已经提供了通讯基础，那么问题来了：单机性能有限，如何扩展到分布式呢? socket.io的另一个优点adapter机制让socket.io易于扩展，官方提供了redis-adapter来支持消息分发。\nBy running socket.io with the socket.io-redis adapter you can run multiple socket.io instances in different processes or servers that can all broadcast and emit events to and from each other.\n注意：redis-adapter会同步所有的广播和emit事件\nconst io = require(\u0026#39;socket.io\u0026#39;)(3000); const redisAdapter = require(\u0026#39;socket.io-redis\u0026#39;); io.adapter(redisAdapter({ host: \u0026#39;localhost\u0026#39;, port: 6379 })); 事件机制 # 如下：\nvar io = require(\u0026#39;socket.io\u0026#39;)(80); var chat = io .of(\u0026#39;/chat\u0026#39;) .on(\u0026#39;connection\u0026#39;, function (socket) { socket.emit(\u0026#39;event1\u0026#39;, { that: \u0026#39;only\u0026#39; , \u0026#39;/chat\u0026#39;: \u0026#39;will get\u0026#39; }); chat.emit(\u0026#39;event2\u0026#39;, { everyone: \u0026#39;in\u0026#39; , \u0026#39;/chat\u0026#39;: \u0026#39;will get\u0026#39; }); }); var news = io .of(\u0026#39;/news\u0026#39;) .on(\u0026#39;connection\u0026#39;, function (socket) { socket.emit(\u0026#39;item\u0026#39;, { news: \u0026#39;item\u0026#39; }); }); 除了内置保留的事件（connect message disconnect error ping pong等）外，可以自定义任何事件名，用于业务逻辑。\n这里只是简单介绍了一下socket.io，可以前往官网获取更多细节。\n事件机制对与客户端状态和人数的作用 # 前面说过使用阿里云微消息队列无法准确获取到客户端状态和数量的问题，在阿里云文档中提到了：异步上下线通知因为采用消息解耦，状态判断更加复杂，且误判可能性更大，但该方法可以基于事件分析多个客户端的运行状态轨迹。异步上下线正是之前用于统计在线人数和客户端状态选择的方案。在实际使用的时候，能很直观的发现，上下线消息不能保证正确的顺序，这也导致了计算人数和状态时候出错～\n基于socket-io的APP设计 # 基于业务场景，要求app的功能有：\n1. 客户端和服务端全双工方式通信 2. 同一Namesapce下，相同用户ID仅能存在一个 3. 服务端正常触发上线/下线，用于维系客户端状态 4. 提供RPC调用方式，用于业务服务器与客户端交互 5. 提供客户端鉴权，非法客户端和未鉴权客户端会被主动关闭连接 6. Namespace可动态创建 7. 要求一定的监控数据，如链接数量，房间数量，消息数量等等。 应用对接 # 客户端生存周期泳道图 # 实现过程中的问题 # socket.io是对长连接服务的，在分布式服务的场景下，虽然有redis-adapter可以方便的做消息分发到不同的服务实例上去，如果不是emit相关的事件则不支持分布式。下面列举了一部分关于分布式时需要解决的问题：\nSticky Session # 因为socket.io同时支持了long-polling和websocket，并自带自动升级功能。因此存在升级过程中，存在HTTP协议升级时，链接到不同服务器，导致无法完成升级过程。解决办法有两种：\n使用nginx / ip_hash或类似功能，如果有自己的LB或者网关，可以自定义这个规则，保证负载均衡的同时保证同一连接接在同一服务器上完成升级过程。 配置 transports 为仅 websocket，规避升级过程。 var io = require(\u0026#39;socket.io-client\u0026#39;) var client = io(\u0026#39;https://myhost.com\u0026#39;, {transports: [\u0026#39;websocket\u0026#39;]}) https://socket.io/docs/internals/#socket-io-client\nRPC调用如何扩展到分布式 # 需RPC提供的功能有踢人，断开与客户端的连接，清空房间之类。这一类命令不会调用emit / broadcast，因此也不能通过redis-adapter来实现分发。这里可以借鉴redis-adapter消息分发的思想，通过 redis PUB/SUB 来分发RPC命令，由此扩展到分布式。举例如下:\ninterface IRpcCommand { evt: rpcCommandEvt meta: any } class grpcService { /* * 订阅redis channel，并分发命令 */ private _subscribeCommandsToRedis() { this._sub.on(\u0026#34;message\u0026#34;, (ch: string, message: string) =\u0026gt; { logger.info(\u0026#34;recv an command from: \u0026#34;, ch, message) try { let command: IRpcCommand = JSON.parse(message) this._call(command) } catch (error) { logger.error(\u0026#34;could not execute message command: \u0026#34;, error) } }) this._sub.SUBSCRIBE(gRPCService.pubsubTopic()) } private _call(command: IRpcCommand) { switch (command.evt) { /* ignore other cases */ case rpcCommandEvt.clearRooms: let d6: clearRoomMeta = command.meta as clearRoomMeta this._clearRooms(d6.nspName, d6.roomIds) break default: logger.error(\u0026#34;undefined command evt\u0026#34;) } } /* ignore some codes */ private _clearRooms = (nspName: string, roomIds: string[]) =\u0026gt; { this._socketioSrv.clearRooms(nspName, roomIds) } /** * clearRoom */ clearRooms = (call: grpc.ServerUnaryCall\u0026lt;api_pb.ClearRoomsReq\u0026gt;, cb: grpc.requestCallback\u0026lt;api_pb.ClearRoomsResp\u0026gt;) =\u0026gt; { let roomIds = call.request.getRoomidsList() let nspName = call.request.getNspname() let resp = new api_pb.ClearRoomsResp() let meta: clearRoomMeta = { nspName, roomIds } this._publishCommandsToRedis(rpcCommandEvt.clearRooms, meta) resp.setErrcode(codes.OK) resp.setErrmsg(getMessage(codes.OK)) cb(null, resp) } } Namespace下唯一用户ID如何保证 # 这个需求简单的就是说，业务逻辑上想对客户端加以限制，不能让同一个用户开启多个客户端来交互。逻辑很简单：每个连接鉴权的时候，去查询是否该用户ID已经存在Session了，如果存在则把之前的连接断开。同样的在分布式场景下：用户A分配到了S1服务器，用户A在其他地方S2服务器又建立一个新的连接，那么S2如何把S1上的连接断开呢？\n说过上面RPC的命令分发之后，这里简单了：只需要触发一次RPC的disconnect事件就行了。\nNodeJS + gRPC + TypeScript 实战 # 😭😭😭先哭为敬！想起来就心酸。核心问题是： 1. 用什么工具给proto文件生成ts文件；2. ts 中如何使用gRPC；3. ts 配置问题；其他；这里直接祭出最终结果：\n生成工具 # # generate js code for grpc proto file gen-js: ./node_modules/.bin/grpc_tools_node_protoc \\ --js_out=import_style=commonjs,binary:./src/codegen/api/ \\ --grpc_out=./src/codegen/api/ \\ --plugin=protoc-gen-grpc=./node_modules/.bin/grpc_tools_node_protoc_plugin \\ --proto_path=./src/api/ \\ api.proto # generate ts code for grpc messages and service gen-ts: protoc \\ --plugin=protoc-gen-ts=./node_modules/.bin/protoc-gen-ts \\ --ts_out=./src/codegen/api \\ --proto_path=./src/api/ \\ api.proto ts中grpc.Server绑定service # // https://github.com/yeqown/socket.io-app/blob/master/src/server/grpc.ts serve = () =\u0026gt; { // const _protoPath = \u0026#39;../api/api.proto\u0026#39; // const _protoDescriptor: GrpcObject = grpc.load(_protoPath); // const service = ; let _srv = new grpc.Server() _srv.addService(grpc_pb.SocketMServiceService, { nspBroadcast: this.nspBroadcast, nspRoomsBroadcast: this.nspRoomsBroadcast, nspUsersBroadcast: this.nspUsersBroadcast, disconnect: this.disconnect, knockoutFromRoom: this.knockoutFromRoom, clearRooms: this.clearRooms, }) /* ignore some codes */ _srv.bind(\u0026#34;0.0.0.0:\u0026#34; + this.port.toString(), grpc.ServerCredentials.createInsecure()) _srv.start() } 总结 # socket.io 已经很全面了，在此基础上只需要接入自己的业务逻辑就可以了。 redis 真好用 TS 真好用，只是部分工具跟不上～ 开源了一个基于socket.io的app架子，socket.io-app，欢迎交流 redis-adapter 会主动分发emit, broadcast指令 socket.io的协议升级期间需要保证在同一台服务器上完成 水平有限，如有错误，欢迎勘误指正🙏。\n参考资料 # https://socket.io/docs https://github.com/yeqown/socket.io-app "},{"id":34,"href":"/2019/11/14/%E5%9F%BA%E4%BA%8ERepository%E8%AE%BE%E8%AE%A1%E7%BC%93%E5%AD%98%E6%96%B9%E6%A1%88/","title":"基于Repository设计缓存方案","section":"Posts","content":" 场景 # Tester—A：这个 getInfo 接口咋这么慢呢？查一下要5+s？QPS竟然只有10！！！！ RD-B ：这是因为getInfo要查库。。。N多库 Tester-B：那优化一下呗？ RD-B ：好的，容我操作一波（给接口加上一个响应缓存），好了你再测试一下 Tester-B：（测试中。。。），速度果然快了不少。诶不对，这个接口里拿到的用户信息不对，我明明已经balaba了，这里没有更新！！！ RD-B ：哦哦哦，我晓得咯，再容我操作一波（缓存加有效时间，个人信息更新的时候再强删缓存），O了 至此开始了针对于QPS+缓存更新的一些列测试。。。剧终。 QPS和响应时间是后（jie）端（kou)工程师非常熟悉的指标，这两个值能比较直观的反映该接口的性能，间接直接影响了前端页面的流畅度。。。\n问题来了 # 接口查询性能如何提高 # 除去机器和编程语言的因素之后，肯定要从业务场景出发，分析接口响应缓慢的原因。譬如，最常见的:\n查N多表，表还没有索引orz 无用数据，增加传输的Size 反复查询某些热点数据，但每次都直接打到数据库 上游服务响应缓慢 其他 好了，这里只讨论热点数据的缓存方案，毕竟要具体场景具体分析，而缓存方案是比较通用的。\n缓存方案如何选择 # 序号 缓存方案 优势 劣势 1 Response缓存 简单暴力 缓存更新时机不好把控，如果面面俱到可能心态崩坏；缓存粒度太大，无法局部更新；针对查询接口有帮助，其他业务下查询数据则毫无帮助 2 Repository缓存 粒度由Repo自行掌握，可控性强；Repo复用场景下会提高应用整体的速度 需要针对各个Repo做缓存的处理；改动较多；其他orz 总的来说，Repository的缓存方案，在上述背景上较简单暴力的中间件缓存法要更加优雅可控～。\n缓存算法 # 提到缓存就一定会提到缓存替换策略，有最常见的：LRU LFU FIFO MRU(最近频繁使用算法) LRU的多个变种算法 LIRS等。 这里选用了LRU-K（K=2）并基于golang来实现 cached-repository，更多算法的详细信息参见参考文档中的LRU和LRU-K:\n这里分成了两个interface:\nCacheAlgor重点在于与Repo交互，所以只提供了简单的增删改查，底层还是基于Cache来实现的。本意是想实现多种缓存替换算法来丰富cached-repository，orz\n// cache.go // CacheAlgor is an interface implements different alg. type CacheAlgor interface { Put(key, value interface{}) Get(key interface{}) (value interface{}, ok bool) Update(key, value interface{}) Delete(key interface{}) } lru.Cache 在于提供 基于LRU-like算法缓存和替换能力，所以接口会更丰富一些，\n// lru/types.go // Cache is the interface for simple LRU cache. type Cache interface { // Puts a value to the cache, returns true if an eviction occurred and // updates the \u0026#34;recently used\u0026#34;-ness of the key. Put(key, value interface{}) bool // Returns key\u0026#39;s value from the cache and // updates the \u0026#34;recently used\u0026#34;-ness of the key. #value, isFound Get(key interface{}) (value interface{}, ok bool) // Removes a key from the cache. Remove(key interface{}) bool // Peeks a key // Returns key\u0026#39;s value without updating the \u0026#34;recently used\u0026#34;-ness of the key. Peek(key interface{}) (value interface{}, ok bool) // Returns the oldest entry from the cache. #key, value, isFound Oldest() (interface{}, interface{}, bool) // Returns a slice of the keys in the cache, from oldest to newest. Keys() []interface{} // Returns the number of items in the cache. Len() int // iter all key and items in cache Iter(f IterFunc) // Clears all cache entries. Purge() } 关于如何实现LRU或者LRU-K，网上已经有很多文章了，原理也不复杂，这里就不过多赘述了，直接上测试结果\n简单测试 # 完整代码参见code\n// MysqlRepo . type MysqlRepo struct { db *gorm.DB calg cp.CacheAlgor // *cp.EmbedRepo } // NewMysqlRepo . func NewMysqlRepo(db *gorm.DB) (*MysqlRepo, error) { // func NewLRUK(k, size, hSize uint, onEvict EvictCallback) (*K, error) c, err := lru.NewLRUK(2, 10, 20, func(k, v interface{}) { fmt.Printf(\u0026#34;key: %v, value: %v\\n\u0026#34;, k, v) }) if err != nil { return nil, err } return \u0026amp;MysqlRepo{ db: db, // func New(c lru.Cache) CacheAlgor calg: cp.New(c), }, nil } // GetByID . func (repo MysqlRepo) GetByID(id uint) (*userModel, error) { start := time.Now() defer func() { fmt.Printf(\u0026#34;this queryid=%d cost: %d ns\\n\u0026#34;,id, time.Now().Sub(start).Nanoseconds()) }() v, ok := repo.calg.Get(id) if ok { return v.(*userModel), nil } // actual find in DB m := new(userModel) if err := repo.db.Where(\u0026#34;id = ?\u0026#34;, id).First(m).Error; err != nil { return nil, err } repo.calg.Put(id, m) return m, nil } // Update . func (repo MysqlRepo) Update(id uint, m *userModel) error { if err := repo.db.Where(\u0026#34;id = ?\u0026#34;, id).Update(m).Error; err != nil { return err } fmt.Printf(\u0026#34;before: %v\\n\u0026#34;, m) m.ID = id if err := repo.db.First(m); err != nil { } fmt.Printf(\u0026#34;after: %v\\n\u0026#34;, m) // update cache, ifcache hit id repo.calg.Put(id, m) return nil } // Delete . func (repo MysqlRepo) Delete(id uint) error { if err := repo.db.Delete(nil, \u0026#34;id = ?\u0026#34;, id).Error; err != nil { return err } repo.calg.Delete(id) return nil } func main() { // ... prepare data rand.Seed(time.Now().UnixNano()) for i := 0; i \u0026lt; 1000; i++ { go func() { wg.Add(1) id := uint(rand.Intn(10)) if id == 0 { continue } v, err := repo.GetByID(id) if err != nil { fmt.Printf(\u0026#34;err: %d , %v\\n\u0026#34;, id, err) continue } if v.ID != id || v.Name != fmt.Sprintf(\u0026#34;name-%d\u0026#34;, id) || v.Province != fmt.Sprintf(\u0026#34;province-%d\u0026#34;, id) || v.City != fmt.Sprintf(\u0026#34;city-%d\u0026#34;, id) { fmt.Printf(\u0026#34;err: not matched target with id[%d]: %v\\n\u0026#34;, v.ID, v) } wg.Done() }() } wg.Wait() } ➜ custom-cache-manage git:(master) ✗ go run main.go this queryid=9 cost: 245505 ns this queryid=1 cost: 131838 ns this queryid=3 cost: 128272 ns this queryid=2 cost: 112281 ns this queryid=7 cost: 123942 ns this queryid=4 cost: 140267 ns this queryid=7 cost: 148814 ns this queryid=9 cost: 126904 ns this queryid=6 cost: 129676 ns this queryid=2 cost: 174202 ns this queryid=1 cost: 151673 ns this queryid=4 cost: 156370 ns this queryid=3 cost: 159285 ns this queryid=6 cost: 142215 ns this queryid=3 cost: 691 ns this queryid=1 cost: 450 ns this queryid=8 cost: 160263 ns this queryid=5 cost: 149655 ns this queryid=4 cost: 756 ns this queryid=8 cost: 143363 ns this queryid=3 cost: 740 ns this queryid=9 cost: 558 ns this queryid=2 cost: 476 ns this queryid=5 cost: 184098 ns this queryid=1 cost: 824 ns this queryid=8 cost: 556 ns this queryid=9 cost: 632 ns this queryid=7 cost: 480 ns this queryid=5 cost: 439 ns this queryid=5 cost: 409 ns this queryid=7 cost: 431 ns this queryid=6 cost: 479 ns this queryid=4 cost: 423 ns this queryid=8 cost: 423 ns this queryid=1 cost: 411 ns this queryid=6 cost: 423 ns this queryid=8 cost: 394 ns this queryid=7 cost: 410 ns this queryid=9 cost: 424 ns this queryid=4 cost: 428 ns this queryid=2 cost: 433 ns this queryid=4 cost: 420 ns this queryid=9 cost: 424 ns this queryid=6 cost: 406 ns this queryid=6 cost: 399 ns this queryid=5 cost: 405 ns this queryid=2 cost: 428 ns this queryid=9 cost: 383 ns this queryid=4 cost: 399 ns this queryid=7 cost: 413 ns this queryid=4 cost: 381 ns this queryid=1 cost: 427 ns this queryid=2 cost: 430 ns this queryid=1 cost: 468 ns this queryid=1 cost: 406 ns this queryid=4 cost: 380 ns this queryid=2 cost: 360 ns this queryid=3 cost: 660 ns this queryid=6 cost: 393 ns this queryid=5 cost: 419 ns this queryid=7 cost: 1254 ns this queryid=6 cost: 723 ns this queryid=4 cost: 503 ns this queryid=8 cost: 448 ns this queryid=3 cost: 510 ns this queryid=1 cost: 432 ns this queryid=2 cost: 999 ns this queryid=1 cost: 419 ns this queryid=8 cost: 658 ns this queryid=9 cost: 1322 ns this queryid=9 cost: 543 ns this queryid=4 cost: 1311 ns this queryid=5 cost: 348 ns this queryid=4 cost: 309 ns this queryid=5 cost: 350 ns this queryid=9 cost: 311 ns this queryid=5 cost: 336 ns this queryid=3 cost: 567 ns this queryid=9 cost: 293 ns this queryid=7 cost: 338 ns this queryid=4 cost: 499 ns this queryid=7 cost: 318 ns this queryid=3 cost: 330 ns this queryid=7 cost: 322 ns this queryid=6 cost: 339 ns this queryid=7 cost: 1273 ns this queryid=4 cost: 1175 ns this queryid=6 cost: 306 ns this queryid=2 cost: 316 ns this queryid=5 cost: 330 ns this queryid=5 cost: 322 ns this queryid=6 cost: 324 ns this queryid=8 cost: 291 ns this queryid=2 cost: 310 ns this queryid=3 cost: 321 ns this queryid=3 cost: 294 ns this queryid=6 cost: 293 ns this queryid=8 cost: 3566 ns ...more ignored 水平有限，如有错误，欢迎勘误指正🙏。\n代码 # github.com/yeqown/cached-repository\n参考 # LIRS LRU-k "},{"id":35,"href":"/2019/10/10/AMQP%E9%87%8D%E8%BF%9E%E6%9C%BA%E5%88%B6%E5%AE%9E%E7%8E%B0-Go/","title":"AMQP重连机制实现","section":"Posts","content":" 文中代码基于 https://github.com/streadway/amqp 实现。此方式简单暴力，但没有做到最小成本迁移（可以选择分别包装Producer和Consumer）。\n文中所有代码参见：https://github.com/yeqown/infrastructure/tree/master/framework/amqp\n基本知识 # AMQP # AMQP，即Advanced Message Queuing Protocol,一个提供统一消息服务的应用层标准高级消息队列协议，是应用层协议的一个开放标准,为面向消息的中间件设计。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件同产品，不同的开发语言等条件的限制。用下图简单描述下AMQP模型：\n背景 # -问题 # 生产环境中使用了RabbitMQ做异步消息分发，隔一段时间会出现：发送接口报错；发送成功后未被消费等情况。重启服务后恢复。\n-问题代码 # 生产者：\n// producer.go // NewClient . func NewClient(cfg *types.RabbitMQConfig) *Client { // init MQ connection return \u0026amp;Client{ ch: ch, // *amqp.Channel cfg: cfg, } } // Client . type Client struct { ch *amqp.Channel cfg *types.RabbitMQConfig } // Send . send by routing func (c *Client) Send(payload *types.Payload) error { var routing string switch payload.Typ { case types.PayloadTypUsers: routing = c.cfg.MqRoutingUser default: routing = c.cfg.MqRoutingSys } dat, _ := json.Marshal(payload) publishing := amqp.Publishing{ ContentType: \u0026#34;text/plain\u0026#34;, Body: dat, } return c.ch.Publish(c.cfg.MqExchagne, routing, false, false, publishing) } 消费者：\n// consumer.go // ... func (c *Client) Consume() error { err := c.initRabbitmqResource() if err != nil { logger.Std.Errorf(\u0026#34;could init rabbitmq resource: %v\u0026#34;, err) return err } msgs, err := c.rabbitmqChannel.Consume( c.rabbitmqQ, // queue \u0026#34;\u0026#34;, // consumer false, // auto ack false, // exclusive false, // no local false, // no wait nil, // args ) if err != nil { logger.Std.Errorf(\u0026#34;could not consume messages: %v\u0026#34;, err) return err } logger.Std.Infof(\u0026#34;start consumming messages\u0026#34;) for d := range msgs { logger.Std.Debugf(\u0026#34;consumming msg: body=%s, ex=%s, routing=%s, content-type=%s\u0026#34;, d.Body, d.Exchange, d.RoutingKey, d.ContentType) if err := c.sendout(d.Body); err != nil { continue } if err := d.Ack(false); err != nil { logger.Std.Errorf(\u0026#34;ack message error: %v\u0026#34;, err) } } // 队列被删除时会被执行到 logger.Std.Infof(\u0026#34;this should not be executed !!!!!\u0026#34;) return nil } // ... -分析问题 # 服务故障后，首先查看了生产者的系统日志，发现了 channel/connection is not open。从github中检索后发现https://github.com/streadway/amqp/blob/75d898a42a/channel.go#L155 有一个逻辑是处理已经关闭的channel, 会触发ErrClosed = \u0026amp;Error{Code: ChannelError, Reason: \u0026quot;channel/connection is not open\u0026quot;}，恰恰channel被关闭是上述代码没有考虑到的问题。\n在消费者这边的问题，在channel被关闭的情况下，就更显眼了：channel被关闭,消费者也就被关闭了，消费的goroutine自然也退出了，也就出现了消息无法被消费的情况。具体代码参见：\nchannel.Consume https://github.com/streadway/amqp/blob/75d898a42a/channel.go#L1049 consumers.buffer https://github.com/streadway/amqp/blob/75d898a42a/consumers.go#L54 如果还有兴趣可以翻看下connection.shutdown, channel.shutdown。\nfunc (subs *consumers) buffer(in chan *Delivery, out chan Delivery) { // 退出时关闭消费channel defer close(out) defer subs.Done() var inflight = in var queue []*Delivery for delivery := range in { queue = append(queue, delivery) for len(queue) \u0026gt; 0 { select { // 很明显，协程会在消费者被关闭的时候退出 case \u0026lt;-subs.closed: // closed before drained, drop in-flight return case delivery, consuming := \u0026lt;-inflight: if consuming { queue = append(queue, delivery) } else { inflight = nil } case out \u0026lt;- *queue[0]: queue = queue[1:] } } } } 而为什么channel会被关闭，在网上检索后并结合代码：\nSeq Description Reason Type 1 应用程序主动断开连接，网络异常被动断开连接 连接中断 连接关闭 2 重新声明现有队列或具有不匹配属性的交换将失败 406 PRECONDITION_FAILED 协议异常 3 访问不允许用户访问的资源将失败 403 ACCESS_REFUSED错误 协议异常 4 绑定不存在的队列或不存在的交换将失败 404 NOT_FOUND错误 协议异常 5 从不存在的队列中使用将失败 404 NOT_FOUND错误 协议异常 6 发布到不退出的交换将失败 404 NOT_FOUND错误 协议异常 7 从除声明连接以外的连接访问独占队列将失败 405 RESOURCE_LOCKED 协议异常 得到结论：由于网络波动、生产者消费者使用不当会导致channel被关闭.\n重连原理 # 参考：\nhttps://ms2008.github.io/2019/06/16/golang-rabbitmq/ https://github.com/streadway/amqp/issues/133 简而言之，channel.NotifyClose和connection.NotifyClose可以接收到错误消息，那就以此为重连的触发器。函数原型：func NotifyClose(c chan *Error) chan *Error。实现思路如图：\n结合***CODE***食用，效果更佳哦。handleReconnect代码如下：\nfunc (w *Wrapper) handleReconnect() { for { // 未连接上的情况下一直尝试连接 if !w.isConnected { log.Println(\u0026#34;Attempting to connect\u0026#34;) var ( connected = false err error ) for cnt := 0; !connected; cnt++ { // connect 尝试连接，并在成功后触发changeConn事件，也会更新isConnected if connected, err = w.connect(); err != nil { log.Printf(\u0026#34;Failed to connect: %s.\\n\u0026#34;, err) } if !connected { log.Printf(\u0026#34;Retrying... %d\\n\u0026#34;, cnt) } time.Sleep(reconnectDelay) } } // 连接正常的情况下，处理channel.NotifyClose 和 conn.NotifyClose // 注：在任意一个notify事件通知后，应完全处理两个channel中的消息，否则会造成无缓冲阻塞。 // https://ms2008.github.io/2019/06/16/golang-rabbitmq/ select { case \u0026lt;-w.done: println(\u0026#34;w.done\u0026#34;) return case err := \u0026lt;-w.chNotify: log.Printf(\u0026#34;channel close notify: %v\u0026#34;, err) w.isConnected = false case err := \u0026lt;-w.connNotify: log.Printf(\u0026#34;conn close notify: %v\u0026#34;, err) w.isConnected = false } time.Sleep(reconnectDetectDur) } } 以上。\n水平有限，如有错误，欢迎勘误指正🙏。\n参考 # https://github.com/streadway/amqp/issues/133 https://blog.csdn.net/jaredcoding/article/details/78656636 https://www.cnblogs.com/frankyou/p/5283539.html https://ms2008.github.io/2019/06/16/golang-rabbitmq/【必读】 "},{"id":36,"href":"/2019/09/21/TCP%E6%8B%86%E5%8C%85%E7%B2%98%E5%8C%85/","title":"TCP拆包粘包","section":"Posts","content":" 一些名词 # MTU（Maximum Transmission Unit） # the maximum transmission unit (MTU) is the size of the largest protocol data unit (PDU) that can be communicated in a single network layer transaction. ——from wiki\nMTU 物理接口（数据链路层）提供给其上层（通常是IP层）最大一次传输数据的大小。一般来说MTU=1500byte。如果MSS + TCP首部 + IP首部 \u0026gt; MTU，那么IP报文就会存在分片，否则就不需要分片。\nMSS (Maximum Segment Size) # The maximum segment size (MSS) is a parameter of the options field of the TCP header that specifies the largest amount of data, specified in bytes, that a computer or communications device can receive in a single TCP segment. ——from wiki\nMSS是TCP提交给IP层最大分段大小，只包含payload，不包括TCP头部长度。MSS由TCP握手过程中由双方协商得出，其中SYN字段中的选项部分包括了这个信息。一般来说，MSS = MTU - IP首部大小 - TCP首部大小\n为什么会出现“拆包”？ # “拆包”是无法一次发送所有数据。\n如果应用层需要发送的字节数超过了MSS，就需要发送多个TCP报文才能完所有应用数据的发送。\n为什么会出现“粘包”？ # “粘包”是在一次接收数据不能完全地体现一个完整的消息数据。\n1.Nagle算法所致（Nagle算法是一种改善网络传输效率的算法）简单的说，当我们提交一段数据给TCP发送时，TCP并不立刻发送此段数据，而是等待一小段时间，看看在等待期间是否还有要发送的数据，若有则会一次把这两段数据发送出去。\n2.接收端接收不及时造成的接收端粘包：TCP会把接收到的数据存在自己的缓冲区中，然后通知应用层取数据.当应用层由于某些原因不能及时的把TCP的数据取出来，就会造成TCP缓冲区中存放了几段数据。\n总结一下：\nTCP是字节流协议，没有记录边界，TCP也不理解流所携带的数据内容。因此需要应用层自己去定义“有序的，结构化的数据信息”。\n如何解决呢？ # 现在知道了以上两种情况是由于TCP是字节流的协议，不关心数据结构和边界，所以这一部分工作是要交给应用层自己处理的。下面就先看下应用层中协议和应用是怎么处理的。\nHTTP # HTTP是一个文本协议，用\\r\\n\\r\\n来区分消息头和消息体，在消息头中有Content-Length来说明消息体有多大，如果没有该字段就说明没有消息体。\n二进制协议 # 通过消息固定长度的字节表示消息的总长度。Talk is cheap, show me your Code:\n// Package proto . package proto import ( \u0026#34;bufio\u0026#34; \u0026#34;encoding/binary\u0026#34; \u0026#34;errors\u0026#34; ) const ( // OpRequest . OpRequest uint16 = iota + 1 // OpResponse . OpResponse ) const ( // Ver1 . Ver1 uint16 = 1 // Ver2 . Ver2 = 2 ) var ( // ErrProtoHeaderLen . ErrProtoHeaderLen = errors.New(\u0026#34;not matched proto header len\u0026#34;) // ErrEmptyReader . ErrEmptyReader = errors.New(\u0026#34;empty reader\u0026#34;) ) const ( // size _packSize uint16 = 4 _headerSize uint16 = 2 // uint16 _verSize uint16 = 2 // uint16 _opSize uint16 = 2 // uint16 _seqSize uint16 = 2 // uint16 _rawHeaderSize uint16 = _packSize + _headerSize + _verSize + _opSize + _seqSize // offset _packOffset uint16 = 0 _headerOffset = _packOffset + _packSize _verOffset = _headerOffset + _headerSize _opOffset = _verOffset + _verSize _seqOffset = _opOffset + _opSize ) // Proto .这部分参考了GOIM, 定制了一些协议头信息，可自定义。 type Proto struct { Ver uint16 Op uint16 // Type of Proto Seq uint16 // Seq of message, 0 means done, else means not finished Body []byte // Body of Proto } // New . func New() *Proto { return \u0026amp;Proto{ Ver: Ver1, Op: OpRequest, Seq: 0, Body: nil, } } // WriteTCP . // packLen(32bit):headerLen(16bit):ver(16bit):op(16bit):body func (p *Proto) WriteTCP(wr *bufio.Writer) (err error) { var ( buf = make([]byte, _rawHeaderSize) packLen int ) packLen = int(_rawHeaderSize) + len(p.Body) binary.BigEndian.PutUint32(buf[_packOffset:], uint32(packLen)) binary.BigEndian.PutUint16(buf[_headerOffset:], _rawHeaderSize) binary.BigEndian.PutUint16(buf[_verOffset:], p.Ver) binary.BigEndian.PutUint16(buf[_opOffset:], p.Op) binary.BigEndian.PutUint16(buf[_seqOffset:], p.Seq) if _, err = wr.Write(buf); err != nil { return } if p.Body != nil { _, err = wr.Write(p.Body) } // println(wr.Buffered(), len(p.Body)) return } // ReadTCP . func (p *Proto) ReadTCP(rr *bufio.Reader) (err error) { var ( bodyLen int headerLen uint16 packLen int buf []byte ) if buf, err = ReadNBytes(rr, int(_rawHeaderSize)); err != nil { return } packLen = int(binary.BigEndian.Uint32(buf[_packOffset:_headerOffset])) headerLen = binary.BigEndian.Uint16(buf[_headerOffset:_verOffset]) p.Ver = binary.BigEndian.Uint16(buf[_verOffset:_opOffset]) p.Op = binary.BigEndian.Uint16(buf[_opOffset:_seqOffset]) p.Seq = binary.BigEndian.Uint16(buf[_seqOffset:]) if headerLen != _rawHeaderSize { return ErrProtoHeaderLen } if bodyLen = packLen - int(headerLen); bodyLen \u0026gt; 0 { p.Body, err = ReadNBytes(rr, bodyLen) } else { p.Body = nil } return } // ReadNBytes . read limitted `N` bytes from bufio.Reader. func ReadNBytes(rr *bufio.Reader, N int) ([]byte, error) { if rr == nil { return nil, ErrEmptyReader } var ( buf = make([]byte, N) err error ) for i := 0; i \u0026lt; N; i++ { if buf[i], err = rr.ReadByte(); err != nil { return nil, err } } return buf, err } Proto应用：\n// handleConn to recive a conn, // parse NewRequest and then transfer to call. func (s *Server) handleConn(conn net.Conn) { // receive a request // data, err := bufio.NewReader(conn).ReadBytes(\u0026#39;\\n\u0026#39;) rr := bufio.NewReader(conn) wr := bufio.NewWriter(conn) var ( precv = proto.New() psend = proto.New() ) if err := precv.ReadTCP(rr); err != nil {· DebugF(\u0026#34;response to client connection err: %v\u0026#34;, err) resp := s.codec.NewResponse(nil, nil, InternalErr) psend.Body = encodeResponse(s.codec, resp) // []byte(....) psend.WriteTCP(wr) // Proto写到buffer中去 wr.Flush() // 将字节流从buffer中写到wr.Writer, 可以考虑将这一步藏起来。 return } // ... // more code } 参考文献 # TCP协议 WSS MTU "},{"id":37,"href":"/2019/09/19/%E5%88%9D%E8%AF%86Ring-Buffer/","title":"初识Ring Buffer","section":"Posts","content":" 使用场景 # 一个服务器程序可能会收到多个客户端的网络数据流，在每个数据流上实际上有多个独立的数据包，只有一个数据包接收完整了才能做进一步的处理。如果在一个网络连接上数据包并不完整，就需要暂时缓存住尚未接收完的数据包。\n解决以上场景的方法肯定不止RingBuffer这一种，同理RingBuffer也不只是这一个使用场景，这里只是介绍下RingBuffer在这种场景下的使用。\n实现方式 # RingBuffer结构定义：\nconst A = 10 // Byts ... type Byts []byte // RingBuffer . not concurrent safe type RingBuffer struct { data []*Byts pRead uint pWrite uint size uint } // NewRingBuffer . at least 2 func NewRingBuffer(size uint) *RingBuffer { if size \u0026lt;= 1 { panic(\u0026#34;too small size\u0026#34;) } return \u0026amp;RingBuffer{ data: make([]*Byts, size), pRead: 0, pWrite: 0, size: size, } } RingBuffer方法定义：\nfunc (buf *RingBuffer) Write(d Byts) error { if (buf.pWrite+1)%buf.size == buf.pRead { return errFull } buf.data[buf.pWrite] = d buf.pWrite = (buf.pWrite + 1) % buf.size return nil } func (buf *RingBuffer) Read() (*Byts, error) { if buf.pRead == buf.pWrite { return nil, errEmpty } d := buf.data[buf.pRead] buf.pRead = (buf.pRead + 1) % buf.size return \u0026amp;d, nil } // Reset . drop all data func (buf *RingBuffer) Reset() { buf.pRead = 0 buf.pWrite = 0 } 参考文献 # Ring-Buffer in disroptor Introduction Goim Ring Buffer实现 Ring Buffer 应用 "},{"id":38,"href":"/2019/08/12/LRU%E5%92%8CLRU-K/","title":"LRU和LRU-K","section":"Posts","content":" 缓存淘汰机制 # 缓存淘汰机制在缓存需要被清理的时候使用。主要有以下几种算法：\nFIFO：先入先出，优先清理最先被缓存的数据对象。实现简单，直接使用队列就可以实现。 LRU：最近最久未被使用，优先清理最近没有被使用的对象。使用一个最近使用时间降序的有序队列，优先清理队列对后的数据。与LFU的区别在于：LRU是按照最近使用使用的时间排序，LFU需要维护一个使用频次并用于排序。 LFU：最近最少使用，优先清理最近最少使用的数据对象。使用一个使用次数降序的有序队列，优先清理队列最后的数据。 // 其中LRU和LFU可以通过维护一个Hashmap来提高访问效率。\nLRU / LRU-1 # LRU（Least recently used，最近最少使用）算法根据数据的历史访问记录来进行淘汰数据，其核心思想是“如果数据最近被访问过，那么将来被访问的几率也更高”。实现思路也很简单，就不过多赘述了：\n// data to cache type data struct { key int value int } // LRUCache structture to support LRU alg. type LRUCache struct { recoder map[int]*list.Element // key hit for get op O(1) linked *list.List // linked-list rest int // rest capacity } // Constructor ... to generate a LRUCache func Constructor(capacity int) LRUCache { c := LRUCache{ rest: capacity, linked: list.New(), recoder: make(map[int]*list.Element), } return c } // Get ... O(1) wanted func (c *LRUCache) Get(key int) int { v, ok := c.recoder[key] if !ok { return -1 } // hit the key _ = c.linked.Remove(v) c.recoder[key] = c.linked.PushFront(v.Value) fmt.Println(\u0026#34;get\u0026#34;, key, c.linked.Len(), c.recoder) return v.Value.(*data).value } // Put ... O(1) wanted func (c *LRUCache) Put(key int, value int) { if v, ok := c.recoder[key]; ok { c.linked.Remove(v) goto h } if c.rest == 0 { tail := c.linked.Back() c.linked.Remove(tail) delete(c.recoder, tail.Value.(*data).key) } else { c.rest-- } h: head := c.linked.PushFront(\u0026amp;data{key, value}) c.recoder[key] = head } LRU-K # LRU-K的主要目的是为了解决LRU算法“缓存污染”的问题，其核心思想是将“最近使用过1次”的判断标准扩展为“最近使用过K次”。也就是说没有到达K次访问的数据并不会被缓存，这也意味着需要对于缓存数据的访问次数进行计数，并且访问记录不能无限记录，也需要使用替换算法进行替换。当需要淘汰数据时，LRU-K会淘汰第K次访问时间距当前时间最大的数据。\n简单的描述就是：当访问次数达到K次后，将数据索引从历史队列移到缓存队列中（缓存队列时间降序）；缓存数据队列中被访问后重新排序；需要淘汰数据时，淘汰缓存队列中排在末尾的数据。\n相比于LRU-1，缓存数据更不容易被替换，而且偶发性的数据不易被缓存。在保证了缓存数据纯净的同时还提高了热点数据命中率。\ncode 有点略长\n下面的代码只能帮助理解LRU-K的算法思想，并不适用于生产。\n// LRUKCache history count struct type historyCounter struct { key int visited uint } // LRUKCache . type LRUKCache struct { K uint // the K setting rest uint // max - used = rest historyRest uint // historyMax - used = historyRest history *list.List // history doubly linked list historyHash map[int]*list.Element // history get op O(1) cache *list.List // cache doubly linked list, save cacheHash map[int]*list.Element // cache get op O(1) } // NewLRUKCache . func NewLRUKCache(k uint, capacity uint) *LRUKCache { // ignored } // reorderCache cache linked-list to sort (Least Recently Used) func (c *LRUKCache) reorderCache(ele *list.Element) { // 默认最近访问的元素排在队首 } func (c *LRUKCache) cacheReplacingCheck() { if c.rest == 0 { // 如果超出容量则删除队尾元素 } } func (c *LRUKCache) historyReplacingCheck() { if c.historyRest == 0 { // 如果超出容量则删除队尾元素 } } func (c *LRUKCache) delfromHistory(key int) { // 从历史中删除一个元素 } func (c *LRUKCache) addtoCache(d data) { // 增加一条缓存记录 } func (c *LRUKCache) addtoHistory(key, value int) historyCounter { var ( hc historyCounter ) hEle, ok := c.historyHash[key] if ok { // 如果已经存在于历史队列中，则自增访问次数 hc = hEle.Value.(historyCounter) hc.visited++ // 如果访问次数达到标准，则从历史中演移除，增加在缓存队列中去 if hc.visited \u0026gt;= c.K { // true: removed from history, and add into cache c.delfromHistory(key) c.addtoCache(data{key: key, value: value}) return hc } // 否则写回历史队列 hEle.Value = hc } else { // 如果不存在于历史队列中 c.historyReplacingCheck() hc = historyCounter{key: key, visited: 1} hEle = c.history.PushFront(hc) c.historyRest-- } // 更新历史访问数据 c.historyHash[key] = hEle return hc } // Get key of cache in LRUKCache . func (c *LRUKCache) Get(key int) (value int, ok bool) { ele, ok := c.cacheHash[key] if ok { c.reorderCache(ele) return ele.Value.(data).value, true } // false: missed return 0, false } // Put of LRUKCache func (c *LRUKCache) Put(key, value int) { ele, ok := c.cacheHash[key] if ok { // true: hit the key c.reorderCache(ele) return } _ = c.addtoHistory(key, value) fmt.Printf(\u0026#34;cacheRest: %d / cacheMap: %v \\nhistoryRest: %d / historyMap: %v\\n\\n\u0026#34;, c.rest, c.cacheHash, c.historyRest, c.historyHash) return } testcase\nfunc TestLRUKCache(t *testing.T) { cache := cp.NewLRUKCache(2, 2) cache.Put(2, 1) if _, hit := cache.Get(2); hit { t.Error(\u0026#34;could not get 2 hit\u0026#34;) } cache.Put(3, 1) if _, hit := cache.Get(3); hit { t.Error(\u0026#34;could not get 3 hit\u0026#34;) } cache.Put(3, 1) if _, hit := cache.Get(3); !hit { t.Error(\u0026#34;should get 3 hit\u0026#34;) } cache.Put(4, 1) cache.Put(4, 1) if _, hit := cache.Get(2); hit { t.Error(\u0026#34;could not get 2 hit\u0026#34;) } if _, hit := cache.Get(4); !hit { t.Error(\u0026#34;should get 2 hit\u0026#34;) } if _, hit := cache.Get(3); !hit { t.Error(\u0026#34;should get 3 hit\u0026#34;) } } output\n# cacheRest: 2, historyRest: 6, k: 2 # cache.Put(2, 1) cacheRest: 2 / cacheMap: map[] historyRest: 5 / historyMap: map[2:0xc00006e420] # cache.Put(3, 1) cacheRest: 2 / cacheMap: map[] historyRest: 4 / historyMap: map[2:0xc00006e420 3:0xc0000e0000] # cache.Put(3, 1) cacheRest: 1 / cacheMap: map[3:0xc00006e4b0] historyRest: 5 / historyMap: map[2:0xc00006e420] # cache.Put(4, 1) cacheRest: 1 / cacheMap: map[3:0xc00006e4b0] historyRest: 4 / historyMap: map[2:0xc00006e420 4:0xc00006e570] # cache.Put(4, 1) cacheRest: 0 / cacheMap: map[3:0xc00006e4b0 4:0xc0000e00f0] historyRest: 5 / historyMap: map[2:0xc00006e420] FAQ # Q1. LRU缓存污染指的是什么情况？\n偶发性的、周期性的批量操作会导致LRU命中率急剧下降，这时候缓存中的数据大部分都不是热点数据。\nQ2. LRU-K的K值怎么确定？\nK值增大，命中率会更高，但是适应性差（清除一个缓存需要大量的数据访问，一般选择LRU-2）。\n"},{"id":39,"href":"/2019/04/05/%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8Bsnowflake%E5%92%8Crc4/","title":"介绍一下snowflake和rc4","section":"Posts","content":"snowflake是twitter公司开源的生成唯一ID的网络服务，具有很强的伸缩性，这里只取用生成唯一ID的算法部分。 rc4（Rivest Cipher 4）是一种流加密算法，密钥长度可变，它的加解密使用相同的密钥，因此也属于对称加密算法。\n为啥要介绍这两种算法？ # 其一，snowflake可以生成唯一ID，而相比与UUID，snowflake生成的ID更加“好用”，这个放在后面解释。 其二，UUID和snowflake虽然可以生成唯一ID，但是无法适用于所有场景，譬如说“生成推广码”。生成推广码的时候，希望尽可能短而精，很明显唯一ID都不太短。\nsnowflake # snowflake的唯一ID是一个64bit的int型数据，相较于UUID来说耗费空间更小，可以更方便的作为数据库主键来索引和排序。\n生成过程： # 置0不用 timestamp（41bits）精确到ms。 machine-id（10bits）该部分其实由datacenterId和workerId两部分组成，这两部分是在配置文件中指明的。datacenterId（5bits）方便搭建多个生成uid的service，并保证uid不重复。workerId（5bits）是实际server机器的代号，最大到32，同一个datacenter下的workerId是不能重复的。 sequence-id(12bits)，该id可以表示4096个数字，它是在time相同的情况下，递增该值直到为0，即一个循环结束，此时便只能等到下一个ms到来，一般情况下4096/ms的请求是不太可能出现的，所以足够使用了。 优势和缺陷： # 速度快，无依赖，原理和实现简单，也可以根据自己的需求做算法调整 依赖机器时间，如果时间回拨可能导致重复的ID rc4 # RC4加密算法也是一种基于密钥流的加密算法。\n首先，rc4根据明文和密钥生成的密钥流，其长度和明文的长度是相等的，也就是说明文的长度是500字节，那么密钥流也是500字节，这也是我们用来生成推广码的原因之一了；其次，rc4是是对称加密完全可以通过密文得到明文，也就是说在生成码的时候把必要信息放在明文中，在使用密文的时候可以不用查库也能得到相关的信息，譬如用户ID，这是原因之二。\n使用场景 # 现在需要生成一种码，短小易记，且唯一，但并不需要大量。\n上述的snowflake和UUID都很容易实现唯一，但是短小就不符合要求了。因为并不需要大量生成这种码，因此我们考虑用自增ID + RC4来实现：\npackage main import ( \u0026#34;crypto/rc4\u0026#34; \u0026#34;encoding/hex\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; ) func main() { cipher, err := rc4.NewCipher([]byte(\u0026#34;thisiskey\u0026#34;)) if err != nil { log.Fatalf(\u0026#34;wrong with NewCipher: %v\u0026#34;, err) } c := map[string]bool{} for i := 0; i \u0026lt; 1000; i++ { src := []byte(fmt.Sprintf(\u0026#34;%7d\u0026#34;, i)) dst := make([]byte, len(src)) cipher.XORKeyStream(dst, src) s := toString(dst) // 密文是不可读的字节流，这里采用hex编码 println(s) // 形如：a09def6b6e4797 c[s] = true } println(len(c)) # 1000 } func toString(src []byte) string { return hex.EncodeToString(src) } 参考 # twitter-snowflake bwmarrin/snowflake yeqown/snowflake 关于snowflake的分析 为什么推荐InnoDB引擎使用自增主键? "},{"id":40,"href":"/2019/04/01/go-watcher-%E7%83%AD%E9%87%8D%E8%BD%BD%E8%BD%AE%E5%AD%90/","title":"go-watcher-热重载轮子","section":"Posts","content":"Golang编写的热重载工具，自定义命令，支持监视文件及路径配置，环境变量配置。这是一个重复的轮子～\n安装使用 # go install github.com/yeqown/go-watcher/cmd/go-watcher 命令行 # ➜ go-watcher git:(master) ✗ ./go-watcher -h NAME: go-watcher - A new cli application USAGE: go-watcher [global options] command [command options] [arguments...] VERSION: 2.0.0 AUTHOR: yeqown@gmail.com COMMANDS: init generate a config file to specified postion run execute a command, and watch the files, if any change to these files, the command will reload help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --help, -h show help --version, -v print the version 配置文件 # watcher: # 监视器配置 duration: 2000 # 文件修改时间间隔，只有高于这个间隔才回触发重载 included_filetypes: # 监视的文件扩展类型 - .go # excluded_regexps: # 不被监视更改的文件正则表达式 - ^.gitignore$ - \u0026#39;*.yml$\u0026#39; - \u0026#39;*.txt$\u0026#39; additional_paths: [] # 除了当前文件夹需要额外监视的文件夹 excluded_paths: # 不需要监视的文件名，若为相对路径，只能对于当前路径生效 - vendor - .git envs: # 额外的环境变量 - GOROOT=/path/to/your/goroot - GOPATH=/path/to/your/gopath 使用范例日志 # ➜ go-watcher git:(master) ✗ ./package/osx/go-watcher run -e \u0026#34;make\u0026#34; -c ./config.yml [INFO] directory (/Users/yeqown/Projects/opensource/go-watcher) is under watching [INFO] directory (/Users/yeqown/Projects/opensource/go-watcher/cmd) is under watching [INFO] directory (/Users/yeqown/Projects/opensource/go-watcher/cmd/go-watcher) is under watching [INFO] directory (/Users/yeqown/Projects/opensource/go-watcher/internal) is under watching [INFO] directory (/Users/yeqown/Projects/opensource/go-watcher/internal/command) is under watching [INFO] directory (/Users/yeqown/Projects/opensource/go-watcher/internal/log) is under watching [INFO] directory (/Users/yeqown/Projects/opensource/go-watcher/internal/testdata) is under watching [INFO] directory (/Users/yeqown/Projects/opensource/go-watcher/internal/testdata/exclude) is under watching [INFO] directory (/Users/yeqown/Projects/opensource/go-watcher/internal/testdata/testdata_inner) is under watching [INFO] directory (/Users/yeqown/Projects/opensource/go-watcher/package) is under watching [INFO] directory (/Users/yeqown/Projects/opensource/go-watcher/package/archived) is under watching [INFO] directory (/Users/yeqown/Projects/opensource/go-watcher/package/linux) is under watching [INFO] directory (/Users/yeqown/Projects/opensource/go-watcher/package/osx) is under watching [INFO] directory (/Users/yeqown/Projects/opensource/go-watcher/resources) is under watching [INFO] directory (/Users/yeqown/Projects/opensource/go-watcher/utils) is under watching [INFO] directory (/Users/yeqown/Projects/opensource/go-watcher/utils/testdata) is under watching [INFO] directory (/Users/yeqown/Projects/opensource/go-watcher/utils/testdata/testdata_inner) is under watching rm -fr package go build -o package/osx/go-watcher cmd/go-watcher/main.go GOOS=linux GOARCH=amd64 go build -o package/linux/go-watcher cmd/go-watcher/main.go mkdir -p package/archived tar -zcvf package/archived/go-watcher.osx.tar.gz package/osx a package/osx a package/osx/go-watcher tar -zcvf package/archived/go-watcher.linux.tar.gz package/linux a package/linux a package/linux/go-watcher [INFO] command executed done! [INFO] (/Users/yeqown/Projects/opensource/go-watcher/package/osx/go-watcher) is skipped, not target filetype [INFO] (/Users/yeqown/Projects/opensource/go-watcher/package/osx) is skipped, not target filetype [INFO] (/Users/yeqown/Projects/opensource/go-watcher/package) is skipped, not target filetype [INFO] (/Users/yeqown/Projects/opensource/go-watcher/package/linux/go-watcher) is skipped, not target filetype [INFO] (/Users/yeqown/Projects/opensource/go-watcher/package/linux) is skipped, not target filetype [INFO] (/Users/yeqown/Projects/opensource/go-watcher/package/archived/go-watcher.linux.tar.gz) is skipped, not target filetype [INFO] (/Users/yeqown/Projects/opensource/go-watcher/package/archived) is skipped, not target filetype [INFO] (/Users/yeqown/Projects/opensource/go-watcher/VERSION) is skipped, not target filetype [INFO] [/Users/yeqown/Projects/opensource/go-watcher/cmd/go-watcher/main.go] changed rm -fr package mkdir -p package/osx mkdir -p package/linux echo \u0026#34;2.0.0\u0026#34; \u0026gt; VERSION cp VERSION package/osx cp VERSION package/linux go build -o package/osx/go-watcher cmd/go-watcher/main.go GOOS=linux GOARCH=amd64 go build -o package/linux/go-watcher cmd/go-watcher/main.go mkdir -p package/archived tar -zcvf package/archived/go-watcher.osx.tar.gz package/osx a package/osx a package/osx/go-watcher a package/osx/VERSION tar -zcvf package/archived/go-watcher.linux.tar.gz package/linux a package/linux a package/linux/go-watcher[INFO] (/Users/yeqown/Projects/opensource/go-watcher/package/osx) is skipped, not target filetype [INFO] (/Users/yeqown/Projects/opensource/go-watcher/package/linux) is skipped, not target filetype a package/linux/VERSION [INFO] command executed done! [INFO] (/Users/yeqown/Projects/opensource/go-watcher/package/osx) is skipped, not target filetype [INFO] (/Users/yeqown/Projects/opensource/go-watcher/package/archived) is skipped, not target filetype [INFO] (/Users/yeqown/Projects/opensource/go-watcher/package) is skipped, not target filetype [INFO] (/Users/yeqown/Projects/opensource/go-watcher/VERSION) is skipped, not target filetype [INFO] (/Users/yeqown/Projects/opensource/go-watcher/package) is skipped, not target filetype ^C[INFO] quit signal captured! [INFO] go-watcher exited ➜ go-watcher git:(master) ✗ "},{"id":41,"href":"/2019/03/30/goswagger%E5%85%A5%E9%97%A8%E6%89%8B%E5%86%8C/","title":"goswagger入门手册","section":"Posts","content":" 本文旨在记录使用goswagger过程中遇到的一些问题（只在生成文档方面，不涉及其他功能）：\n如何在go1.11+以上（支持Go Module）版本中的应用swagger 一些注解上的注意事项 如何在团队中管理API文档（主要涵盖了：swagger-ui的部署和使用） 关于swagger # swagger涵盖WebAPI的一整套工具：API设计，API实现（代码生成），API文档，API测试及API规范。更多信息请参见官网\n准备工作 # 一个Golang web项目，并软连接到GOPATH/src下。【毕竟是支持Gomodule的项目，还放在GOPATH下就不科学了😄】 安装swagger工具. 参见安装 环境： ➜ swagger-demo git:(master) ✗ go version go version go1.11.5 darwin/amd64 ➜ swagger-demo git:(master) ✗ swagger version version: v0.18.0 commit: 6b23bb61413826ce42c3b14a37bf5870caf91e0b 编写注释 # 元信息包含了这个应用的基本信息。一般新建一个doc.go放在你的API根目录下；还有一定要注意这句话：\nYou give it a main file and it will parse all the files that are reachable by that main package to produce a swagger specification.To use you can add a go:generate comment to your main file。\n//go:generate swagger generate spec\nThe command requires a main package or file and it wants your code to compile. It uses the go tools loader to load an application and then scans all the packages that are in use by the code base. This means that for something to be discoverable it needs to be reachable by a code path triggered through the main package.\n就是说如果你配置好了一切东西，但是却因为你的main包“花里胡哨”，而无法生成文档的时候，记得尝试下这个方法！！！\n一般来说，一个API的文档无非包括：请求方法，路由，参数，响应。如下：\n// 格式：swagger:parameters 操作ID(也就是标示一个路由的ID) // swagger:parameters opid-get type getForm struct { // in: query ID int `form:\u0026#34;id\u0026#34; binding:\u0026#34;required\u0026#34;` } // 格式：swagger:model ?模型名(可选) // 动手题：为什么还需要额外定义一个模型getmodels来使用？而不是直接在getResponse中使用 []*models.GetModel ? // swagger:model type getmodels []*models.GetModel // 格式：swagger:response 响应结构名 // getResponse response demo of get controller // swagger:response getResponse type getResponse struct { // code int，这里参数较多自行参照文档，个人觉得比较实用的是 “in: query|body” 用于说明参数存在的位置 // in: body Code int `json:\u0026#34;code\u0026#34;` // modesl getmodel // in: body Models getmodels `json:\u0026#34;models\u0026#34;` } // Get ... func Get(c *gin.Context) { // swagger:route GET /base/get 范例 opid-get // // swagger Get范例 // // // Consumes: // - application/json // - application/x-protobuf // // Produces: // - application/json // - application/x-protobuf // // Schemes: http, https, ws, wss // // Responses: // default: getResponse var ( form = new(getForm) resp = new(getResponse) ) resp.Code = 0 resp.Models = services.ListGetModels() log.Println(\u0026#34;form is :\u0026#34;, *form) c.JSON(http.StatusOK, resp) } 上述只是最基本的关于一个API的基础注释。更多请参见官方文档，尽管它不一定是最新的～\n生成API文档（swagger.json） # swagger 生成文档的命令如下：\n➜ gonic git:(master) ✗ swagger generate spec -h Usage: swagger [OPTIONS] generate spec [spec-OPTIONS] generate a swagger spec document from a go application Application Options: -q, --quiet silence logs -o, --output=LOG-FILE redirect logs to file Help Options: -h, --help Show this help message [spec command options] -b, --base-path= the base path to use (default: .) -t, --tags= build tags -m, --scan-models includes models that were annotated with \u0026#39;swagger:model\u0026#39; --compact when present, doesn\u0026#39;t prettify the json -o, --output= the file to write to -i, --input= the file to use as input -c, --include= include packages matching pattern -x, --exclude= exclude packages matching pattern --include-tag= include routes having specified tags (can be specified many times) --exclude-tag= exclude routes having specified tags (can be specified many times) 用于说明的Makefile\ngen-doc: # 与 v3无异，只是不格式化生成的json文件 GO111MODULE=off swagger generate spec -o swagger-demo.swagger.final.json -b ./mainC -c swagger-demo -m --compact gen-doc-v3: # 支持扫描 swagger:model GO111MODULE=off swagger generate spec -o swagger-demo.swagger.v3.json -b ./mainC -c swagger-demo -m gen-doc-v2: # 如果遇到找不到自己项目内部包的情况，且main包花里呼哨 GO111MODULE=off swagger generate spec -o swagger-demo.swagger.v2.json -b ./mainC -c swagger-demo gen-doc-v1: # main包花里胡哨，并不是一个单文件 GO111MODULE=off swagger generate spec -o swagger-demo.swagger.v1.json -b ./mainC gen-doc-v0: # 常规用法（适用于符合goswagger标准的项目） GO111MODULE=off swagger generate spec -o swagger-demo.swagger.v0.json default: gen-doc-v0 gen-doc-v1 gen-doc-v2 gen-doc-v3 gen-doc 管理API文档 # 生成了swagger.json文件之后，接下来就祭出可视化工具了swagger-ui。如果只是尝鲜，可以先用swagger-editor预览一下。\n话不多说： swagger-ui的docker镜像版：自带nginx。\n直接上docker-compose配置文件，语法可能不兼容各个版本，请自行调试。\nversion: \u0026#34;2\u0026#34; services: swagger_ui: environment: - SWAGGER_JSON=/usr/share/nginx/html/docs/swagger.json image: \u0026#34;swaggerapi/swagger-ui\u0026#34; volumes: [\u0026#34;./docs/:/usr/share/nginx/html/docs:rw\u0026#34;] restart: always ports: [\u0026#34;9000:8080\u0026#34;] 把容器运行起来后，你就可以在配置的端口访问到swagger-ui的界面了，在顶部的搜索栏中输入想要查看的文档的URL，就可以方便的查看文档了。\n参考资料 # 以上代码均在yeqown/playground/gonic/swagger-demo可以找到 swagger官网 goswagger.io "},{"id":42,"href":"/2018/11/26/QR-Code%E7%94%9F%E6%88%90%E5%99%A8forgolang/","title":"QRCode Generator based Golang","section":"Posts","content":"项目地址：yeqown/go-qrcode 同类项目：skip2/go-qrcode 纠错算法和bitset使用了该库，后续可能会考虑自己实现一遍\ngo-qrcode # 示例 # link to CODE\npackage main import ( \u0026#34;fmt\u0026#34; qrcode \u0026#34;github.com/yeqown/go-qrcode\u0026#34; ) func main() { qrc, err := qrcode.New(\u0026#34;https://github.com/yeqown/go-qrcode\u0026#34;) if err != nil { fmt.Printf(\u0026#34;could not generate QRCode: %v\u0026#34;, err) } // save file if err := qrc.Save(\u0026#34;../testdata/repo-qrcode.jpeg\u0026#34;); err != nil { fmt.Printf(\u0026#34;could not save image: %v\u0026#34;, err) } } 生成结果如图： QR Code 基本原理 # 1 数据分析（data analysis）： # 分析输入数据，根据数据决定要使用的QR码版本、容错级别和编码模式。低版本的QR码无法编码过长的数据，含有非数字字母字符的数据要使用扩展字符编码模式。\n2 编码数据（data encoding）： # 根据选择的编码模式，将输入的字符串转换成比特流，插入模式标识码（mode indicator）和终止标识符（terminator），把比特流切分成八比特的字节，加入填充字节来满足标准的数据字码数要求。\n3 计算纠错码（error correction coding）： # 对步骤二产生的比特流计算纠错码，附在比特流之后。高版本的编码方式可能需要将数据流切分成块（block）再分别进行纠错码计算。\n4 组织数据（structure final message）： # 根据结构图把步骤三得到的有容错的数据流切分，准备填充。\n5 填充（module placement in matrix）： # 把数据和功能性图样根据标准填充到矩阵中。\n6 应用数据掩码（data masking）： # 应用标准中的八个数据掩码来变换编码区域的数据，选择最优的掩码应用。讲到再展开。\n7 填充格式和版本信息（format and version information）： # 计算格式和版本信息填入矩阵，完成QR码。\n参考文献 # QR Code tutori 如果想要深入了解二维码生成的话，强烈推荐此博客 QRCode Wiki 二维码详解（QR Code） "},{"id":43,"href":"/2018/11/19/go-get%E9%81%87%E5%88%B0%E5%A2%99%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/","title":"go-get遇到🧱的解决方法","section":"Posts","content":"解决方法有两种，在网上也很好找到：\n1. 最简单的，从¥“github.com/golang”找到对应的包并下载到 $GOPATH/src/golang.org/x/ 下 2. 第二种，就是翻过🧱了。 1 问题：翻了🧱，还是没办法直接使用go get来下载呢？ # 先说原因，因为go get并没有走你的代理啊！！！！那么如何设置代理呢？\nexport http_proxy=http://ip:port go get golang.org/xxx 其他设置代理的方式，自行参见 参考\n2 如果你的代理不支持http || https协议，可咋整？ # 那么想办法支持http或者把http再代理到你可以使用的协议（socks5~），那么可以使用cow。\ncow 推荐使用方式：\ngo get 下载安装（因为刚开始图简单，使用程序的时候运行报错了，go get 安装方式并不会有这样的困扰）。 配置的时候也很简单，编辑配置文件 ～/.cow/rc，配置http socks5代理服务和监听代理端口。\nlisten = http://127.0.0.1:7777 proxy = socks5://127.0.0.1:1080 3 运行 # 配置完成之后就可以直接运行了\n./cow # 另开一个Terminal export http_proxy=http://ip:port go get golang.org/xxx 写在后面 # 知道了go get无法翻🧱的原因之后，就可以发挥自己的想象力来解决问题了，这样解决还是挺繁琐的。虽然cow可以配置开机启动，但对于一个懒癌晚期）的人来说（如果不是因为升级Go到了1.11，go-module机制让我无法开心的玩耍，我也不会去考虑为啥翻🧱了go get还是不能用，明明有代理却还有使用另外一个代理～。\n友情提示：vscode-go 也可以设置proxy哦\n参考： # go get 使用代理 cow cow配置文件说明 vscode-go设置proxy "},{"id":44,"href":"/2018/10/23/etcd-service-registration-discovery/","title":"etcd与service-registration-discovery","section":"Posts","content":" 声明：本文对etcd的原理，实现细节，性能等均不考虑，仅将etcd作为一个分布式的K-V存储组件。本文提价代码均在： github.com/yeqown/server-common/tree/master/framework/etcd\n一个核心 # etcd, 分布式Key-Value存储工具。详细资料由此去\netcd下载安装 两个对象 # 服务提供者（在测试环境中，我定义为单独的服务实例），也就是服务的提供者，需要向其他服务暴露自己的ip和端口，方便调用。 服务调用者（同样地，在测试环境中我定义为反向代理网关程序），也就是服务的调用者，需要获取到 可使用 地服务地址并调用。 关于服务注册与发现 # 就具体场景而言：我们的生产环境中使用了一个代理网关服务器，用于转发移动端和PC端的API请求，并完成其他功能。所有的服务实例配置都是硬编码在网关程序中，顶多就是抽离出来成了一个配置文件。这样做的缺点很明显：“非动态”。也就意味着，一旦有服务Down掉，那么用户访问则可能异常，甚至导致整个服务的崩溃；其次，需要对服务进行扩容的情况下，则需要先进行服务部署再更新网关程序，步骤繁琐且容易出错。\n那么如果我们设计成为如下图的样子： 对于新添加的服务实例，只需要启动新的服务，并注册到etcd相应的路径下就行了。\n注册：对于同一组服务，配置一个统一的前缀（如图上的\u0026quot;/specServer\u0026quot;），不同实例使用ID加以区分。\n将现行服务改造成为上述模式需要解决的问题： # etcd 配置安装 网关程序改造（监听etcd的节点夹子/prefix;适配动态的服务实例调用） 服务实例改造（注册服务实例到etcd;心跳更新;其他配套设施，异常退出删除注册信息） etcd安装配置在github.com已经非常详细了。在这里贴一下我在本地测试时候启动的脚本（这部分是从etcd-demo获取到的，做了针对端口的改动）：\n#!/bin/bash # For each machine TOKEN=token-01 CLUSTER_STATE=new NAME_1=machine1 NAME_2=machine2 NAME_3=machine3 HOST_1=127.0.0.1 HOST_2=127.0.0.1 HOST_3=127.0.0.1 CLUSTER=${NAME_1}=http://${HOST_1}:2380,${NAME_2}=http://${HOST_2}:2381,${NAME_3}=http://${HOST_3}:2382 # For machine 1 THIS_NAME=${NAME_1} THIS_IP=${HOST_1} etcd --data-dir=machine1.etcd --name ${THIS_NAME} \\ --initial-advertise-peer-urls http://${THIS_IP}:2380 --listen-peer-urls http://${THIS_IP}:2380 \\ --advertise-client-urls http://${THIS_IP}:2377 --listen-client-urls http://${THIS_IP}:2377 \\ --initial-cluster ${CLUSTER} \\ --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN} \u0026amp; # For machine 2 THIS_NAME=${NAME_2} THIS_IP=${HOST_2} etcd --data-dir=machine2.etcd --name ${THIS_NAME} \\ --initial-advertise-peer-urls http://${THIS_IP}:2381 --listen-peer-urls http://${THIS_IP}:2381 \\ --advertise-client-urls http://${THIS_IP}:2378 --listen-client-urls http://${THIS_IP}:2378 \\ --initial-cluster ${CLUSTER} \\ --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN} \u0026amp; # For machine 3 THIS_NAME=${NAME_3} THIS_IP=${HOST_3} etcd --data-dir=machine3.etcd --name ${THIS_NAME} \\ --initial-advertise-peer-urls http://${THIS_IP}:2382 --listen-peer-urls http://${THIS_IP}:2382 \\ --advertise-client-urls http://${THIS_IP}:2379 --listen-client-urls http://${THIS_IP}:2379 \\ --initial-cluster ${CLUSTER} \\ --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN} \u0026amp; 对于程序的改造，鉴于服务较多且etcd操作流程大体一致，便简单包装了一下，项目地址见文首位置。\n1.对于调用方使用示例如下：\n// etcdtest/gw.go func main() { // ... endpoints := []string{ \u0026#34;http://127.0.0.1:2377\u0026#34;, \u0026#34;http://127.0.0.1:2379\u0026#34;, \u0026#34;http://127.0.0.1:2378\u0026#34;, } // 连接etcd获取KeysAPI kapi, err := etcd.Connect(endpoints...) if err != nil { fmt.Println(err) os.Exit(2) } // debug more, more log ~ etcd.OpenDebug(true) // etcd watch, 监听/prefix目录下的改动（“expire;set;update;delete”） // 如：set {Key: /prefix/srv_3457, CreatedIndex: 1155, ModifiedIndex: 1155, TTL: 12} // 并更新watcher.members, 维持最新的节点状态和数量 watcher = etcd.NewWatcher(kapi, \u0026#34;prefix\u0026#34;) go watcher.Watch() // ... } func ServeHTTP() { // ... srvs := watcher.RangeMember() // 获取所有可用的服务节点 // ... } 2.对于请求提供方，使用示例如下：\n// etcdtest/server.go func main() { // ... endpoints := []string{ \u0026#34;http://127.0.0.1:2377\u0026#34;, \u0026#34;http://127.0.0.1:2379\u0026#34;, \u0026#34;http://127.0.0.1:2378\u0026#34;, } etcd.OpenDebug(true) kapi, err := etcd.Connect(endpoints...) if err != nil { fmt.Errorf(err.Error()) os.Exit(2) } // 根据服务生成一个provider, 用于生成K:V provider := etcd.NewProvider( fmt.Sprintf(\u0026#34;srv_%d\u0026#34;, *port), // name fmt.Sprintf(\u0026#34;http://127.0.0.1:%d\u0026#34;, *port), // addr ) ctx, cancel := context.WithCancel(context.Background()) // 每10s设置一个TTL=12s的 “/prefix/id”:“http://host:port” 的的键值对 // 10s和12s是写死的，没有考虑动态～～，后续考虑升级，目前仅仅是测试。 go provider.Heartbeat(ctx, kapi, \u0026amp;etcd.ProvideOptions{ NamePrefix: \u0026#34;prefix\u0026#34;, SetOpts: nil, }) //... } 关于详细的代码，可以参见:\ngodoc.org/go.etcd.io/etcd/client provider.go#L42 requester.go#L134 etcdtest-测试代码 测试 # dplayer \"url=/mov/etcd-example-video.mov\" \"loop=no\" \"theme=#FADFA3\" \"autoplay=false\" \"token=tokendemo\" 总结 # 代码包装得比较粗糙，视频演示还没有包含到服务异常（退出）之后网关程序的应对（这部分是已经完成，只是没有演示）。\n"},{"id":45,"href":"/2018/08/29/Golang%E9%80%82%E7%94%A8%E7%9A%84DTO%E5%B7%A5%E5%85%B7/","title":"Golang适用的DTO工具","section":"Posts","content":" DTO (Data Transfer Object) 是Java中的概念，起到数据封装和隔离的作用。在使用Golang开发Web应用的过程中，也会有类似的需求。先贴项目地址 github.com/yeqown/server-common/tree/master/dbs/tools\n举个例子 # 现在有一个用户数据结构如下,\ntype UserModel struct { ID int64 `gorm:\u0026#34;column:id\u0026#34;` Name string `gorm:\u0026#34;column:name\u0026#34;` Password string `gorm:\u0026#34;column:password\u0026#34;` } // 问题1: 现在要求是想要JSON格式返回用户数据，并且不希望其中包含有Password字段 // 解决1:\ntype UserModel struct { ID int64 `gorm:\u0026#34;column:id\u0026#34; json:\u0026#34;id\u0026#34;` Name string `gorm:\u0026#34;column:name\u0026#34; json:\u0026#34;name\u0026#34;` Password string `gorm:\u0026#34;column:password\u0026#34; json:\u0026#34;-\u0026#34;` } // 问题2: 同样是JSON数据格式，并且希望额外返回用户的身份标示Ident（假设必须要跟用户数据放在一起） // 解决2: （这也是我的场景）\ntype UserDTO struct { ID int64 `json:\u0026#34;id\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` Password string `json:\u0026#34;-\u0026#34;` Ident string `json:\u0026#34;ident\u0026#34;` } func LoadUserDTOFromModel(data *UserMolde) *UserDTO { ident := genUserIdent(data) return \u0026amp;{ ID data.ID, Name data.Name, Ident ident, } } 背景和需求 # 一般来说我的项目结构如下：其中models和services也就是分开定义Data struct(UserModel)和Object(UserDTO)的文件夹。\n其实DTO的过程对于我来说，就是基于Data Struct生成一个新的Struct结构，并附带一个func LoadDTOTypeFromModel(data *ModelType) *DTOType。在这个过程中，其实除了个别Object结构体需要额外处理以外，大部分都是新换一个tag~。因此这部分工作步骤都是类似的，那么为什么不用一个工具来避免这部分重复的工作呢～？\n思路 # 先说一下思路:\n· 1. 从.go中获取到指定结构体的结构性描述 · 2. 根据结构性描述来生成新的结构体 · 3. 根据额外的配置，生成一个新的文件types.go\n其中结构性描述如下：\ntype innerStruct struct { fields []*field content string name string pkgName string } type field struct { name string // field name string typ string // field type string tag string // tag name string } 在整个流程中比较麻烦的就是，怎么获取到，特定类型结构体的结构性描述？Go文件解析部分。这里想记录一个小插曲：最开始我找解析go文件方法的时候，在Google中搜索“如何解析go文件”，出来的结果没有太大帮助，然后我又尝试了“How to parse .go file source code”，结果就提示了parser \u0026amp; loader 两个看起来就很有帮助的包名。。。。这里我选用了loader。\n关于loader包的说明 # Package loader loads a complete Go program from source code, parsing and type-checking the initial packages plus their transitive closure of dependencies.\n正好这个包是从源代码去加载Go程序，对初始包进行解析和类型检查等。\nGo文件解析部分 # 经过阅读loader包文档，我完成了一个函数用于获取指定的结构体的结构性描述信息：代码在此\n// Exported, and specified type func loadGoFiles(dir string, filenames ...string) ([]*innerStruct, error) { newFilenames := []string{} for _, filename := range filenames { newFilenames = append(newFilenames, path.Join(dir, filename)) } conf.CreateFromFilenames(\u0026#34;\u0026#34;, newFilenames...) prog, err := conf.Load() if err != nil { log.Println(\u0026#34;load program err:\u0026#34;, err) return nil, err } return loopProgramCreated(prog.Created), nil } // loopProgramCreated to loo and filter: // 1. unexported type // 2. bultin types // 3. only specified style struct name func loopProgramCreated( created []*loader.PackageInfo, ) (innerStructs []*innerStruct) { for _, pkgInfo := range created { pkgName := pkgInfo.Pkg.Name() defs := pkgInfo.Defs for indent, obj := range defs { if !indent.IsExported() || obj == nil || !strings.HasSuffix(indent.Name, specifiedStructTypeSuffix) { continue } // obj.String() 得到的string如： // type testdata.UserModel struct{Name string \u0026#34;gorm:\\\u0026#34;colunm:name\\\u0026#34;\u0026#34;; Password string \u0026#34;gorm:\\\u0026#34;column:password\\\u0026#34;\u0026#34;} is := parseStructString(obj.String()) is.pkgName = pkgName is.pureName() if isDebug { log.Println(\u0026#34;parse one Model: \u0026#34;, is.name, is.pkgName, is.content) } innerStructs = append(innerStructs, is) } } return } 其中parseStructString是对形如***type testdata.UserModel struct{Name string \u0026ldquo;gorm:\u0026quot;colunm:name\u0026quot;\u0026rdquo;; Password string \u0026ldquo;gorm:\u0026quot;column:password\u0026quot;\u0026rdquo;}***的字符串进行处理并整理成为innerStruct数据。\n使用说明 # go get github.com/yeqown/server-common/dbs/tools # 获取 github.com/yeqown/server-common/tool.main.go, # 并选择性的实现自己的 CustomParseTagFunc \u0026amp; CustomGenerateTagFunc go build -o dbtools tool.main.go ➜ ✗ dbtools -h Usage of ./dbtools: -debug 调试模式开关，调试模式下会输出额外的信息 -dir string 指定需要解析的目录 -filename string 指定哪些文件需要被解析，如果未设置默认dir路径下所有的.go文件 -generateDir string 生成文件存放的目录，默认当前路径 -generateFilename string 生成文件名，默认\u0026#34;types.go\u0026#34; -generatePkgName string 生成文件的包名，默认\u0026#34;types\u0026#34; -generateStructSuffix string 替换model struct的后缀，默认无后缀，如UserSuffix =\u0026gt; User -modelImportPath string 指明model struct的导入路径, 如my-server/models -modelStructSuffix string 指明特定后缀的model struct需要被解析，默认\u0026#34;Model\u0026#34; 工具测试结果 # ./bin/dbtools \\ -dir=./dbs/tools/testdata \\ -filename=type_model.go \\ -generatePkgName=main \\ -modelImportPath=github.com/yeqown/server-common/dbs/tools/testdata \\ 参考链接 # loader parser "},{"id":46,"href":"/2018/06/28/%E6%80%8E%E4%B9%88%E6%89%8D%E5%8F%AB%E7%86%9F%E6%82%89http%E5%8D%8F%E8%AE%AE/","title":"怎么才叫熟悉http协议?","section":"Posts","content":"“熟悉http协议”，肯定很多IT小伙伴都在招聘岗位上看得到过，但是怎么才叫熟悉http协议呢？抽空梳理了一下，也算是对这一部分知识的笔记吧！\n可能对于大部分人来说，网络web编程就是使用一些第三方库来进行请求和响应的处理，再多说一点就是这个URI要使用POST方法，对于携带的数据需要处理成为formdata。\n基础知识 # Q1: HTTP协议是什么？用来干什么？\nHTTP协议是基于TCP/IP协议的应用层协议，主要规定了客户端和服务端之间的通信格式。主要作用也就是传输数据（HTML，图片，文件，查询结果）。\n#网络分层 # 互联网的实现分成了几层，如何分层有不同的模型（七层，五层，四层），这里按五层模型来解释：\n（靠近用户）应用层 \u0026lt; 传输层 \u0026lt; 网络层 \u0026lt; 链接层 \u0026lt; 物理层（靠近硬件）\n层级 作用 拥有协议 物理层 传送电信号0 1 无 数据链路层 定义数据包;网卡MAC地址;广播的发送方式; Ethernet 802.3; Token Ring 802.5 网络层 引进了IP地址，用于区分不同的计算机是否属于同一网络 IP; ARP; RARP 传输层 建立端口到端口的通信，实现程序时间的交流，也就是socket TCP; UDP 应用层 约定应用程序的数据格式 HTTP; FTP; DNS 每一层级，都是解决问题而诞生的，也就是他们各自作用对应的问题，推荐参考资料中的“互联网协议入门”。\n#HTTP通信流程 # #拓展\u0026ndash;三次握手和四次挥手 # 经常在其他地方看到这些，一直不知道了解这部分有什么用，但是syn Flood攻击，恰恰是利用了TCP三次握手中的环节。利用假IP伪造SYN请求，服务端会多次尝试发送SYN-ACK给客户端，但是IP并不存在也就无法成功建立连接。在一定时间内伪造大量这种请求，会导致服务器资源耗尽无法为正常的连接服务。(注：服务器SYN连接数量有限制，SYN-ACK超时重传机制)\n三次握手流程：\nThe client requests a connection by sending a SYN (synchronize) message to the server. The server acknowledges this request by sending SYN-ACK back to the client. The client responds with an ACK, and the connection is established. 四次挥手流程：\nWhen an endpoint wishes to stop its half of the connection, it transmits a FIN packet. which the other end acknowledges with an ACK. Therefore, a typical tear-down requires a pair of FIN and ACK segments from each TCP endpoint. After the side that sent the first FIN has responded with the final ACK, it waits for a timeout before finally closing the connection, during which time the local port is unavailable for new connections.\nHTTP报文 # HTTP报文是由一行一行的简单字符串组成的，HTTP报文都是纯文本。\nHTTP报文包括三个部分：起始行；头部字段；主体数据。其中头部是非常重要的部分，会单独成章。\n举例：\nGET / HTTP/1.1 Host: www.baidu.com Connection: keep-alive Upgrade-Insecure-Requests: 1 User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8 Accept-Encoding: gzip, deflate, sdch Accept-Language: zh-CN,zh;q=0.8,en;q=0.6 Cookie: BAIDUID=4082549DEE5E64678FC46642E185D98C:FG=1 举例：\nHTTP/1.1 200 OK Server: bfe/1.0.8.18 Date: Thu, 03 Nov 2016 08:30:43 GMT Content-Type: text/html Content-Length: 277 Last-Modified: Mon, 13 Jun 2016 02:50:03 GMT Connection: Keep-Alive ETag: \u0026#34;575e1f5b-115\u0026#34; Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform Pragma: no-cache Accept-Ranges: bytes #状态码 # 状态代码有三位数字组成，第一个数字定义了响应的类别，共分五种类别:\n1xx：指示信息 （表示请求已接收，继续处理） 2xx：成功 （表示请求已被成功接收、理解、接受） 3xx：重定向 （要完成请求必须进行更进一步的操作） 4xx：客户端错误 （请求有语法错误或请求无法实现） 5xx：服务器端错误 （服务器未能实现合法的请求）\n这里我觉得很有必要说一下的：3xx。最近开发的时候遇到一个情况：\n从页面上post跳转到第三方页面，完成后对方会通过POST携带数据的方式返回到我们的页面。在前后端分离的开发模式下，暂时没有想到前端自行解决的方法，因此我们通过服务端来处理这个POST回调接受数据，再通过重定向的方式，跳回到我们自己的页面（只需要将处理结果：成功或者失败，通过url参数传给前端）。\n重定向的时候，随便选择了一个307（Temporary Redirect），然后返回页面的时候还是会提示：“501 Not Implemented， 意思就是：页面不支持POST请求”。解决办法是要把这个POST转成GET咯，怎么转呢？如下：\nCode Text Method handling Typical use case 302 Found GET methods unchanged.Others may or may not be changed to GET The Web page is temporarily not available for reasons that have not been unforeseen. That way, search engines don\u0026rsquo;t update their links. 303 See Other GET methods unchanged.Others changed to GET (body lost). Used to redirect after a PUT or a POST to prevent a refresh of the page that would re-trigger the operation. 307 Temporary Redirect Method and body not changed. The Web page is temporarily not available for reasons that have not been unforeseen. That way, search engines don\u0026rsquo;t update their links. Better than 302 when non-GET links/operations are available on the site. 我们需要的就是303了：\nhttp.Redirect(w, r, url, 303) // 把r(POST)重定向到url(GET) HTTP头部 # 这部分也就是常说的Header，在HTTP协议中头部主要作用为：传递额外信息。在HTTP中头部常见分类有：请求头部/响应头部/通用头部/实体头部。 这里也不细说每个请求头的作用了（反正都是搜集别人的资料～，可以参见MDN HTTP Headers），就放几个可能会有帮助的：\n#Set-Cookie [Response] # 发送cookies到客户端，客户端请求的时候带上Cookie发送给服务端，可以完成一些验证。\nSet-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt; Set-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt;; Expires=\u0026lt;date\u0026gt; Set-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt;; Max-Age=\u0026lt;non-zero-digit\u0026gt; Set-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt;; Domain=\u0026lt;domain-value\u0026gt; Set-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt;; Path=\u0026lt;path-value\u0026gt; Set-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt;; Secure Set-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt;; HttpOnly Set-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt;; SameSite=Strict Set-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt;; SameSite=Lax // Multiple directives are also possible, for example: Set-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt;; Domain=\u0026lt;domain-value\u0026gt;; Secure; HttpOnly #Access-Control-Allow-Origin [Repsonse] # 允许哪个域可以访问你的资源\nAccess-Control-Allow-Origin: * Access-Control-Allow-Origin: \u0026lt;origin\u0026gt; #Access-Control-Allow-Credentials [Response] # 跨域请求必设置\nThe Access-Control-Allow-Credentials header works in conjunction with the XMLHttpRequest.withCredentials property or with the credentials option in the Request() constructor of the Fetch API. Credentials must be set on both sides (the Access-Control-Allow-Credentials header and in the XHR or Fetch request) in order for the CORS request with credentials to succeed.\nAccess-Control-Allow-Credentials: true #Access-Control-Allow-Methods [Response] # 响应头指定访问资源以响应预检请求时允许的方法。\nAccess-Control-Allow-Methods: \u0026lt;method\u0026gt;, \u0026lt;method\u0026gt;, ... #Access-Control-Allow-Headers [Response] # （预检请求）用于指明在实际请求中可以使用哪些HTTP头。\nAccess-Control-Allow-Headers: \u0026lt;header-name\u0026gt;, \u0026lt;header-name\u0026gt;, ... #Content-Type [Response/Request] # 用于表明资源是哪种格式\nContent-Type: text/html; charset=utf-8 Content-Type: multipart/form-data; boundary=something HTTP Methods # 方法 描述 使用场景 请求是否有body 响应是否有body GET 获取指定资源 获取网页，查询资源 没有 有 POST 发送数据给服务端 新建资源；用户通过表单登录 有 有 PUT 新建资源或者替换指定资源 更新一条记录 有 有 DELETE 删除指定资源 删除一条记录； 有 有 OPTIONS 用于描述特定资源的访问选项 获取服务端支持的请求方法 没有 有 CONNECT 开启双向通信 undefined 没有 有 HEAD 描述 使用场景 没有 没有 PATCH 对资源部分更改 使用场景 有 没有 TRACE 描述 使用场景 有 有 #GET和POST区别 # 被人熟知的区别有以下几点：\nGET后退按钮/刷新无害，POST数据会被重新提交（浏览器应该告知用户数据会被重新提交）。 GET能被缓存，POST大部分不缓存。 GET编码类型application/x-www-form-url，POST编码类型encodedapplication/x-www-form-urlencoded 或 multipart/form-data。为二进制数据使用多重编码。 GET对数据长度有限制，当发送数据时，GET方法向URL添加数据；URL 的长度是受限制的（URL的最大长度是2048个字符。POST无限制。 GET只允许ASCII字符。POST没有限制。也允许二进制数据。 与 POST 相比，GET的安全性较差，因为所发送的数据是URL的一部分。 上述是从表象来说两者有什么区别，那么从方法语义呢？HTTP协议中GET和POST的区别好吧，其实就是说：GET请求对于资源不应该产生影响，POST\t请求会造成资源变化，且多次请求变化不固定（非幂等）。\n#PUT和POST区别 # 同一个PUT调用多次，不对产生其他影响，返回结果一致，资源变化一致。但是多次提交相同的POST可能会有不一样的响应，根据设计的不同，服务端可能提示：资源重复，或者新增相同的资源多次。也就是说PUT幂等，POST非幂等。\n总结 # 上述只能算HTTP协议的小部分知识，其包含及相关知识还有很多，如基本的HTTP鉴权，HTTPS的工作流程，HTTP如何基于TCP/IP协议来实现的。\n参考资料 # 来一场轰轰烈烈的HTTP协议扫盲 HTTP协议入门 互联网协议入门 MDN HTTP Headers HTTP协议中GET和POST的区别 "},{"id":47,"href":"/2018/06/08/api-gateway%E4%B8%AD%E5%AE%9E%E7%8E%B0%E5%9F%BA%E4%BA%8E%E6%9D%83%E9%87%8D%E7%9A%84%E8%BD%AE%E8%AF%A2%E8%B0%83%E5%BA%A6/","title":"api-gateway中实现基于权重的轮询调度","section":"Posts","content":" 背景和目标 # 背景 # 项目需要在现有项目的基础上实现权限系统，但为了低耦合，选择实现了一个基于ne7ermore/gRBAC的auth-server，用于实现权限，角色，用户的管理，以及提供鉴权服务。在开发环境对接没有问题，正常的鉴权访问。到了线上部署的时候，才发现：\n线上某服务部署在多台机器上; 目前的api-gateway并不支持同一服务配置多个node; 想的办法有：\n序号 描述 优点 缺点 1 api-gateway通过url来转发请求，之前是配置IP加端口 api-gateway改动小 影响web和APP升级 2 api-gateway能支持多台机器，并进行调度 api-gateway功能更强大，把以后要做的事情提前做好基础 好像没啥缺点，只是费点时间支持下多节点配置，并调度 如果没说清，请看下图：\n目标 # 那么，目标也就明确了，需要实现api-gateway中实现基于权重的调度。为啥要基于权重？其一是仿照nginx基于权重的负载均衡，其二是服务器性能差异。\n轮询调度算法介绍 # 轮询调度算法: # 轮询调度算法的原理是每一次把来自用户的请求轮流分配给内部中的服务器，从1开始，直到N(内部服务器个数)，然后重新开始循环。该算法的优点是其简洁性，它无需记录当前所有连接的状态，所以它是一种无状态调度。\n假设有一组服务器N台，S = {S1, S2, …, Sn}，一个指示变量i表示上一次选择的服务器ID。变量i被初始化为N-1。其算法如下：\nj = i; do { j = (j + 1) mod n; i = j; return Si; } while (j != i); return NULL; 平滑加权轮询调度算法： # 上述的轮询调度算法，并没有考虑服务器性能的差异，实际生产环境中，每一台服务器配置和安装的业务并不一定相同，处理能力不完全一样。因此需要根据服务器能力，分配不同的权值，以免服务的超负荷和过分闲余。\n假设有一组服务器S = {S0, S1, …, Sn-1}, 其算法如下：\n// i表示上一次选择的服务器，变量i初始化为-1 // W(Si)表示服务器Si的权值 // cw表示当前调度的权值 cw初始化为0 // max(S)表示集合S中所有服务器的最大权值 // gcd(S)表示集合S中所有服务器权值的最大公约数 while (true) { i = (i + 1) mod n; if (i == 0) { cw = cw - gcd(S); if (cw \u0026lt;= 0) { cw = max(S); if (cw == 0) return NULL; } } if (W(Si) \u0026gt;= cw) return Si; } Nginx基于权重的轮询算法的实现可以参考它的一次代码提交： Upstream: smooth weighted round-robin balancing。\n它不但实现了基于权重的轮询算法，而且还实现了平滑的算法。所谓平滑，就是在一段时间内，不仅服务器被选择的次数的分布和它们的权重一致，而且调度算法还比较均匀的选择服务器，而不会集中一段时间之内只选择某一个权重比较高的服务器。如果使用随机算法选择或者普通的基于权重的轮询算法，就比较容易造成某个服务集中被调用压力过大。\n举个例子，比如权重为{a:5, b:1, c:1)的一组服务器，Nginx的平滑的轮询算法选择的序列为{ a, a, b, a, c, a, a },这显然要比{ c, b, a, a, a, a, a }序列更平滑，更合理，不会造成对a服务器的集中访问。\n在参考文献平滑的基于权重的调度算法中，作者给出了仿照nginxd的平滑权重轮询调度算法的Golang版本代码，还有一个weighted库自荐。这里做一个延伸～\n贴一下测试测试代码及结果：\nfunc Test_Init(t *testing.T) { servers := []*conf.ProxyServerConfig{ { Schema: \u0026#34;http://\u0026#34;, Host: \u0026#34;127.0.0.1\u0026#34;, Prefix: \u0026#34;/api\u0026#34;, Port: 8808, Weight: 4, }, { Schema: \u0026#34;http://\u0026#34;, Host: \u0026#34;127.0.0.1\u0026#34;, Prefix: \u0026#34;/api\u0026#34;, Port: 8808, Weight: 4, }, { Schema: \u0026#34;http://\u0026#34;, Host: \u0026#34;127.0.0.1\u0026#34;, Prefix: \u0026#34;/api\u0026#34;, Port: 8808, Weight: 2, }, } bla := InitBalancer(servers) cnt := 0 mark := map[int]int{} for cnt \u0026lt; 10 { idx := bla.Distribute() t.Log(idx) mark[idx]++ cnt++ } t.Log(mark) } // === RUN Test_Init // --- PASS: Test_Init (0.00s) // balance_test.go:45: 0 // balance_test.go:45: 1 // balance_test.go:45: 0 // balance_test.go:45: 1 // balance_test.go:45: 2 // balance_test.go:45: 0 // balance_test.go:45: 1 // balance_test.go:45: 0 // balance_test.go:45: 1 // balance_test.go:45: 2 // balance_test.go:50: map[0:4 1:4 2:2] // 在10次调度中，三个服务分别被调用了响应权重的次数，且顺序并不是[0,0,0,0,1,1,1,1,2,2] 调度器实现 # 算法讲完了，我就直接上代码了。\ntype Balancer struct { serverWeights map[int]int // host and weight maxWeight int // 0 maxGCD int // 1 lenOfSW int // 0 i int //表示上一次选择的服务器, -1 cw int //表示当前调度的权值, 0 } // 根据服务配置初始化调度器 // conf.ProxyServerConfig 配置可以参见上述测试代码 func InitBalancer(servers []*conf.ProxyServerConfig) *Balancer { bla := \u0026amp;Balancer{ serverWeights: make(map[int]int), maxWeight: 0, maxGCD: 1, lenOfSW: len(servers), i: -1, cw: 0, } _tmp_gcd := make([]int, 0, bla.lenOfSW) for idx, psc := range servers { bla.serverWeights[idx] = psc.Weight if psc.Weight \u0026gt; bla.maxWeight { bla.maxWeight = psc.Weight } _tmp_gcd = append(_tmp_gcd, psc.Weight) } // 求最大公约数 bla.maxGCD = nGCD(_tmp_gcd, bla.lenOfSW) return bla } // 调度 func (bla *Balancer) Distribute() int { for true { bla.i = (bla.i + 1) % bla.lenOfSW if bla.i == 0 { bla.cw = bla.cw - bla.maxGCD if bla.cw \u0026lt;= 0 { bla.cw = bla.maxWeight if bla.cw == 0 { return 0 } } } if bla.serverWeights[bla.i] \u0026gt;= bla.cw { return bla.i } } return 0 } // 求最大公约数 func GCD(a, b int) int { if a \u0026lt; b { a, b = b, a // swap a \u0026amp; b } if b == 0 { return a } return GCD(b, a%b) } // N个数的最大公约数 func nGCD(data []int, n int) int { if n == 1 { return data[0] } return GCD(data[n-1], nGCD(data, n-1)) } 嵌入Api-Gateway # 已经实现了调度器，并测试妥当，那么如何实际放到网关服务中去呢？我先简单描述下，之前的服务是如何调用的。在Golang标准库里有一个东西叫做：\nhttputil.NewSingleHostReverseProxy 文档由此去\n这就是本网关中主要用于请求转发的关键。在转发请求之前，我需要做的事就是：匹配本次请求应该转发给哪一个服务？大家都通过：api.xxx.com的域名来访问，但是对于不同的功能会有不同的路径前缀如：\u0026quot;/api\u0026quot;, \u0026ldquo;/auth\u0026rdquo;, \u0026ldquo;/res\u0026quot;等等。匹配完成，那么问题来了：我要转发给的服务有多个，咋个调度呢？这个位置就是我需要把调度器嵌入的位置，上代码：\ntype ProxyServer struct { // ... some other data Servers map[string][]*conf.ProxyServerConfig Balancer map[string]*Balancer } func (ps *ProxyServer) InitBalancer() { for srv_name, srvs := range ps.Servers { ps.Balancer[srv_name] = InitBalancer(srvs) } } // 匹配需要转发的前缀 func (ps *ProxyServer) MatchProxy(path string, req *http.Request) (error, *url.URL) { for srv_name, srvs := range ps.Servers { // default set 0, to choose srvs to service srv := srvs[0] if strings.HasPrefix(path, srv.Prefix) { // load balance srv_idx := ps.Balancer[srv_name].Distribute() srv = srvs[srv_idx] _url := utils.Fstring(\u0026#34;%s%s:%d\u0026#34;, srv.Schema, srv.Host, srv.Port) remote, err := url.Parse(_url) if err != nil { return err, nil } req.URL.Path = strings.TrimPrefix(path, srv.Prefix) return nil, remote } } return errors.New(\u0026#34;did not match any proxy server\u0026#34;), nil } 至此，也就完成了在api-gateway中嵌入基于权重的轮询调度。\n注 # 用词表述不当，还望谅解并及时告知。其次没有过多的分析算法是如何设计的，这并非我所擅长，也不是本文的重点～ 以上\n参考文献 # 轮询调度算法(Round-Robin Scheduling) golang实现权重轮询调度算法 平滑的基于权重的调度算法 "},{"id":48,"href":"/2018/05/18/%E4%BD%BF%E7%94%A8golang-%E5%AE%9E%E7%8E%B0JSON-RPC2-0/","title":"使用golang 实现JSON-RPC2.0","section":"Posts","content":" 什么是RPC？ # 远程过程调用（英语：Remote Procedure Call，缩写为 RPC）是一个计算机通信协议。该协议允许运行于一台计算机的程序调用另一台计算机的子程序，而程序员无需额外地为这个交互作用编程。如果涉及的软件采用面向对象编程，那么远程过程调用亦可称作远程调用或远程方法调用。\n远程过程调用是一个分布式计算的客户端-服务器（Client/Server）的例子，它简单而又广受欢迎。远程过程调用总是由客户端对服务器发出一个执行若干过程请求，并用客户端提供的参数。执行结果将返回给客户端。由于存在各式各样的变体和细节差异，对应地派生了各式远程过程调用协议，而且它们并不互相兼容。——————源自维基百科\n什么又是JSON-RPC? # JSON-RPC，是一个无状态且轻量级的远程过程调用（RPC）传送协议，其传递内容通过 JSON 为主。相较于一般的 REST 通过网址（如 GET /user）调用远程服务器，JSON-RPC 直接在内容中定义了欲调用的函数名称（如 {\u0026ldquo;method\u0026rdquo;: \u0026ldquo;getUser\u0026rdquo;}），这也令开发者不会陷于该使用 PUT 或者 PATCH 的问题之中。 更多JSON-RPC约定参见：https://zh.wikipedia.org/wiki/JSON-RPC\n问题 # 服务端注册及调用 # 约定如**net/rpc**：\nthe method\u0026rsquo;s type is exported. the method is exported. the method has two arguments, both exported (or builtin) types. the method\u0026rsquo;s second argument is a pointer. the method has return type error. // 这就是约定 func (t *T) MethodName(argType T1, replyType *T2) error 那么问题来了:\n问题1: Server怎么来注册`t.Methods`? 问题2: JSON-RPC请求参数里面的Params给到args? server端类型定义：\ntype methodType struct { method reflect.Method // 用于调用 ArgType reflect.Type ReplyType reflect.Type } type service struct { name string // 服务的名字, 一般为`T` rcvr reflect.Value // 方法的接受者, 即约定中的 `t` typ reflect.Type // 注册的类型, 即约定中的`T` method map[string]*methodType // 注册的方法, 即约定中的`MethodName`的集合 } // Server represents an RPC Server. type Server struct { serviceMap sync.Map // map[string]*service } 解决问题1，参考了net/rpc中的注册调用。主要使用reflect这个包。代码如下：\n// 解析传入的类型及相应的可导出方法，将rcvr的type，Methods的相关信息存放到Server.m中。 // 如果type是不可导出的，则会报错 func (s *Server) Register(rcvr interface{}) error { _service := new(service) _service.typ = reflect.TypeOf(rcvr) _service.rcvr = reflect.ValueOf(rcvr) sname := reflect.Indirect(_service.rcvr).Type().Name() if sname == \u0026#34;\u0026#34; { err_s := \u0026#34;rpc.Register: no service name for type \u0026#34; + _service.typ.String() log.Print(err_s) return errors.New(err_s) } if !isExported(sname) { err_s := \u0026#34;rpc.Register: type \u0026#34; + sname + \u0026#34; is not exported\u0026#34; log.Print(err_s) return errors.New(err_s) } _service.name = sname _service.method = suitableMethods(_service.typ, true) // sync.Map.LoadOrStore if _, dup := s.m.LoadOrStore(sname, _service); dup { return errors.New(\u0026#34;rpc: service already defined: \u0026#34; + sname) } return nil } // 关于suitableMethods，也是使用reflect， // 来获取_service.typ中的所有可导出方法及方法的相关参数，保存成*methodType suitableMethods代码由此去：[https: //github.com/yeqown/rpc/blob/master/server.go#L105](https: //github.com/yeqown/rpc/blob/master/server.go#L105)\n解决问题2，要解决问题2，且先看如何调用Method，代码如下：\n// 约定： func (t *T) MethodName(argType T1, replyType *T2) error // s.rcvr： 即约定中的 t // argv： 即约定中的 argType // replyv： 即约定中的 replyType func (s *service) call(mtype *methodType, req *Request, argv, replyv reflect.Value) *Response { function := mtype.method.Func returnValues := function.Call([]reflect.Value{s.rcvr, argv, replyv}) errIter := returnValues[0].Interface() errmsg := \u0026#34;\u0026#34; if errIter != nil { errmsg = errIter.(error).Error() return NewResponse(req.ID, nil, NewJsonrpcErr(InternalErr, errmsg, nil)) } return NewResponse(req.ID, replyv.Interface(), nil) } 看了如何调用，再加上JSON-RPC的约定，知道了传给服务端的是一个JSON，而且其中的Params是一个json格式的数据。那就变成了:interface{} - req.Params 到reflect.Value - argv。那么怎么转换呢？看代码：\nfunc (s *Server) call(req *Request) *Response { // .... // 根据req.Method来查询method // req.Method 格式如：\u0026#34;ServiceName.MethodName\u0026#34; // mtype *methodType mtype := svc.method[methodName] // 根据注册时候的mtype.ArgType来生成一个reflect.Value argIsValue := false // if true, need to indirect before calling. var argv reflect.Value if mtype.ArgType.Kind() == reflect.Ptr { argv = reflect.New(mtype.ArgType.Elem()) } else { argv = reflect.New(mtype.ArgType) argIsValue = true } if argIsValue { argv = argv.Elem() } // 为argv参数生成了一个reflect.Value,但是argv目前为止都还是是0值。 // 那么怎么把req.Params 复制给argv ? // 我尝试过，argv = reflect.Value(req.Params)，但是在调用的时候 会提示说：“map[string]interface{} as main.*Args”， // 这也就是说，并没有将参数的值正确的赋值给argv。 // 后面才又了这个convert函数: // bs, _ := json.Marshal(req.Params) // json.Unmarshal(bs, argv.Interface()) // 因此有一些限制～，就不多说了 convert(req.Params, argv.Interface()) // Note: 约定中ReplyType是一个指针类型，方便赋值。 // 根据注册时候的mtype.ReplyType来生成一个reflect.Value replyv := reflect.New(mtype.ReplyType.Elem()) switch mtype.ReplyType.Elem().Kind() { case reflect.Map: replyv.Elem().Set(reflect.MakeMap(mtype.ReplyType.Elem())) case reflect.Slice: replyv.Elem().Set(reflect.MakeSlice(mtype.ReplyType.Elem(), 0, 0)) } return svc.call(mtype, req, argv, replyv) } 支持HTTP调用 # 已经完成了上述的部分，再来谈支持HTTP就非常简单了。实现http.Handler接口就行啦～。如下：\n// 支持之POST \u0026amp; json func (s *Server) ServeHTTP(w http.ResponseWriter, r *http.Request) { var resp *Response w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) if r.Method != http.MethodPost { resp = NewResponse(\u0026#34;\u0026#34;, nil, NewJsonrpcErr( http.StatusMethodNotAllowed, \u0026#34;HTTP request method must be POST\u0026#34;, nil), ) response(w, resp) return } // 解析请求参数到[]*rpc.Request reqs, err := getRequestFromBody(r) if err != nil { resp = NewResponse(\u0026#34;\u0026#34;, nil, NewJsonrpcErr(InternalErr, err.Error(), nil)) response(w, resp) return } // 处理请求，包括批量请求 resps := s.handleWithRequests(reqs) if len(resps) \u0026gt; 1 { response(w, resps) } else { response(w, resps[0]) } return } 使用示例 # 服务端使用 # // test_server.go package main import ( // \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;github.com/yeqown/rpc\u0026#34; ) type Int int type Args struct { A int `json:\u0026#34;a\u0026#34;` B int `json:\u0026#34;b\u0026#34;` } func (i *Int) Sum(args *Args, reply *int) error { *reply = args.A + args.B return nil } type MultyArgs struct { A *Args `json:\u0026#34;aa\u0026#34;` B *Args `json:\u0026#34;bb\u0026#34;` } type MultyReply struct { A int `json:\u0026#34;aa\u0026#34;` B int `json:\u0026#34;bb\u0026#34;` } func (i *Int) Multy(args *MultyArgs, reply *MultyReply) error { reply.A = (args.A.A * args.A.B) reply.B = (args.B.A * args.B.B) return nil } func main() { s := rpc.NewServer() mine_int := new(Int) s.Register(mine_int) go s.HandleTCP(\u0026#34;127.0.0.1:9999\u0026#34;) // 开启http http.ListenAndServe(\u0026#34;:9998\u0026#34;, s) } 客户端使用 # // test_client.go package main import ( \u0026#34;github.com/yeqown/rpc\u0026#34; ) type Args struct { A int `json:\u0026#34;a\u0026#34;` B int `json:\u0026#34;b\u0026#34;` } type MultyArgs struct { A *Args `json:\u0026#34;aa\u0026#34;` B *Args `json:\u0026#34;bb\u0026#34;` } type MultyReply struct { A int `json:\u0026#34;aa\u0026#34;` B int `json:\u0026#34;bb\u0026#34;` } func main() { c := rpc.NewClient() c.DialTCP(\u0026#34;127.0.0.1:9999\u0026#34;) var sum int c.Call(\u0026#34;1\u0026#34;, \u0026#34;Int.Sum\u0026#34;, \u0026amp;Args{A: 1, B: 2}, \u0026amp;sum) println(sum) c.DialTCP(\u0026#34;127.0.0.1:9999\u0026#34;) var reply MultyReply c.Call(\u0026#34;2\u0026#34;, \u0026#34;Int.Multy\u0026#34;, \u0026amp;MultyArgs{A: \u0026amp;Args{1, 2}, B: \u0026amp;Args{3, 4}}, \u0026amp;reply) println(reply.A, reply.B) } 运行截图 # 实现 # 上面只挑了我觉得比较重要的部分，讲了实现，更多如客户端的支持，JSON-RPC的请求响应定义，可以在项目中里查阅。目前基于TCP和HTTP实现了JSON-RPC，项目地址：github.com/yeqown/rpc\n缺陷 # 只支持JSON-RPC, 且还没有完全实现JSON-RPC的约定。譬如批量调用中：\n若批量调用的 RPC 操作本身非一个有效 JSON 或一个至少包含一个值的数组，则服务端返回的将单单是一个响应对象而非数组。若批量调用没有需要返回的响应对象，则服务端不需要返回任何结果且必须不能返回一个空数组给客户端。\n阅读参考中的两个RPC，发现两者都是使用的codec的方式来提供扩展。因此以后可以考虑使用这种方式来扩展。\n参考 # net/rpc grollia/rpc "},{"id":49,"href":"/2018/05/18/%E8%87%AA%E5%B7%B1%E5%86%99%E4%B8%80%E4%B8%AA%E6%89%8B%E6%9C%BA%E8%8F%9C%E8%B0%B1APP/","title":"自己写一个手机菜谱APP","section":"Posts","content":"需要的技术及工具：\nPython3 + Selenuium Golang net/http React-Native 相关（使用了react-navigation） MongoDB Redis 代码地址：\ngithub.com/yeqown/recipe 项目构思及构成 # 食谱类型的App，应用市场肯定有更好的的食谱APP。所以自己开发的目的，首先是写代码，其次是定制APP～ 好的，现在化身产品经理，设计一下APP有哪些功能：\n每日菜谱推荐，推荐可更换 每天需要准备的材料提醒 发现更多菜谱 分类筛选菜谱 搜索菜谱 查看菜谱详情 设置（不知道设置啥，提前准备吧） 设计稿？不存在的，随心所欲。\n现在分析下我需要做的事情：\n能跑起来的APP，与restful web api 交互。 能跑起来的web-api，提供菜谱数据，筛选，推荐，搜索等功能 能跑起来的简易spider，从网上获取菜谱信息。（这个爬虫能解析动态生成网站就够用了，姑且称之为爬虫吧） 没有考虑大量数据，因此爬虫并不通用，只适合特定XX网站。\n实战爬虫 # 这个APP里面最重要的就是菜谱数据了，那么开发之前，需要明确的数据格式，如下：\n{ \u0026#34;name\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;cat\u0026#34;: \u0026#34;cat\u0026#34;, \u0026#34;img\u0026#34;: \u0026#34;img_url\u0026#34;, \u0026#34;mark_cnt\u0026#34;: 19101, \u0026#34;view_cnt\u0026#34;: 181891, \u0026#34;setps\u0026#34;: [ { \u0026#34;desc\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;img\u0026#34;: \u0026#34;\u0026#34;, }, // more step ], \u0026#34;material\u0026#34;: { \u0026#34;ingredients\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;ingredients_name\u0026#34;, \u0026#34;weight\u0026#34;: \u0026#34;ingredients_weight\u0026#34;, }, // more ingredients ], \u0026#34;seasoning\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;seasoning_name\u0026#34;, \u0026#34;weight\u0026#34;: \u0026#34;seasoning_weight\u0026#34;, }, // more seasoning ], }, \u0026#34;create_time\u0026#34;: \u0026#34;2018xxxxxx\u0026#34;, \u0026#34;update_time\u0026#34;: \u0026#34;2018xxxxxx\u0026#34;, } 目标 # 前提：无法直接获取到该网站的服务API，才使用爬虫间接获取数据。\n目标是某个网站的菜谱，网页层次和结构比较明确，信息也正好。结合需要的数据量级和信息，因此爬虫遵循了以下流程：\nstep1. 获取特定分类的入口网址 step2. 从分类的入口网址获取该分类的的所有菜谱详情网址 step3. 从单个菜谱详情网址中解析菜谱数据。 使用了redis作为爬虫间的数据通道，主要是为了方便开多个进程爬取。\n问题 # 翻页和判定当前页为最后一页 # 描述：某些分类下不止一页数据，如何进行翻页和下一页的判定。 原因：无 解决：点击翻页按钮，观察url的变化（30 * (page-1)）；直接跳转到最后一页，观察页面特征（无“下一页”按钮） 爬取动态页面 # 描述：菜谱详情页面是动态生成的，无法直接使用bs4 + requests来爬取。 原因：无 解决：查看了requests获取到的页面结构，发现在script标签中，有需要的数据，但是需要额外处理，而且某些数据信息缺失。因此 开始考虑selenium来加载页面，解析菜谱信息。 实战手机App # 为了方便高效，没有选用原生的的开发方式，而是选择了React-Native来撸App。App主要运行于Android，吾等吊丝用不起iPhone）。\n运行截图 # 问题 # 怎么用React-Native来撸一个App的步骤，这里就不多说了。主要记录一下这期间遇到的问题：\n问题一：无法访问本地的API服务 # 描述：开发过程中遇到了使用`fetch\\axios`均无法访问`localhost`api服务的问题，错误提示“network request failed”，本地的api服务也确实没有接收到任何请求。但是尝试访问其他域名api却可以正常。 原因：React-Native把自己当成了localhost，因此localhost并没有请求的到真的api服务。 解决：将localhost替换为`10.0.2.2`便定向到了本地。 问题二：TabNavigator无法使用icon # 描述：根据官方文档来设置Icon，一直报错，Icon也看不见。 原因：不晓得。 解决：在TabNavigator注册的各个Screen中指定。 TabNavigator定义参见：https://github.com/yeqown/recipe/blob/master/recipes-mobile/index.js#L18 指定代码：https://github.com/yeqown/recipe/blob/master/recipes-mobile/src/Home.js#L34\n实战web-api # 代码由此去\n这一部分没有啥特别的地方，就略过了。在这里推销一下: github.com/yeqown/gweb一个简易封装的golang web 框架。\n"},{"id":50,"href":"/2018/05/03/docker-jenkins-golang%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%E5%AE%9E%E8%B7%B5/","title":"docker+jenkins+golang持续集成实践","section":"Posts","content":" 起因 # 因为生产需要最近又重新折腾了一下Jenkins和docker。主要目的是想自动编译，打包，部署一些Golang的HttpServer。于是决定使用Jenkins来做这个持续集成的载体，选择Jenkins出于两点原因：\n1. 以前就使用过，上手会更快 2. 社区比较成熟，插件和文档丰富\n安装Docker和Pull Jenkins镜像 # 这一步，作为前置条件且不是本文主要要描述的步骤，因此略去。网上也有很多参考资料～\nJenkins \u0026amp; docker-compose配置 # 为了方便我才用了docker-compose这个工具，docker-compose 基础可以参见我的docker-compose上手。这里直接上配置：\nversion: \u0026#39;2\u0026#39; services: jenkins: container_name: jenkins-lts ports: - 9001:8080 - 50000:50000 image: jenkins/jenkins:lts volumes: - /home/worker/jenkins/jenkins_home:/var/jenkins_home 配置也是官方的示例配置。\nNote: 将宿主机的/home/worker/jenkins/jenkins_home挂载为容器的/var/jenkins_home目录。这样做的目的是，如果容器被不小心删除也不至于Jenkins的数据丢失。\n到这里，我们只需要执行docker-compose up -d便可以将Jenkins容器跑起来了，再配置一下Nginx，便可以直接访问到Jenkins页面了，并进行初始化。\n我的目录结构如下：\n➜ jenkins ll total 8.0K -rw-rw-r-- 1 worker worker 220 May 2 17:19 docker-compose.yml drwxrwxr-x 19 worker worker 4.0K May 3 15:53 jenkins_home ➜ jenkins pwd /home/worker/jenkins ➜ jenkins docker-compose up -d # 运行 Publish Over SSH配置 # Publish Over SSH配置，由于我们是通过docker运行的Jenkins，因此要特别配置一下SSH，方便Jenkins部署项目。这里先列出步骤：\n安装Publish Over SSH 容器内ssh-keygen 配置Publish Over SSH 配置Git仓库部署公钥 #这一步相当于拉取代码的Credentials Golang Build-env # 因为默认的Jenkins镜像是不带有Go的编译工具的，所以我们很有必要安装一个Go插件Go-Plugin-Jenkins 具体步骤如下：\n安装JenkinsGo插件 在全局工具配置中，安装Go 在对应任务配置-\u0026gt; 构建环境项，选择Go版本 这一步的详细步骤可以在参考文献第一条中查询Setup Go Build Environment\nNote: 官方文档中说全局配置Go是在系统设置中进行的，但是我用的Jenkins ver. 2.107.2，这一项配置是在全局工具配置中。\n如何打包部署 # 编写一个makefile来打包项目，通过scp来分发部署服务，这也是配置Publish Over SSH的目的。\n由于Jenkins镜像不带有make相关工具（甚至Vim也没有），所以需要安装Make及其相关工具。与此同时，如果项目使用了相关的依赖管理工具，因此还有必要去配置相关的GO环境变量。其中Go的安装路径在： /var/jenkins_home/tools/org.jenkinsci.plugins.golang.GolangInstallation/$GOVERSION中。\n这里贴上我的Makefile\n# To test, build, deploy offline-tasks # -: ignore this commnad error # @: no display current commnad to std output # Commnads declare GOCMD=go GOTEST=$(GOCMD) test GOBUILD=$(GOCMD) build # Params define MAIN_PATH=../main PACKAGE_PATH=../package PACKAGE_BIN_PATH=../package/bin BIN=offline-tasks FILENAME=offline-tasks.tar.gz # Deploy Params DEV_HOST=zy-dev DEV_TAR_PATH=/home/worker/project/offline-tasks PROD_HOST=zy-pro2 PROD_TAR_PATH=/home/worker/project/offline-tasks default: build pack test: # testing - $(GOTEST) ../... -v build: # building mkdir $(PACKAGE_PATH) mkdir $(PACKAGE_BIN_PATH) cd $(MAIN_PATH) \u0026amp;\u0026amp; $(GOBUILD) -o $(BIN) mv \u0026#34;$(MAIN_PATH)/$(BIN)\u0026#34; $(PACKAGE_BIN_PATH) cp -r \u0026#34;../configs\u0026#34; $(PACKAGE_PATH) cp \u0026#34;../sh/start.sh\u0026#34; $(PACKAGE_BIN_PATH) pack: # packing cd $(PACKAGE_PATH) \u0026amp;\u0026amp; tar -zcvf ../$(FILENAME) ./* mv ../$(FILENAME) $(PACKAGE_PATH) ################################################## # # # deploy: from zy-dev to execute # # deploy-dev: from dev-CI to execute # # deploy-prod: from prod-CI to execute # # # ################################################## deploy: clean build pack # deploy dev from dev cp $(PACKAGE_PATH)/$(FILENAME) $(DEV_TAR_PATH) cd $(DEV_TAR_PATH) \u0026amp;\u0026amp; tar zxvf $(FILENAME) \u0026amp;\u0026amp; supervisorctl -c configs/dev.supervisord.conf restart offline-tasks deploy-dev: clean build pack # deploy-dev from CI scp $(PACKAGE_PATH)/$(FILENAME) $(DEV_HOST):$(DEV_TAR_PATH) ssh $(DEV_HOST) \u0026#34;cd $(DEV_TAR_PATH) \u0026amp;\u0026amp; tar zxvf $(FILENAME) \u0026amp;\u0026amp; supervisorctl -c configs/dev.supervisord.conf restart offline-tasks\u0026#34; deploy-prod: clean build pack # deploying prod from dev or CI scp $(PACKAGE_PATH)/$(FILENAME) $(PROD_HOST):$(PROD_TAR_PATH) ssh $(PROD_HOST) \u0026#34;cd $(PROD_TAR_PATH) \u0026amp;\u0026amp; tar zxvf $(FILENAME) \u0026amp;\u0026amp; supervisorctl -c configs/prod.supervisord.conf restart offline-tasks\u0026#34; clean: # cleaning rm -fr $(PACKAGE_PATH) rm -fr ../$(FILENAME) 总结 # 进过上述的一系列操作之后，只剩下一个比较尴尬的问题了：如果Go代码仓库中vendor不带有依赖项目，那么获取依赖的动作就要自己手动来操作了～。或许可以在makefile中新增一个deps，如下：\n# default set $CURDIR=\u0026#34;$PROJ_ROOT/sh\u0026#34; # preparing works... GVT_RESTORE=gvt restore PROJ_ROOT=../ deps: cd ($PROJ_ROOT) \u0026amp;\u0026amp; $(GVT_RESTORE) build: deps # building mkdir $(PACKAGE_PATH) mkdir $(PACKAGE_BIN_PATH) cd $(MAIN_PATH) \u0026amp;\u0026amp; $(GOBUILD) -o $(BIN) mv \u0026#34;$(MAIN_PATH)/$(BIN)\u0026#34; $(PACKAGE_BIN_PATH) cp -r \u0026#34;../configs\u0026#34; $(PACKAGE_PATH) cp \u0026#34;../sh/start.sh\u0026#34; $(PACKAGE_BIN_PATH) # other commands... 并且加deps命令，加build命令中，每次打包都检查一下依赖。\n参考资料 # https://zpjiang.me/2017/08/09/Setup-Jenkins-for-Go-Projects/ https://wiki.jenkins.io/display/JENKINS/Go+Plugin "},{"id":51,"href":"/2018/04/20/gorm%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/","title":"gorm使用记录","section":"Posts","content":" 关于Gorm # gorm文档\n遇见问题 # 无法通过结构体的方式更新或查询零值 # 这里零值是说，各个类型的默认值。 关于这一点是在这里中注明了的，也提供了解决方案：\nWARNING when update with struct, GORM will only update those fields that with non blank value\nFor below Update, nothing will be updated as \u0026ldquo;\u0026rdquo;, 0, false are blank values of their types\nNOTE When query with struct, GORM will only query with those fields has non-zero value, that means if your field’s value is 0, \u0026lsquo;\u0026rsquo;, false or other zero values, it won’t be used to build query conditions,\nYou could consider to use pointer type or scanner/valuer to avoid this.\n// Use pointer value type User struct { gorm.Model Name string Age *int } // Use scanner/valuer type User struct { gorm.Model Name string Age sql.NullInt64 } 同样适用其他形式的数据来进行更新也是可以，如：\nupdateData := map[string]interface{}{ \u0026#34;age\u0026#34;: 0, \u0026#34;update_time\u0026#34;: time.Now(), } db.Model(\u0026amp;User{}).Update(updateData) 同一个连接无法多次查询 # 在实际使用过程中遇到的情况跟https://github.com/jinzhu/gorm/issues/1574描述类似，但是回复中的解决办法并不能帮我解决问题。\n我的代码如下：\n// mysql.go var myDB *gorm.DB func GetMyDB() *gorm.DB { return myDB } func ConnMysql() { //... db, err := gorm.Open(...) //... myDB = db } 使用\n// models/user.go type User struct { Name string `gorm:\u0026#34;column:name\u0026#34;` Age int `gorm:\u0026#34;column:age\u0026#34;` //... } type UserColl struct { *gorm.DB } func NewUserColl() *UserColl{ return \u0026amp;UserColl{ DB: GetMyDB().Model(\u0026amp;User{}) } } func FindUserWithName(name string) (error, *User) { uc := NewUserColl() u := new(User) if err := uc.Where(\u0026#34;name = ?\u0026#34;, name).First(u).Error; err != nil { return err, nil } return nil, u } // 无法正常使用的函数 func FindUsersWithNames(names []string) (map[string]error, []*User) { uc := NewUserColl() us := make([]*User, len(names)) errs := map[string]error{} for _, name := range names { u := new(User) if err := uc.Where(\u0026#34;name = ?\u0026#34;, name).First(u).Error; err != nil { map[name] = err continue } us = append(us, u) } return errs, us } 上述的FindUsersWithNames在使用过程中,多个查询只有第一个查询是正常的，其他的会报错：“record not found”，但其实数据是存在的。\n进过检索我发现了，在gorm实现中，db是复用的，因此多次查询会在已经存在的结果中查询，那么找不到就能解释了。那么如何解决呢？issue#1574中提到了重新申请一个connection能解决这个问题。因此我尝试了一下更改：\n// 更改1 - 无效 func FindUsersWithNames(names []string) (map[string]error, []*User) { us := make([]*User, len(names)) errs := map[string]error{} for _, name := range names { uc := NewUserColl().New() // 每次loop都调用一次`New` u := new(User) if err := uc.Where(\u0026#34;name = ?\u0026#34;, name).First(u).Error; err != nil { map[name] = err continue } us = append(us, u) } return errs, us } // 更改2 - 无效 func FindUsersWithNames(names []string) (map[string]error, []*User) { us := make([]*User, len(names)) errs := map[string]error{} for _, name := range names { ConnMysql() // 每次loop都重新连接一次数据库 uc := NewUserColl() u := new(User) if err := uc.Where(\u0026#34;name = ?\u0026#34;, name).First(u).Error; err != nil { map[name] = err continue } us = append(us, u) } return errs, us } // 更改3 可用 func FindUsersWithNames(names []string) (map[string]error, []*User) { us := make([]*User, len(names)) errs := map[string]error{} for _, name := range names { err, u := FindUserWithName(name) // 调用一次函数 if err != nil { errs[name] = err continue } us = append(us, u) } return errs, us } 经过以上尝试，找到了一个解决办法。\n后言 # 但是问题并不是到此为止，为什么调用一层函数，就可以解决，而gorm.DB.New却不行呢？大胆分析一下：上述的三种尝试的区别在于，uc在第三种方式中，每次循环都是一个全新的变量，而不是重新赋值，但这依然不能解释为什么New不能解决上述问题。具体的原因后续研究后补上。\n"},{"id":52,"href":"/2018/04/17/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E5%85%A5%E9%97%A8/","title":"分布式架构入门","section":"Posts","content":"在开始之前必须明确的是，分布式和集群的区别，简单的说：\n1.分布式是一种工作方式，把一个系统的不同功能放在不同的机器上； 2.集群是一种物理形态，同一个任务放在不同的机器上； 这样说，也并不是说这两个概念是完全不同，还互相独立，而是实际应用中相辅相成。 我们常说的负载均衡的背后就是集群部署，某些公司为了能扛住突然增长的流量，会采用加很多台服务器的方式来提高性能。看下图： 对于小公司来说，这样部署的方式在于：简单快速，成本在可接受范围。（如果愿意多花点时间进行代码重构和技术再选型或许效果会更好，当然估计要换个不懂技术的老板，然后忽悠他😊）\n但是也不是没有隐患，一方面，集群部署虽然能提高系统的可用性，但是如果多台机器离线，会导致其他机器压力增大，如果严重超过机器负载能力，会导致越来越多的机器离线，一旦解决不及时便会导致整个应用崩溃。其次，集群部署为了保证数据的一致性，一般多采用相同数据源，因此集群并不能无限制扩张。\n分布式系统的应用场景 # 分布式主要解决的问题是提升应用的负载能力。\n分布式的优缺点 # 将一个系统的不同模块分别部署在不同的机器上。这里不得不说到微服务，微服务是把系统服务拆分成为独立的服务来部署（这里说的独立，是数据独立和部署独立，不依赖于其他服务）。\n从本质上来说微服务也是分布式部署的一种。只是相比一般分布式应用，拆分更加彻底。\n优点 # 分成多个模块，并使用接口通信，低耦合； 团队协作开发，事先约定好接口，独立开发，效率更高； 部署更加灵活； 缺点 # 依赖网络通信，增加额外开发通信接口。 总结 # 以上说的都是垃圾，只希望打开思路。某邓同志说：能抗住压力的应用才是好应用。\n参考资料 # https://blog.csdn.net/boonya/article/details/55046568 https://www.jianshu.com/p/39c1e4ec0d63\n"},{"id":53,"href":"/2018/04/17/golang%E7%83%AD%E9%87%8D%E8%BD%BD%E5%B7%A5%E5%85%B7/","title":"golang热重载工具","section":"Posts","content":" 运行截图 # 项目地址 # https://github.com/yeqown/gw\n本项目由https://github.com/silenceper/gowatch改造而来。不同之处在于:\n本gw任意指定重新执行的命令。 而原来的gw默认是重新编译打包go程序。\n"},{"id":54,"href":"/2018/04/08/Golang%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","title":"Golang学习笔记","section":"Posts","content":" 目录 # Channel Goroutine Channel # 一开始是在看channel的源码，结果发现里面含有一些抽象的描述（可能也就是我觉得。。。毕竟没有深入）\nDo not change another G\u0026rsquo;s status while holding this lock (in particular, do not ready a G), as this can deadlock with stack shrinking.\n其中G是啥？我看着是很懵逼的，去google了一下，其实是goroutine相关的知识，那就把goroutine理解了先。\n2020-04-13 填坑\nchannel in go\nGoroutine # G: 表示goroutine，存储了goroutine的执行stack信息、goroutine状态以及goroutine的任务函数等；另外G对象是可以重用的。 P: 表示逻辑processor，P的数量决定了系统内最大可并行的G的数量（前提：系统的物理cpu核数\u0026gt;=P的数量）；P的最大作用还是其拥有的各种G对象队列、链表、一些cache和状态。 M: 代表着真正的执行计算资源。在绑定有效的p后，进入schedule循环；而schedule循环的机制大致是从各种队列、p的本地队列中获取G，切换到G的执行栈上并执行G的函数，调用goexit做清理工作并回到m，如此反复。M并不保留G状态，这是G可以跨M调度的基础。M必须关联了P才能执行Go代码。\n结合下图更方便理解： \u0026ndash;源于Tonybai的博客，见参考资料。 参考资料 # runtime/runtime2.go Tonybai-goroutine调度器 "},{"id":55,"href":"/2018/03/02/aliyun-rds%E6%95%B0%E6%8D%AE%E5%A4%87%E4%BB%BD%E6%96%B9%E6%A1%88/","title":"aliyun-rds数据备份方案","section":"Posts","content":"本文主要是总结下在使用aliyun-rds数据备份方案过程中的心得。\n高可用一直都是线上服务维护用户体验的关键之一。为了达到高可用，业界已经有了很多方案。最典型的就是“冗余备份+自动故障转移”。冗余备份是说，当一个节点服务不可用时，有其他服务能够代替其工作。除此之外，如果服务出现了必须人工介入解决的故障，也会影响系统的高可用特性。\n本文着重介绍数据的高可用方案\n数据库冗余 # 如果是单节点的数据库，还用的着说吗？要保证服务高可用，除了主-从数据库之外，还需要从备份数据库，当然不能保证说一定不会遇到所有的备份数据库，都挂掉的情况\u0026hellip;。阿里云提供了RDS-高可用版本和RDS-单机版,两者的区别见下图： 这就算最基本的冗余了，没有主从复制，没有读写分离。但是能保证主库在换掉的时候，还能使用备库提供服务。如果服务对于数据库性能和可用性有一定要求，那么可以在这个基础上升个级，见下图： 数据故障自动转移 # 已经有了冗余的数据库节点了，那么接下来要做的事情就是怎么感知数据库异常，并实现自动切换到备份实例中? 阿里云灾备方案的文档是这样描述的：\n主实例和灾备实例均搭建主备高可用架构，当主实例所在区域发生突发性自然灾害等状况，主节点（Master）和备节点（Slave）均无法连接时，可将异地灾备实例切换为主实例，在应用端修改数据库链接地址后，即可快速恢复应用的业务访问。\n对于主节点全部不可用的情况对应用服务是可见的，因此应用服务可以通过指定一些异常判断，在判定主节点不可用的时候，主动切换数据库连接地址来获取数据，提供服务。\n// sql-detect.go package main import ( \u0026#34;database/sql\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; _ \u0026#34;github.com/go-sql-driver/mysql\u0026#34; _ \u0026#34;github.com/mxk/go-sqlite\u0026#34; ) var ( mysqlAvailable bool = true mutex = sync.Mutex{} db *sql.DB = nil ) func MysqlDetection(db *sql.DB, ticker *time.Ticker) { for { select { case \u0026lt;-ticker.C: if e := db.Ping(); e != nil { fmt.Println(\u0026#34;got error\u0026#34;, e) mutex.Lock() mysqlAvailable = false mutex.Unlock() } else { fmt.Println(\u0026#34;status ok\u0026#34;) } } } } func MysqlSwitch() { for { mutex.Lock() if !mysqlAvailable { fmt.Println(\u0026#34;Switch Sqlite3\u0026#34;) db, _ = sql.Open(\u0026#34;sqlite3\u0026#34;, \u0026#34;./foo.db\u0026#34;) } mutex.Unlock() time.Sleep(time.Second * 4) } } func main() { c := make(chan bool) db, _ = sql.Open(\u0026#34;mysql\u0026#34;, \u0026#34;yeqiang:yeqiang@/test_yeqiang\u0026#34;) ticker := time.NewTicker(time.Second * 2) go MysqlDetection(db, ticker) go MysqlSwitch() \u0026lt;-c } 测试截图：\n阿里云高可用版本的RDS，会自动检测主节点和备节点是否正常提供服务。通过8-10秒的心跳，检测模块可以在30秒内完成异常切换，对于应用服务来说是透明的。\n阿里云DBS # DBS，这是阿里云新出的备份服务，目前处于公测阶段。文档中提到的实时备份和增量备份，倒是很吸引人。 图片多源于阿里云\n"},{"id":56,"href":"/2018/03/01/Stack%E5%AE%9E%E7%8E%B0O1%E7%9A%84Min%E5%92%8CMax/","title":"Stack实现O(1)的Min和Max","section":"Posts","content":"栈（Stack）Pop和Push操作只需要对栈顶元素进行操作，就不多加描述了。那么对于Max和Min操作，怎么保证O(1)的时间复杂度?最直接想到的就是设置两个标记位，最小值的最大值，在push和pop的时候更新两个值。那么怎么更新呢，怎么保证最大值和最小值弹出之后还能正确获取到当前所有元素中的最大值和最小值呢？请看下文：\n辅助最大值栈SM # 算法描述 # type Stack Struct{ data []int } var SMax *Stack = new(Stack) push: 如果当前元素大于等于辅助栈的栈顶元素或者辅助栈为空，那么当前元素push到辅助栈中 pop: 如果当前元素等于辅助栈的栈顶元素，那么从辅助栈中弹出当前元素 举个例子 # 如果有1，3，6，1，12，512，12，5121，121，412数据放入栈中 Step-1. 元素1入栈，当前SM栈为空，SM栈也同步更新\nStack: 1 SMax: 1 Step-2. 元素3入栈，3 \u0026gt; 1，SMax栈也同步更新\nStack: 1, 3 SMax: 1, 3 Step-3. 元素6入栈，6\u0026gt;3，SMax栈也同步更新\nStack: 1, 3, 6 SMax: 1, 3, 6 \u0026hellip;此处省略更多步骤\n最大值标记法 # 第一种方式利用辅助栈来标记当前最大值和上一个最大值，并利用栈来实现O(1)复杂度。但是根据上述的例子，可以看到如果插入的元素是依次增大，那么耗费2N+1空间才能实现栈的最大值和最小值在O(1)复杂度。现在介绍的方法，能够很好的减少空间耗费，并保证O(1)时间复杂度。\n算法描述 # type Stack struct { data []int max int // default = math.MinInt32 } push: 将(当前元素-Max)放入栈中；如果当前元素大于Max，用当前元素替换Max pop: 如果栈顶元素\u0026gt;0，弹出Max，用Max-栈顶元素替换Max；否则弹出Max+栈顶元素 再举个例子 # 如果有5, 23, 12, 499, 45, 20, 60入栈 Step-1. 元素5-Max入栈，5 \u0026gt; math.MinInt32, 更新Max=5\nStack: 5-math.MinInt32 Max: 5 Step-2. 元素23-5入栈，23 \u0026gt; Max=5, 更新Max=23\nStack: 5-math.MinInt32, 18 Max: 23 Step-3. 元素12-23入栈，5 !\u0026gt; Max=23, 不更新Max\nStack: 5-math.MinInt32, 18, -11 Max: 23 Step-4.-11 \u0026lt; 0, 元素Max+-11 = 12 弹出, 不更新Max\nStack: 5-math.MinInt32, 18 Max: 23 Step-5. 18 \u0026lt; 0, 元素Max弹出, 更新Max = (Max-18)\nStack: 5-math.MinInt32 Max: 5 \u0026hellip;此处省略更多步骤\n最小值的引导 # 上述的两种方法，只是描述了最大值的思路。\n对于第一种思路来说，推导到最小值身上就是SMin辅助栈，push的时候，如果当前元素小于等于SMin栈定元素，便将当前元素push到SMin中去；同理pop的时候，如果当前元素等于SMin的栈顶元素，也将当前元素从SMin中弹出。\n第二种的思路是，通过栈中的元素来标记最大值。推导到最小值上来说就是：\npush: 将(当前元素-Min)放入栈中；如果当前元素小于Min，用当前元素替换Min pop: 如果栈顶元素\u0026lt;0，弹出Min，用Min-栈顶元素替换Min；否则弹出Min+栈顶元素 问题 # 如果考虑采用第二种方式来同时实现最小值和最大值的话\u0026hellip;？\n"},{"id":57,"href":"/2018/02/11/Trie%E6%A0%91%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D/","title":"Trie树学习","section":"Posts","content":"Trie树(Retrieval Tree)又称前缀树，可以用来保存多个字符串，并且查找效率高。在trie中查找一个字符串的时间只取决于组成该串的字符数，与树的节点数无关。Trie树形状如下图： 应用场景 # 词频统计（搜索引擎常用） 前缀单词搜索 构造Trie树 # 构造Trie树有如下几种方式（非全部）：\n// 结构1，简单且直观，但是空间闲置较多，利用率低下 type TrieNode struct{ Char\trune Children [27]*TrieNode } // 结构二 可变数组Trie树, 减少了闲置指针，但是只能通过遍历来获取下一状态，降低了查询效率 type TrieNode struct { Char rune Children []*TrieNode } // 结构3，双数组Trie树，空间和时间上耗费较为均衡，但是动态构建，解决冲突耗费时间较多 type TrieNode struct { Base []int Check []int } 数组构造方式 # 这里选择双数组方式来实现Trie树。\n基于数组的实现方式，把trie看作一个DFA，树的每个节点对应一个DFA状态，每条从父节点指向子节点的有向边对应一个DFA变换。遍历从根节点开始，字符串的每个字符作为输入用来确定下一个状态，直到叶节点。 \u0026mdash;- 摘自参考资料，Trie数组实现原理\n关于双数组：\nBase数组，表示后即节点的基地址的数组，叶子节点没有后继 Check数组，用于检查 Trie树应用之前缀搜索 # 前缀搜索。也就是给一定的字符串，给出所有以该字符串开始的单词。譬如，Search(\u0026ldquo;go\u0026rdquo;)，得到[\u0026ldquo;go\u0026rdquo;, \u0026ldquo;golang\u0026rdquo;, \u0026ldquo;google\u0026rdquo;, \u0026hellip;]\n三种构造方式的优劣分析 # Trie树（数组Trie树） # 每个节点都含有26个字母指针，但并不是都会使用，内存利用率低。时间复杂度：O(k)， 空间复杂度：O(26^n)\n双数组Trie树 # 构造调整过程中，每个状态都依赖于其他状态，所以当在词典中插入或删除词语的时候，往往需要对双数组结构进行全局调整,灵活性能较差。双数组已经大幅改善了经典Trie树的空间浪费，但是冲突发生的时候，总是往后寻址，不可避免数组空置。随着数据量增大，冲突的几率也越来越大，字典树的构建也越来越慢。如果核心词典已经预先建立好并且有序的，并且不会添加或删除新词，那么这个缺点是可以忽略的。所以常用双数组Tire树都是载入整个预先建立好的核心分词词典。\nTail-Trie树 # 三数组Trie树实在双数组的基础上优化而来，增加了tail节点，就是将非公共前缀的词尾合并成为一个节点，减少节点总数，提升词典树的构建速度。如图： 参考资料 # An Implementation of Double-Array Trie Trie树详解及应用 DoubleArrayTrie An Efficient Implementation of Trie Structures github.com/gansidui/trie.go github.com/smtc/godat "},{"id":58,"href":"/2018/02/11/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E4%B9%8B%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB%E9%97%AE%E9%A2%98-LD/","title":"动态规划之字符串编辑距离","section":"Posts","content":" 问题描述 # 给定 2 个字符串 a, b. 编辑距离是将 a 转换为 b 的最少操作次数，操作只允许如下 3 种： 插入一个字符，例如：fj -\u0026gt; fxj 删除一个字符，例如：fxj -\u0026gt; fj 替换一个字符，例如：jyj -\u0026gt; fyj\n函数原型：\nfunc LevenshteinDis(str1, str2 string) int { ... } 算法适用场景 # 拼写检查 输入联想 语音识别 论文检查 DNA分析 问题分析 # 假定函数edit_dis(stra, strb)表示，stra到strb的编辑距离。算法问题可以分为四种情况：\nedit_dis(0, 0) = 0 edit_dis(0, strb) = len(strb) edit_dis(stra, strb) = len(stra) edit_dis(stra, strb) = ? 对于4th一般情况，没有办法直接给出求解方式，我们来分析edit_dis(stra+chara, strb+charb)可能的情况：\nstra能转成strb，那么只需要判断chara是不是等于charb (cur_cost = 0 if chara == charb else 1) stra+chara能转成strb, 那么要让stra + chara 转成strb+ charb, 只需要插入charb就行了 如果stra 可以直接转成strb+charb，那么删除chara就可以转换成功了 综上的分析，可以得到如下DP公式：\n|-- 0, (i=0, j=0) |-- j, (i=0, j\u0026gt;0) edit_dis(i, j) = |-- i, (i\u0026gt;0, j=0) |-- min{edit_dis(i-1, j)+1, edit_dis(i, j-1)+1, edit_dis(i-1, j-1) + cur_cost} # cur_cost = 0 if chara == charb else 1 到这里，完全可以开始动手写代码了。如果还不清楚，可以参考Levenshtein Distance, in Three Flavors，里面有详细步骤和分析\n编码及测试 # 使用Golang编码LevenshteinDistance如下：\nfunc LevenshteinDistance(source, dest string) int { var cols, rows int = len(source), len(dest) if cols == 0 { return rows } if rows == 0 { return cols } var ld *LD = \u0026amp;LD{Rows: rows, Cols: cols} ld.constructMatrix() // 初始化二维矩阵 // PrintMatrix(ld.M) // step 5 for c := 1; c \u0026lt;= cols; c++ { for r := 1; r \u0026lt;= rows; r++ { var cur_cost int = 1 if source[c-1] == dest[r-1] { cur_cost = 0 } // step 6 cost := minOfThree(ld.M[r-1][c-1]+cur_cost, ld.M[r-1][c]+1, ld.M[r][c-1]+1) // step 7 ld.setMatrix(cost, r, c) } } PrintMatrix(ld.M) return ld.M[rows][cols] } 做了简单的划分，让算法看起来更清晰，这里是放入了一般情况的处理。\n测试代码及测试结果截图：\n// ... func Test_PrintMatrix(t *testing.T) { m := Matrix{ {1, 2, 3, 4}, {1, 2, 3, 4}, {1, 2, 3, 4}, {1, 2, 3, 4}, {1, 2, 3, 4}, {1, 2, 3, 4}, {1, 2, 3, 4}, } PrintMatrix(m) } func Test_LD_ConstructMatrix(t *testing.T) { ld := \u0026amp;LD{Rows: 3, Cols: 6} ld.constructMatrix() if len(ld.M[0]) != 7 { t.Fatal(\u0026#34;ConstructMatrix make a wrong matrix with cols\u0026#34;) } if len(ld.M) != 4 { t.Fatal(\u0026#34;ConstructMatrix make a wrong matrix with rows\u0026#34;) } // display PrintMatrix(ld.M) } func Test_LevenshteinDistance(t *testing.T) { source := \u0026#34;GUMBO\u0026#34; dest := \u0026#34;GAMBOL\u0026#34; dis := LevenshteinDistance(source, dest) if dis != 2 { t.Fatalf(\u0026#34;wrong dis is got, %d, actual: %d\u0026#34;, dis, 2) } } func Test_LevenshteinDistance_case1(t *testing.T) { source := \u0026#34;\u0026#34; dest := \u0026#34;GAMBOL\u0026#34; dis := LevenshteinDistance(source, dest) if dis != 6 { t.Fatalf(\u0026#34;wrong dis is got, %d, actual: %d\u0026#34;, dis, 2) } } func Test_LevenshteinDistance_case2(t *testing.T) { source := \u0026#34;GUMBO\u0026#34; dest := \u0026#34;\u0026#34; dis := LevenshteinDistance(source, dest) if dis != 5 { t.Fatalf(\u0026#34;wrong dis is got, %d, actual: %d\u0026#34;, dis, 2) } } func Test_LevenshteinDistance_case3(t *testing.T) { source := \u0026#34;GUMBO\u0026#34; dest := \u0026#34;GUMBO\u0026#34; dis := LevenshteinDistance(source, dest) if dis != 0 { t.Fatalf(\u0026#34;wrong dis is got, %d, actual: %d\u0026#34;, dis, 2) } } func Test_LevenshteinDistance_case4(t *testing.T) { source := \u0026#34;\u0026#34; dest := \u0026#34;\u0026#34; dis := LevenshteinDistance(source, dest) if dis != 0 { t.Fatalf(\u0026#34;wrong dis is got, %d, actual: %d\u0026#34;, dis, 2) } } 参考资料 # 编辑距离-明无梦的博客 文章中采用了两种方法（分治，动态规划）来解决编辑距离的问题 字符串编辑距离详解-菜鸟加贝的爬升 Levenshtein Distance, in Three Flavors 代码地址 # github.com/yeqown/alg/dp/levenshtein_dis.go github.com/yeqown/alg/dp/levenshtein_dis_test.go\n"},{"id":59,"href":"/2018/01/29/ShortURL%E7%94%9F%E6%88%90%E7%B3%BB%E7%BB%9F/","title":"ShortURL系统实现","section":"Posts","content":"在知乎上看了一个很有启发的回答，因此实际动手来实现短URL生成系统。贴上链接： 知乎 - 短URL系统是如何设计的。其中提到了，要实现短URL生成系统要解决的问题有：\n如何优雅的实现？ 怎么基本实现长对短、一对一？ 如何实现分布式，高并发，高可用？ 储存选用？ 基本原理 # 数据库自增ID转换62进制\n使用自增ID不会产生重复的短链接。 为了解决自增ID超长和不便记忆，对ID进行62进制编码。所谓62进制就是0-9，a-z，A-Z。 简单计算下：\n62 ^ 4 = 14,776,336 62 ^ 5 = 916,132,832 62 ^ 6 = 56,800,235,584 // 已经足够使用了 总体结构及处理流程 # 长链接处理流程 # 获取参数，调用shortURL服务 尝试从缓存中获取，如果命中，则读取短链接(重置过期时间)。跳转第4步 将长链接存储到Mysql数据库，根据ID进行base62编码，组装Domain+Encoded字符串并更新数据库 返回生成的短链接 短链接处理流程 # 解析短链接为ID 查询ID对应的长链接 以301方式跳转到长链接 长链接与短链接的对应关系 # 一对多，一个长链接可能对应多个短链接。数据表存储结构如下：\n+-----------+--------------+------+-----+---------+----------------+ | Field | Type | Null | Key | Default | Extra | +-----------+--------------+------+-----+---------+----------------+ | id | int(64) | NO | PRI | NULL | auto_increment | | long_url | varchar(100) | NO | | NULL | | | short_url | varchar(40) | YES | | NULL | | +-----------+--------------+------+-----+---------+----------------+ 分布式和高并发设计 # ###注：这部分未实现。我的思路如下：\n分布式部署会遇到的问题\u0026quot;ID重复\u0026quot;，关于这一点在回答中也提示了：根据node数来配置自增step。譬如说，有1000个node：\n|节点|1st-URL-ID|2nd-URL-ID|3th-URL-ID|\u0026hellip;|Nth-URL-ID| |\u0026mdash;|\u0026mdash;|\u0026mdash;|\u0026mdash;-|\u0026mdash;-| |node-1|0|1000|2000|\u0026hellip;| 0 + (N-1) * 1000| |||||\u0026hellip;.|| |node-1000|999|1999|2999|\u0026hellip;|999 + (N-1) * 1000|\n测试 # 单元测试（不写单元测试不准push!!） # ➜ _shorturl git:(master) ✗ go test -v === RUN Test_Encode --- PASS: Test_Encode (0.00s) === RUN Test_Decode --- PASS: Test_Decode (0.00s) === RUN Test_ProcessEncodeThenDecode --- PASS: Test_ProcessEncodeThenDecode (0.00s) === RUN Test_SetUrlCache --- PASS: Test_SetUrlCache (0.01s) === RUN Test_LoadConfig --- PASS: Test_LoadConfig (0.00s) === RUN Test_GetInstance --- PASS: Test_GetInstance (0.00s) === RUN Test_Insert --- PASS: Test_Insert (0.08s) === RUN Test_QueryUrl --- PASS: Test_QueryUrl (0.02s) === RUN Test_Update --- PASS: Test_Update (0.00s) === RUN Test_RegRouter --- PASS: Test_RegRouter (0.00s) === RUN Test_ShortenAndParse 2018/02/01 11:23:28 deal with url: http://www.baidu.com --- PASS: Test_ShortenAndParse (0.02s) === RUN Test_ConnectDB --- PASS: Test_ConnectDB (0.00s) === RUN Test_GetDB --- PASS: Test_GetDB (0.00s) === RUN Test_CloseConnection --- PASS: Test_CloseConnection (0.00s) PASS ok shorturl/_shorturl\t0.157s 功能测试 # {% dplayer \u0026ldquo;url=/mov/演示短链接功能.mov\u0026rdquo; \u0026ldquo;loop=no\u0026rdquo; \u0026ldquo;theme=#FADFA3\u0026rdquo; \u0026ldquo;autoplay=false\u0026rdquo; \u0026ldquo;token=tokendemo\u0026rdquo; %}\n性能测试 # 未使用分布式测试结果截图如下（没有达到最大吞吐量）： 存储选用 # 选择Mysql用于存储数据，Redis作为缓存。\n源码 # github.com/yeqown/shorturl\n"},{"id":60,"href":"/2018/01/27/Golang%E6%9C%8D%E5%8A%A1%E7%AB%AF%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/","title":"Golang服务端技术笔记","section":"Posts","content":"总结使用Golang开发服务端时，使用的基础的工具和部署方式。用于思考不足并优化，提升编码效率。 总体上采用MVCS的软件模式，如下图：\n从图中可以看出，MVCS是从MVC进化而来，相比于MVC，增加了Service层。把业务逻辑从Controller层中抽离出来，这样做的好处在于，项目日益庞大之后，将某些功能独立出来。\nGolang工具 # \u0026ldquo;gvt\u0026rdquo; 依赖管理工具 \u0026ldquo;httprouter\u0026rdquo; 路由及中间件配置 \u0026ldquo;schema\u0026rdquo; 解析请求参数到结构体 \u0026ldquo;beego/validation\u0026rdquo; 结构体校验工具 \u0026ldquo;github.com/go-redis/redis\u0026rdquo; redis操作库 \u0026ldquo;github.com/go-sql-driver/mysql\u0026rdquo; Mysql Driver\n文件结构 # --Golang Project |-sh # shell脚本，包括数据库脚本 |-config # 配置文件 |-logs # 日志文件 |-vendor # 项目源码及依赖 | |-github.com # | |-mainfest # gvt 依赖管理文件 | |-app | |-utils | |-controllers | |-models | |-route | |-services |-Dockerfile # docker构建镜像配置文件 |-docker-compose.yml # docker-compose.yml文件 `-entry.go # web服务入口文件 部署方式 # 采用docker来部署应用。分别编写Dockerfile和docker-composer.yml文件，实例如下：\nDockerfile # #### # build #### FROM golang:1.9-alpine AS build WORKDIR /go/src/web-server COPY ./entry.go ./ COPY ./vendor ./vendor RUN CGO_ENABLED=0 GOOS=linux ARCH=amd64 go build -o web-server entry.go #### # deploy #### FROM alpine WORKDIR /usr/src/web-server COPY --from=build /go/src/web-server/web-server ./ COPY ./config ./ RUN mkdir ./logs EXPOSE 9090 # CMD [\u0026#34;./web-server\u0026#34;, \u0026#34;--conf ./config/config.json\u0026#34;] 打包镜像的方法如下： docker build -t web-server:release . -f Dockerfile\ndocker-compose.yml配置 # version: \u0026#34;2\u0026#34; services: web: image: web-server:release container_name: ${SERVER_NAME} links: - postgres:postgres volumes: - ./logs:/usr/src/web-server/logs ports: - 9090:9090 postgres: image: postgres:9.4 container_name: postgres volumes: - ./postgresql:/var/lib/postgresql/data ports: - 5432:5432 environment: POSTGRES_PASSWORD: ${USER_PWD} POSTGRES_USER: ${USER} POSTGRES_DB: ${$DB_NAME} 简单解释：\nlinks：采用容器内联的方式来通信，如此便可以在web-server容器中，访问HOST=postgres来访问postgresql数据库 volumes：挂载宿主机上文件或者文件夹到容器内，以便完成数据持久化\n数据库选择 # no Sql选用Mongo，Redis（缓存系统）\nSql选用Postgre或者MySql\n"},{"id":61,"href":"/2018/01/27/Drone%E4%BD%93%E9%AA%8C/","title":"Drone体验","section":"Posts","content":"相较于Jenkins，Gitlab-CI\u0026hellip;等，尝试Drone的首要原因是，天生的docker支持。不用去操心部署CI或者CD的环境配置等等烦心事。只需要上手，如何配置这个CD工具，让我使用更加畅快和顺手。\n安装部署 # 前提：已经安装了docker，docker-compose，并基本掌握docker用法，基本熟悉docker-compose配置文件\npull镜像 # docker pull drone/drone:0.8 # droner-server 镜像 docker pull dorner/agent:0.8 # drone-agent 镜像 也可以跳过这一步，docker运行的时候，如果匹配不到本地镜像，会自动拉取。\ndocker-compose.yml配置文件 # 为了方便，新建一个Drone文件夹，目录结构如下：\n--Drone # 文件夹 |---docker-compose.yml # docker-compose 配置文件 |---data # 用于挂载的数据文件 |---drone.domain.com # nginx sever 配置文件 `---other.file # 其他文件 已知文件结构后，编写的docker-compose.yml文件如下：\nversion: \u0026#39;2\u0026#39; services: drone-server: image: drone/drone:0.8 container_name: drone-server ports: - 8000:8000 - 9000:9000 volumes: - ./data:/var/lib/drone/ # 在没有跟数据库绑定的情况下，默认使用sqlite数据库 restart: always environment: - DRONE_OPEN=false - DRONE_HOST=http://127.0.0.1:8000 # 最好是在服务器上，localhost无法收到webhook的通知 - DRONE_ADMIN=yourname - DRONE_GITHUB=true - DRONE_GITHUB_CLIENT=7bc7971bxxxxx # 需要预先注册一个github oauth应用 - DRONE_GITHUB_SECRET=9456c630xxxxxxxxxxxxxx - DRONE_SECRET=secret - DRONE_DEBUG=false drone-agent: image: drone/agent:0.8 container_name: drone-agent command: agent restart: always depends_on: - drone-server volumes: - /var/run/docker.sock:/var/run/docker.sock environment: - DRONE_SERVER=drone-server:9000 - DRONE_SECRET=secret - DRONE_DEBUG=true 启动Drone # 启动就很简单了，Drone目录下执行：docker-compose up -d，启动结果截图如下： 如果是首次打开，会先去github请求授权，然后回调schema://drone.your_domain.com/authorize，如截图： 其他配置 # 这部分就参见官方文档了\n终止 # 到这里也只是完成了，配置和运行，但是CI和CD的功能，由于服务器关系没有深入。稍稍尝试了一下：\n在自己的Repo中加入.drone.yml配置文件 设置好触发条件（如，push，merge） push代码触发，这时候刷新页面，便能看见，Drone的pipeline处于运行中 然而也没那么顺心 # 诚然Drone安装部署比其他CI工具简单很多，但是在使用过程中也遇见了不足：\n文档不足，显得粗燥，网上也没有太多的教程文档。 对于gitlab并非完全支持。精力有限，只尝试了gitlab和github，其中gitlab API=v3,v4 均未成功，github一次成功。 相较于Jenkins，Drone似乎没有主动执行构建的可能性。可以参见Drone与Jenkins的优劣 Drone与Jenkins的优劣 # 对比项 Jenkins Drone 安装部署 一般（如果不采用docker部署） 非常简单 配置复杂度 复杂，需要进行插件配置，账户管理.. 一个配置文件 权限管理 独立 依赖VCS 能否手动触发 能 不能 是否支持pipeline 不支持 支持 界面复杂程度 复杂 简单 开发语言 Java Golang 是否支持docker部署 支持 支持 构建时对环境依赖程度 脚本+docker 全程在docker中 Drone简单使用 # 玩腻了Jenkins，也来玩玩Drone CI\n最后 # 相比老牌CI，从支持和稳定性上来说，都还差一些。不建议用于生产（大牛除外），期待越来越好吧！！！\n"},{"id":62,"href":"/2018/01/24/Docker-Compose%E4%B8%8A%E6%89%8B/","title":"docker-compose上手","section":"Posts","content":"docker compose 用于快速在集群中部署分布式应用。按我的理解也可以用于简化部署单个应用。譬如我要使用dock er启动一个nginx服务，需要做端口映射，挂载数据文件，指定镜像\u0026hellip;等等，这种情况下，可以将启动容器的命令整合到docker-compose.yml文件中，可以在多个服务器上运行，瞬间就完成了nginx的安装及配置，再也不用去编译，解决环境依赖了，这种感觉实在是太爽了！！！\n安装 # 使用pip pip install docker-compose 从官方Github Release下载二进制包文件 其他方法略去 使用场景 # 在日常工作中，经常会碰到需要多个容器相互配合来完成某项任务的情况。例如要实现一个 Web 项目，除了 Web 服务容器本身，往往还需要再加上后端的数据库服务容器，甚至还包括负载均衡容器等。\n实战场景 # 需要部署的项目，只有两个docker容器，一个server，一个db。一般的部署方式是，分别启动两个容器，容器间通过互联的方式通信：\nsudo docker run --rm -p 5433:5432 --name postgres -e POSTGRES_PASSWORD=minepwd -e POSTGRES_USER=mineusr -d postgres sudo docker run --rm -p 9091:9091 --link postgres:postgres --name mineserver -d me/mineserver 这两条命令还是有挺麻烦的，如果记不住，当然可以用shell脚本来运行，可以如果其中某一个服务无法如期运行。。。就很监介了。这时候就可以引入docker-compose了。\n编写docker-compose.yml来部署项目 # version: \u0026#34;2\u0026#34; # 指定docker-compose版本 services: # 项目依赖的服务 postgres: # 服务名字 image: postgres # 服务需要的docker镜像与docker run命令中的镜像指定方式一致 volumes: # 挂载卷，这里的主要目的是，方便同步数据库和数据脚本 - ./postgres:/var/lib/postgresql/data - ./sh:/usr/src/sh ports: # 端口绑定 - 5433:5432 container_name: postgres environment: # 设置环境变量 POSTGRES_PASSWORD: \u0026#34;minepwd\u0026#34; POSTGRES_USER: \u0026#34;mineusr\u0026#34; POSTGRES_DB: \u0026#34;minedb\u0026#34; mineserver: image: me/mineserver volumes: # 挂载卷，方便查看输出日志 - ./logs:/usr/src/mineserver/logs ports: - 9091:9091 container_name: mineserver links: # 容器互联 - postgres:postgres 在编写docker-compose.yml的时候，需要注意的是各个选项的数据类型，不过docker-compose会有提示，也很方便\n这样编写之后，可以直接运行docker-compose up就可以启动两个容器了。输出如下：\n➜ mineserver git:(yeqown) docker-compose up Starting postgres ... done Starting mineserver ... done Attaching to postgres, mineserver mineserver | 2018/01/25 06:28:02 Loading config: ./configs/config.json mineserver | 2018/01/25 06:28:02 init all logger done mineserver | 2018/01/25 06:28:02 host=postgres user=mineusr dbname=minedb sslmode=disable password=minepwd mineserver | panic: dial tcp 172.18.0.2:5432: getsockopt: connection refused mineserver | mineserver | goroutine 1 [running]: mineserver | mineserver/vendor/app/models.ConnPgsql(0xc4200127d0, 0x4a) mineserver | /go/src/mineserver/vendor/app/models/pgsql.go:17 +0x28b mineserver | main.main() mineserver | /go/src/mineserver/main.go:32 +0x16d mineserver exited with code 2 postgres | 2018-01-25 06:28:05.360 UTC [1] LOG: listening on IPv4 address \u0026quot;0.0.0.0\u0026quot;, port 5432 postgres | 2018-01-25 06:28:05.360 UTC [1] LOG: listening on IPv6 address \u0026quot;::\u0026quot;, port 5432 postgres | 2018-01-25 06:28:05.363 UTC [1] LOG: listening on Unix socket \u0026quot;/var/run/postgresql/.s.PGSQL.5432\u0026quot; postgres | 2018-01-25 06:28:05.449 UTC [20] LOG: database system was shut down at 2018-01-25 05:48:24 UTC postgres | 2018-01-25 06:28:05.475 UTC [1] LOG: database system is ready to accept connections 如输出，使用过程中遇到一个问题，mineserver依赖于postgre，导致mineserver无法正常启动。解决方案有： 1 重启minerserver，docker-compose start mineserver 2 改写mineserver链接数据库的部分，增加重试机制 3 分成多个docker-composr.yml，按依赖关系来分先后启动 4 添加依赖关系来控制启动顺序，新增depends_on选项如下：\n# docker-compose.yml ... mineserver: ... depends_on: # 依赖关系 - postgres links: # 容器互联 - postgres:postgres 但是，也没有办法知道被依赖的服务是不是启动成功了，所以在必须依赖的时候后还是会有问题 5 参见官网解决方案，增加一个wait-for-it.sh。 docker-compose.yml 文件如下：\n# docker-compose.yml ... mineserver: ... depends_on: # 依赖关系 - postgres links: # 容器互联 - postgres:postgres command: [\u0026#34;./wait-postgres.sh\u0026#34;, \u0026#34;./mineserver\u0026#34;] # 结合wait-postgres.sh理解 wait-postgres.sh 文件如下：\n#!/bin/sh # wait-postgre.sh host=\u0026#34;postgres\u0026#34; for i in $(seq 1 10) do nc -z $host 5432 if [[ $? -eq \u0026#34;\u0026#34; ]]; then echo -n . sleep 1 else echo \u0026#34;Postgres is ready\u0026#34; break fi done # execute mineserver, $1 is command that start mineserver $1 # how to use? # command: [\u0026#34;./wait-postgres.sh\u0026#34;, \u0026#34;./imetro-server\u0026#34;] 关于5th，需要注意的要注意wait-for-it.sh, 需要放到服务镜像中去；要根据server镜像中的shell来指定sh，譬如说，我最开始使用的是bash，然而bash在alpine中并不存在……；另外wait-for-it.sh 需要可执行权限哦。\n关于docker-compose.yml的其他命令 # 官方文档或者Docker入门到实践已经讲的很清楚了\n"},{"id":63,"href":"/2018/01/23/docker-selenium-python%E6%9E%84%E5%BB%BA%E5%89%8D%E7%AB%AF%E8%87%AA%E5%8A%A8%E5%8C%96%E5%88%86%E5%B8%83%E5%BC%8F%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83/","title":"docker+selenium+python构建前端自动化分布式测试环境","section":"Posts","content":"docker + selenium + python 构建前端自动化分布式测试环境。利用seleninum-grid分布式框架，python编写测试代码，docker部署来进行前端自动化测试\n2018-2-1 更新 使用docker-compose编排\n分布式部署的优点 # 自动化的优缺点就不再重复了，主要分析下docke部署和分布式的优势\n提高自动化的测试效率（分布式） 方便打包和持续集成（docker） 解决多人coding，却因为路径不一致导致无法运行的问题（当然也可以通过其他方式来解决～） 这里还有一个问题就是：使用docker部署方式运行测试代码，是看不见本地浏览器启动的，因此在调试测试代码的时候，需要一定的工具来协助，譬如VNC viewer\n开篇-selenium # 大家都知道 Selenium 是支持多种浏览器多个编程语言的一个自动化测试工具。而 Selenium Grid 是一种可以让用户在不同的环境和不同的浏览器上并行运行 web 测试用例的框架。换而言之，使用 Selenium Grid 可以让我们在分布式测试环境下执行测试，例如 Windows，Linux，Mac OS，Andoid/iOS 等等，这样可以大大减少重复的工作量，提高我们的工作效率。\nselenium分布式结构如图： 搭建分布式环境 # 在Dockerhub已经具有了相应的selenium的镜像，我们直接使用就行了\n拉取镜像 # docker pull selenium/hub docker pull selenium/node-chorme-debug 关于node-chrome-debug和node-chrome的区别： 暂未研究\n运行容器 # docker run -d -p 4444:4444 --name sel-hub selunium/hub # 运行hub服务 docker run -d -p 5900:5900 --link sel-hub:hub selunium/node-chrome-debug # 运行slenium chrome 节点 # more node could append like node-chrome-debug 查看节点信息 # 在浏览器中打开http://127.0.0.1:4444/grid/console 这里需要注意的是，如果是在本地运行的容器，并映射4444端口，因此得到127.0.0.1:4444，如果是在虚拟机中运行，ip和端口应该根据网络来获取相应的IP和PORT\n查看容器 # 可以使用docker ps -a来查看容器的状态，确保hub服务和node节点已经成功运行了\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES f2559a691250 selenium/node-chrome-debug \u0026#34;/opt/bin/entry_poin…\u0026#34; 24 minutes ago Up 24 minutes 0.0.0.0:5900-\u0026gt;5900/tcp vibrant_bardeen 69c8bf2544b8 selenium/hub \u0026#34;/opt/bin/entry_poin…\u0026#34; 25 minutes ago Up 25 minutes 0.0.0.0:4444-\u0026gt;4444/tcp selenium-hub 编写简单测试代码 # 关于python-selenium的使用方法，参见http://selenium-python.readthedocs.io/index.html\nOS python selenium MacOS 3.6.5 3.8.0 测试代码使用python编写，在运行过程中遇到了报错，通过升级python-selenium版本到3.8.0解决\nfrom selenium import webdriver as wd from from selenium.webdriver.common.desired_capabilities import DesiredCapabilities as DC hub_remote_url = \u0026#34;http://127.0.0.1:4444/wd/hub\u0026#34; driver = wd.Remote( command_executor=hub_remote_url, desired_capabilities=DC.CHROME) test_url = \u0026#34;http://www.baidu.com\u0026#34; def test_baidu_screenshot(): driver.get(test_url) driver.save_screenshot(\u0026#34;baidu.png\u0026#34;) print(\u0026#34;save baidu screenshot done\u0026#34;) test_baidu_screenshot() driver.quit() 扩展-测试代码制作镜像 # 使用了pytest测试框架 测试代码的目录结构如下：\ndocker-test |---lib //python虚拟环境相关 |---bin //python虚拟环境相关 |---include //python虚拟环境相关 |---src | `-测试代码... |---requirements.txt //依赖清单 `---Dockerfile //用于说明如何构建一个docker镜像 编写Dockerfile # FROM python:alpine3.6 WORKDIR /usr/src/test COPY requirements.txt ./ COPY src/ ./src RUN pip install --no-cache-dir -r requirements.txt \\ \u0026amp;\u0026amp; find ./src -name \u0026#34;__pycache__\u0026#34; | xargs rm -fr # 注：删除__pycache__文件夹是避免把调试环境的差异构建到镜像中（该差异会导致无法正确执行） 使用docker-compose编排 # 基本上就是单个容器的执行转换成docker-compose配置文件格式就行了\nversion: \u0026#34;2\u0026#34; networks: private: driver: bridge services: hub: image: selenium/hub ports: - 4444:4444 networks: - private chrome: image: selenium/node-chrome ports: - 5900:5900 links: - hub depends_on: - hub # 注意这一点，如果不配置这些环境变量会导致node无法找到hub服务 environment: HUB_PORT_4444_TCP_ADDR: hub HUB_PORT_4444_TCP_PORT: 4444 networks: - private 构建镜像截图 # 运行容器截图 # 这里选择的是，交互方式执行测试代码docker run --rm -it dcoker-test /bin/sh, 也可以直接执行pytest测试docker run --rm docker-test pytest -v，只是这种方式在输出方式上没有第一种美观（～^～) 总结 # 使用docker极大屏蔽了部署selenium会遇到的系统差异问题，步骤简洁，易于配置。 分布式部署selenium又能提升前端自动化测试的效率，能够同时运行在不同的系统和浏览器，高效又便捷。 docker打包测试代码，方便协同coding和运行在不同系统环境，简化配置方案 目前遇到的不足，selenium-grid框架在执行测试代码的时候，调试不是很方便，需要依赖其他的工具(VNC，连接在上方) 在docker中运行测试，会遇到中文字符乱码的情况，可以通过改写Dockerfile重新打包镜像来解决该问题 "},{"id":64,"href":"/2018/01/18/docker%E6%8E%A2%E7%B4%A2/","title":"docker探索","section":"Posts","content":"docker 学习实战笔记。关于docker的详细信息参见官方文档\n实战环境配置：\n系统 Docker MacOS 17.12.0-ce 实战一 简单部署可执行文件 # 本次实战是部署一个golang的web服务, 需要mongo, redis支持\n省略代码，打包过程（Mac用户别忘了打包GOOS=linux，0·0）\n编写Dockerfile\nFROM alpine # 简版linux RUN mkdir /app # 在镜像中创建一个文件夹 COPY ./webapp /app COPY ./config /app/config COPY ./logs /app/logs EXPOSE 35764 # 暴露35764端口，也就是为了从外部可以访问到容器类的服务 WORKDIR /app # 切换到应用文件夹（在镜像执行的时候，会自动切换） 构建镜像 执行命令 docker build -t tp-api:v1 . 执行结果如下：\nSending build context to Docker daemon 55.5MB Step 1/7 : FROM alpine ---\u0026gt; 3fd9065eaf02 Step 2/7 : RUN mkdir /app ---\u0026gt; Using cache ---\u0026gt; 6948fdc89bbd Step 3/7 : COPY ./webapp /app ---\u0026gt; Using cache ---\u0026gt; 555307961735 Step 4/7 : COPY ./config /app/config ---\u0026gt; ab15feb5a3ab Step 5/7 : COPY ./logs /app/logs ---\u0026gt; 0055512da60b Step 6/7 : EXPOSE 35764 ---\u0026gt; Running in db0503538961 Removing intermediate container db0503538961 ---\u0026gt; 1d214b7aff6d Step 7/7 : WORKDIR /app Removing intermediate container 42889dd384a7 ---\u0026gt; f0b05bcb428d Successfully built f0b05bcb428d Successfully tagged tp-api:v2 调试容器 执行命令docker run -it --rm -p 35765:35764 tp-api:v1,会进入容器交互模式，如下：\n-\u0026gt; /app # -\u0026gt; /app # -\u0026gt; /app # ls config logs webapp -\u0026gt; /app # 此时输入./webapp 就可以在容器内运行该二进制文件了。\n-\u0026gt; /app # -\u0026gt; /app # -\u0026gt; /app # ls config logs webapp -\u0026gt; /app # ./webapp -\u0026gt; 2018/01/22 07:38:50 loading config from: [ ./config/dev/config.json \u0026amp; ./config/dev/server.json ] -\u0026gt; 2018/01/22 07:38:50 load dbconfig file done -\u0026gt; 2018/01/22 07:38:50 load server config file done -\u0026gt; /app # 总结 本次实战遇到的问题有：\n如何让容器内的应用可以访问宿主机的应用，譬如（mongo，redis）？ A： 经过查阅官方文档和google检索。docker会创建一个网桥来负责容器之间，容器与宿主机的通信，如下图： 更多资料可以参考Docker — 从入门到实践 总的来说，不想通过容器互联的方式来运行程序，那么就需要配置一个host来提供给容器内的应用访问。那么这个host怎么确认呢？\ndocker inspect --format \u0026#34;{{.NetworkSettings.Gateway}}\u0026#34; c41c11eefc83 # c41c11eefc83 容器ID 会得到172.17.0.1（根据系统不同会得到不一样的网关地址）\n注意： Mac 上到这里仍然没有成功运行程序，因为这样的配置还是无法访问宿主机器上的Mongo和Redis服务。最终正确的Host应该是docker.for.mac.localhost\t附上官方文档的说明Docker-for-Mac-Networking\n从17.06开始，我们的建议是连接到特殊的仅限于Mac的DNS名称docker.for.mac.localhost，该名称将解析为主机使用的内部IP地址。\n实战二 Docker多阶段构建 # https://yeasy.gitbooks.io/docker_practice/content/image/multistage-builds.html 链接中已经充分阐述了，多阶段构建的使用场景，本实战只是描述我自己在使用过程中的状况及小小思考。\n在实战一中，部署的web服务是一个手动打包好的可执行程序文件。之所以这样部署，是因为当时在练习使用docker，不知道如何制作自动打包的镜像（监介0-0）。 再后来便尝试了自动打包，Dockerfile如下：\nFROM golang:1.9-alpine WORKDIR /go/src/myapp COPY ./vendor ./vendor COPY ./webapp.go ./ RUN CGO_ENABLED=0 GOOS=linux ARCH=amd64 go build -o webapp webapp.go EXPOSE 35764 CMD [\u0026#34;./webapp\u0026#34;] 查看下打包的镜像，足足有282MB，相比第一种打包方式，体积变大了很多（尽管已经使用了golang:1.9-alpine镜像，如果不是使用这个的话，还会更大。。。）。\n➜ test-platform git:(master) ✗ docker images REPOSITORY TAG IMAGE ID CREATED SIZE tp-api v1.2 9cbe31c46664 7 seconds ago 282MB 当然为了减小镜像体积，有如下方案：\n方案一：可以采用编写shell脚本或手动打包，然后再部署服务。 # 弊端：但是在不同的环境上，shell脚本不能保证百分百可靠，而且还需要该环境支持 方案二：多个Dockerfile, 将编译后的程序及其他文件放入用于部署的镜像中 # 弊端：步骤较为繁琐，因为编译后的文件位于镜像中，必须先将其复制出来；其次是至少需要编写三个文件，一个用于编译的dockerfile.build, 一个用于部署的dockerfile.deploy，一个用于链接两个步骤的shell脚本 方案三： docker多阶段构建 # 为解决以上问题，Docker v17.05 开始支持多阶段构建 (multistage builds)。使用多阶段构建我们就可以很容易解决前面提到的问题，并且只需要编写一个 Dockerfile\n相比于方案二，这是一种更优雅的解决方案。因为在一个dockerfile中你可以给各个阶段命名，然后复制的时候带上--from=stage_name便可以方便的复制编译后的文件了。\n##### # 编译阶段 #### FROM golang:1.9-alpine AS build WORKDIR /go/src/test-platform COPY ./vendor ./vendor COPY ./webapp.go ./ RUN CGO_ENABLED=0 GOOS=linux go build ./webapp.go ##### # 部署阶段 ##### FROM alpine RUN mkdir /app COPY --from=build /go/src/test-platform/webapp /app COPY ./config /app/config COPY ./logs /app/logs EXPOSE 35764 WORKDIR /app CMD [\u0026#34;./webapp\u0026#34;, \u0026#34;-env $RUN_ENV\u0026#34;] 实战三 Docker网络基础及容器互联 # 使用docker部署web服务的时候，一定会接触到的就是：\n如何从容器外访问容器内的服务？ 如何从容器内访问宿主机的服务？ 如何容器间通信？ 1.如何从容器外访问容器内的服务？ # 2.如何从容器内访问宿主机的服务？ # 3.如何容器间通信？ # "},{"id":65,"href":"/2018/01/16/wrk%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/","title":"wrk性能测试工具使用总结","section":"Posts","content":"wrk 压力测试工具的简单小结。 项目地址：https://github.com/wg/wrk\n安装 # Win: https://github.com/wg/wrk/wiki/Installing-wrk-on-Windows-10 Linux: https://github.com/wg/wrk/wiki/Installing-wrk-on-Linux MacOS: brew install wrk\n基本命令 # ➜ ~ wrk Usage: wrk \u0026lt;options\u0026gt; \u0026lt;url\u0026gt; Options: -c, --connections \u0026lt;N\u0026gt; 连接数 -d, --duration \u0026lt;T\u0026gt; 持续时间 -t, --threads \u0026lt;N\u0026gt; 线程数 -s, --script \u0026lt;S\u0026gt; 制定lua脚本 -H, --header \u0026lt;H\u0026gt; 添加请求头 --latency 打印延迟分布信息 --timeout \u0026lt;T\u0026gt; 设置请求超时 -v, --version 打印版本信息 \u0026lt;N\u0026gt;表示数字参数，支持国际单位 (1k, 1M, 1G) \u0026lt;T\u0026gt;表示时间参数，支持国际单位 (2s, 2m, 2h) 简单使用及解释 # wrk -t1 -d20s -c10 -s post.lua http://api.example.com/fake/post 以单线程 保持10个连接 持续20秒 运行post.lua脚本访问http://api.example.com/fake/post\n报告分析 # 如下是一个简单的性能测试报告\nRunning 10s test @ http://api.example.com/vioce/url 1 threads and 1 connections Thread Stats Avg Stdev Max +/- Stdev Latency 1.71s 98.25ms 1.85s 75.00% Req/Sec 0.00 0.00 0.00 100.00% 5 requests in 10.04s, 1.55KB read Socket errors: connect 0, read 0, write 0, timeout 1 Requests/sec: 0.50 //QPS Transfer/sec: 157.88B //每秒处理数据 编写lua脚本实现post请求 # -- wrk 全局变量，改动之后会影响所有的请求 wrk = { scheme = \u0026#34;http\u0026#34;, host = \u0026#34;localhost\u0026#34;, port = nil, method = \u0026#34;GET\u0026#34;, path = \u0026#34;/\u0026#34;, headers = {}, body = nil, thread = \u0026lt;userdata\u0026gt;, } json格式 # 对于json格式数据发情post请求也是很简单的\n-- filename: post.json.lua -- 设置请求方式 wrk.method = \u0026#34;POST\u0026#34; -- json string wrk.body = \u0026#39;{\u0026#34;key1\u0026#34;: \u0026#34;val1\u0026#34;, \u0026#34;key2\u0026#34;: \u0026#34;val2\u0026#34;}\u0026#39; -- 设置content-type wrk.headers[\u0026#34;Content-Type\u0026#34;] = \u0026#34;application/json\u0026#34; 使用post.json.lua 脚本： wrk -t4 -d1m -c10 -s post.json.lua http://api.example.com/fake/post/json\n上传文件 # 实现上传文件测试，与json类似，也是设置wrk.body和wrk.headers, 只是body较麻烦一些\nwrk.method = \u0026#34;POST\u0026#34; wrk.headers[\u0026#34;Content-Type\u0026#34;] = \u0026#34;multipart/form-data;boundary=------WebKitFormBoundaryX3bY6PBMcxB1vCan\u0026#34; file = io.open(\u0026#34;path/to/fake.jpg\u0026#34;, \u0026#34;rb\u0026#34;) -- 拼装form-data form = \u0026#34;------WebKitFormBoundaryX3bY6PBMcxB1vCan\\r\\n\u0026#34; form = form .. \u0026#34;Content-Disposition: form-data; name=\u0026#34;file\u0026#34;; filename=\u0026#34;fake.jpg\u0026#34;\\r\\n\u0026#34; form = form .. \u0026#34;Content-Type: image/jpeg\\r\\n\\r\\n\u0026#34; form = form .. file:read(\u0026#34;*a\u0026#34;) form = form .. \u0026#34;\\r\\n------WebKitFormBoundaryX3bY6PBMcxB1vCan--\u0026#34; wrk.body = form 定制化wrk # 定制参考： https://github.com/wg/wrk/blob/master/SCRIPTING\n未完待续\n"},{"id":66,"href":"/2018/01/13/%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/","title":"持续集成-Jenkins","section":"Posts","content":"持续集成是一种软件开发实践，即团队开发成员经常集成它们的工作，通过每个成员每天至少集成一次，也就意味着每天可能会发生多次集成。每次集成都通过自动化的构建（包括编译，发布，自动化测试）来验证，从而尽早地发现集成错误。\n关于持续集成 # 要素\n. 统一的代码库 . 自动构建 . 自动测试 . 每个人每天都要向代码库主干提交代码 . 每次代码递交后都会在持续集成服务器上触发一次构建 . 保证快速构建 . 模拟生产环境的自动测试 . 每个人都可以很容易的获取最新可执行的应用程序 . 每个人都清楚正在发生的状况 . 自动化的部署\nJenkins的搭建与使用： # 前提：安装好Java环境\n. 下载参见下载地址, 我采用的是java -jar jenkins.war的方式部署 . 可能有用的教程 http://geek.csdn.net/news/detail/95824\n更改jenkins服务端口 # 使用java -jar jenkins.war这样的命令来启动jenkins时会使用默认的端口8080，有些情况下8080端口已经被我们使用了，这个时候如果希望修改这个端口应该怎么办呢？\n在命令行后面添加 --httpPort=8899，其实就是配置jetty的启动端口。如下：\nset JENKINS_HOME=./ java -Djsse.enableSNIExtension=false -jar path/to/jenkins.war --httpPort=8899 zsh设置Jenkins环境变量 # export JENKINS_HOME = \u0026#34;your/jenkins/path\u0026#34; 补充 # 于2018-1-26日更新\n鉴于尝试了drone和docker, 强烈建议采用docker部署jenkins，或者替换Jenkins为Drone\n最后 # 写得很简略，没有提供配置时候遇到的坑以及解决方法，遇到之后再补上。。。以上hahah\n"},{"id":67,"href":"/2018/01/13/pytest%E7%94%A8%E6%B3%95%E5%B0%8F%E7%BB%93/","title":"pytest用法小结","section":"Posts","content":"pytest最常用法总结，当然不止这一点功能。关于更多更强大的插件，可以根据自己需要来定制。\n安装 # pytest 安装和使用都非常简单, 只需pip install pytest\n编写测试代码 # 使用pytest，不需要像unittest模块一样，pytest使用的是python自带的assert，如：\ndef test_global_function(): assert 1 == 1 使用pytest.mark # pytest.mark 用于给测试方法打上标签，在稍后的执行中会讲到如何使用marker\n@pytest.mark.marker_self def test_global_function(): assert 1 == 1 使用pytest.fixture # @pytest.fixture def google_url(): return \u0026#34;http://google.com\u0026#34; setup 和 teardown # setup和teardown方法作用范围，分为全局作用，类作用，方法作用\n全局：作用于全局测试函数 类： 作用于自身类 类方法： 作用于类函数 简单举例: # # 全局 def setup_function(function): print(\u0026#34;setup function global\u0026#34;) def teardown_function(function): print(\u0026#34;teardown function global\u0026#34;) # 类 class Test_fixture: @classmethod def setup_class(cls): print(\u0026#34;class setup method\u0026#34;) @classmethod def teardown_class(cls): print(\u0026#34;class teardown method\u0026#34;) # 类方法 def setup_method(self, method): print(\u0026#34;class method setup function\u0026#34;) def teardown_method(self, method): print(\u0026#34;class method teardown function\u0026#34;) pytest配置文件 # 配置文件名为pytest.ini setup.cfg tox.ini 关于配置文件优先级请查阅官方文档 简单举例：\n[pytest] addopts = --maxfail=2 // set default pytest options python_classes = *Api //execute all *Api like TestSingUpApi or TestSignUpApi 更多配置内容参见https://docs.pytest.org/en/latest/customize.html\n执行方法 # 此处只列举了较常用的参数，主要用于演示marker和选中，和文件，类，函数的选中\n# 执行方法：pytest [options] test_file::Test_class::test_method [plugin-options] # 执行有标记为marker_self的case pytest -v -m \u0026#34;marker_self\u0026#34; test_demo.py::Test_fixture::test_fixture_demo # 执行有标记为marker_self 且 marker_other与的case pytest -v -m \u0026#34;marker_self and marker_other\u0026#34; test_demo.py::Test_fixture # 执行有标记为marker_self 或 marker_other的case pytest -v -m \u0026#34;yeqiag or marker_other\u0026#34; test_demo.py # 执行测试文件中某一个测试类的一个测试case pytest -v test_demo.py::Test_fixture::test_fixture_demo # 执行测试文件中某一个测试类 pytest -v test_demo.py::Test_fixture # 执行测试文件 pytest -v test_demo.py 全部代码 # \u0026#34;\u0026#34;\u0026#34; 1.setup teardown 作用范围： 全局的 作用于 全局的单个函数， class 作用于 该类的单个函数 2.mark的使用 使用全局的skip, xfail来标记并略过一部分的测试case 使用自定义的marker来区分测试等级 3.fixture的作用范围 全局作用 class内部作用 \u0026#34;\u0026#34;\u0026#34; import pytest def setup_function(function): print(\u0026#34;setup function global\u0026#34;) def teardown_function(function): print(\u0026#34;teardown function global\u0026#34;) def test_simple_method(): assert 1 == 1 @pytest.mark.marker_self def test_global_function(): assert 1 == 1 class Test_class_case: \u0026#34;\u0026#34;\u0026#34; Must Know These Things 1. 测试类 init函数不会被执行 2. 测试类 属性只读 \u0026#34;\u0026#34;\u0026#34; @classmethod def setup_class(cls): print(\u0026#34;class setup function\u0026#34;) @classmethod def teardown_class(cls): print(\u0026#34;class teardown function\u0026#34;) def setup_method(self, method): print(\u0026#34;class method setup function\u0026#34;) def teardown_method(self, method): print(\u0026#34;class method teardown function\u0026#34;) @pytest.mark.xfail(reason=\u0026#34;一定失败\u0026#34;) def test_case0(self): assert 1 == 0 def test_case1(self): assert 1 == 1 @pytest.mark.skip(reason=\u0026#34;我知道这是错的\u0026#34;) def test_case2(self): assert 1 == 0 def test_case3(self): assert True == False @pytest.fixture def google_url(): return \u0026#34;http://google.com\u0026#34; @pytest.mark.marker_self def test_fixture_google(google_url): assert google_url == \u0026#34;http://google.com\u0026#34; assert 1 == 0 def test_(google_url): assert google_url == \u0026#34;http;//google.com\u0026#34; def test_baidu_url(baidu_url): assert baidu_url == \u0026#34;http://baidu.com\u0026#34; class Test_fixture: @classmethod def setup_class(cls): print(\u0026#34;class setup method\u0026#34;) @classmethod def teardown_class(cls): print(\u0026#34;class teardown method\u0026#34;) @pytest.fixture def baidu_url(self): return \u0026#34;http://www.baidu.com\u0026#34; @pytest.mark.marker_self def test_fixture_demo(self, baidu_url): assert baidu_url == \u0026#34;http://www.baidu.com\u0026#34; assert 1 == 0 @pytest.mark.marker_other def test_fixture_demo_2(self, baidu_url): assert baidu_url == \u0026#34;http://google.com\u0026#34; @pytest.mark.marker_self @pytest.mark.skip(reason=\u0026#34;没有原因三\u0026#34;) def test_self_marker_demo0(self): assert 10 == 0 @pytest.mark.marker_self def test_self_marker_demo1(self): assert 10 == 0 @pytest.mark.marker_self @pytest.mark.marker_other def test_fixture_demo_google(self, google_url): assert google_url == \u0026#34;http://google.com\u0026#34; 执行结果截图\n未完待续\u0026hellip;\n"},{"id":68,"href":"/2018/01/11/git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/","title":"git命令清单","section":"Posts","content":"git命令清单\n基本命令 # git help \u0026lt;command\u0026gt; # 显示command的help git show # 显示某次提交的内容 git show $id git co -- \u0026lt;file\u0026gt; # 抛弃工作区修改 git co . # 抛弃工作区修改 git add \u0026lt;file\u0026gt; # 将工作文件修改提交到本地暂存区 git add . # 将所有修改过的工作文件提交暂存区 git rm \u0026lt;file\u0026gt; # 从版本库中删除文件 git rm \u0026lt;file\u0026gt; --cached # 从版本库中删除文件，但不删除文件 git reset \u0026lt;file\u0026gt; # 从暂存区恢复到工作文件 git reset -- . # 从暂存区恢复到工作文件 git reset --hard # 恢复最近一次提交过的状态，即放弃上次提交后的所有本次修改 # git ci \u0026lt;file\u0026gt; git ci . git ci -a # 将git add, git rm和git ci等操作都合并在一起做 git ci -am \u0026#34;some comments\u0026#34; git ci --amend # 修改最后一次提交记录 git revert \u0026lt;$id\u0026gt; # 恢复某次提交的状态，恢复动作本身也创建次提交对象 git revert HEAD # 恢复最后一次提交的状态 查看文件(diff) # git diff \u0026lt;file\u0026gt; # 比较当前文件和暂存区文件差异 git diff git diff \u0026lt;id1\u0026gt;\u0026lt;id1\u0026gt;\u0026lt;id2\u0026gt; # 比较两次提交之间的差异 git diff \u0026lt;branch1\u0026gt;..\u0026lt;branch2\u0026gt; # 在两个分支之间比较 git diff --staged # 比较暂存区和版本库差异 git diff --cached # 比较暂存区和版本库差异 git diff --stat # 仅仅比较统计信息 查看提交记录(log) # git log git log \u0026lt;file\u0026gt; # 查看该文件每次提交记录 git log -p \u0026lt;file\u0026gt; # 查看每次详细修改内容的diff git log -p -2 # 查看最近两次详细修改内容的diff git log --stat #查看提交统计信息 *tig: Mac上可以使用tig代替diff和log * brew install tig Git 本地分支管理 # 查看、切换、创建和删除分支\ngit br -r # 查看远程分支 git br \u0026lt;new_branch\u0026gt; # 创建新的分支 git br -v # 查看各个分支最后提交信息 git br --merged # 查看已经被合并到当前分支的分支 git br --no-merged # 查看尚未被合并到当前分支的分支 git co \u0026lt;branch\u0026gt; # 切换到某个分支 git co -b \u0026lt;new_branch\u0026gt; # 创建新的分支，并且切换过去 git co -b \u0026lt;new_branch\u0026gt; \u0026lt;branch\u0026gt; # 基于branch创建新的new_branch git co $id # 把某次历史提交记录checkout出来，但无分支信息，切换到其他分支会自动删除 git co $id -b \u0026lt;new_branch\u0026gt; # 把某次历史提交记录checkout出来，创建成一个分支 git br -d \u0026lt;branch\u0026gt; # 删除某个分支 git br -D \u0026lt;branch\u0026gt; # 强制删除某个分支 (未被合并的分支被删除的时候需要强制) ### 分支合并和rebase\ngit merge \u0026lt;branch\u0026gt; # 将branch分支合并到当前分支 git merge origin/master --no-ff # 不要Fast-Foward合并，这样可以生成merge提交 git rebase master \u0026lt;branch\u0026gt; # 将master rebase到branch，相当于： git co \u0026lt;branch\u0026gt; \u0026amp;\u0026amp; git rebase master \u0026amp;\u0026amp; git co master \u0026amp;\u0026amp; git merge \u0026lt;branch\u0026gt; Git补丁管理(方便在多台机器上开发同步时用) # git diff \u0026gt; ../sync.patch # 生成补丁 git apply ../sync.patch # 打补丁 git apply --check ../sync.patch #测试补丁能否成功 ### Git暂存管理\ngit stash # 暂存 git stash list # 列所有stash git stash apply # 恢复暂存的内容 git stash drop # 删除暂存区 Git远程分支管理 # git pull # 抓取远程仓库所有分支更新并合并到本地 git pull --no-ff # 抓取远程仓库所有分支更新并合并到本地，不要快进合并 git fetch origin # 抓取远程仓库更新 git merge origin/master # 将远程主分支合并到本地当前分支 git co --track origin/branch # 跟踪某个远程分支创建相应的本地分支 git co -b \u0026lt;local_branch\u0026gt; origin/\u0026lt;remote_branch\u0026gt; # 基于远程分支创建本地分支，功能同上 git push # push所有分支 git push origin master # 将本地主分支推到远程主分支 git push -u origin master # 将本地主分支推到远程(如无远程主分支则创建，用于初始化远程仓库) git push origin \u0026lt;local_branch\u0026gt; # 创建远程分支， origin是远程仓库名 git push origin \u0026lt;local_branch\u0026gt;:\u0026lt;remote_branch\u0026gt; # 创建远程分支 git push origin :\u0026lt;remote_branch\u0026gt; #先删除本地分支(git br -d \u0026lt;branch\u0026gt;)，然后再push删除远程分支 Git远程仓库管理 # GitHub\ngit remote -v # 查看远程服务器地址和仓库名称 git remote show origin # 查看远程服务器仓库状态 git remote add origin git@ github:robbin/robbin_site.git # 添加远程仓库地址 git remote set-url origin git@ github.com:robbin/robbin_site.git # 设置远程仓库地址(用于修改远程仓库地址) git remote rm \u0026lt;repository\u0026gt; # 删除远程仓库 创建远程仓库 # git clone --bare robbin_site robbin_site.git # 用带版本的项目创建纯版本仓库 scp -r my_project.git git@ git.csdn.net:~ # 将纯仓库上传到服务器上 mkdir robbin_site.git \u0026amp;\u0026amp; cd robbin_site.git \u0026amp;\u0026amp; git --bare init # 在服务器创建纯仓库 git remote add origin git@ github.com:robbin/robbin_site.git # 设置远程仓库地址 git push -u origin master # 客户端首次提交 git push -u origin develop # 首次将本地develop分支提交到远程develop分支，并且track git remote set-head origin master # 设置远程仓库的HEAD指向master分支 设置跟踪远程库和本地库 # git branch --set-upstream master origin/master git branch --set-upstream develop origin/develop "},{"id":69,"href":"/2018/01/11/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95%E5%9F%BA%E7%A1%80/","title":"数据库索引基础","section":"Posts","content":"数据库索引的简单介绍和使用注意事项\n树 # 二叉树 # 性质1：在二叉树中第 i 层的结点数最多为2^(i-1)（i ≥ 1） 性质2：高度为k的二叉树其结点总数最多为2^k－1（ k ≥ 1） 性质3：对任意的非空二叉树 T ，如果叶结点的个数为 n0，而其度为 2 的结点数为 n2，则：n0 = n2 + 1 二叉搜索树 BST # 若左子树不空，则左子树上所有结点的值均小于它的根结点的值； 若右子树不空，则右子树上所有结点的值均大于或等于它的根结点的值； 左、右子树也分别为二叉排序树； 没有键值相等的节点 平衡二叉树 AVL树 # 平衡二叉树（balanced binary tree）,又称 AVL 树。它或者是一棵空树,或者是具有如下性质的二叉树：\n它的左子树和右子树都是平衡二叉树， 左子树和右子树的深度之差的绝对值不超过1。 平衡二叉树是对二叉搜索树(又称为二叉排序树)的一种改进。二叉搜索树有一个缺点就是，树的结构是无法预料的，随意性很大，它只与节点的值和插入的顺序有关系，往往得到的是一个不平衡的二叉树。在最坏的情况下，可能得到的是一个单支二叉树，其高度和节点数相同，相当于一个单链表，对其正常的时间复杂度有O(log(n))变成了O(n)，从而丧失了二叉排序树的一些应该有的优点。\nB树 # BTree是平衡搜索多叉树，设树的度为2d（d\u0026gt;1），高度为h，那么BTree要满足以下条件：\n每个叶子结点的高度一样，等于h； 每个非叶子结点由n-1个key和n个指针point组成，其中d\u0026lt;=n\u0026lt;=2d,key和point相互间隔，结点两端一定是key； 叶子结点指针都为null； 非叶子结点的key都是[key,data]二元组，其中key表示作为索引的键，data为键值所在行的数据； B+树 # B+Tree是BTree的一个变种，设d为树的度数，h为树的高度，B+Tree和BTree的不同主要在于：\nB+Tree中的非叶子结点不存储数据，只存储键值； B+Tree的叶子结点没有指针，所有键值都会出现在叶子结点上，且key存储的键值对应data数据的物理地址； B+Tree的每个非叶子节点由n个键值key和n个指针point组成； 索引 # 聚簇索引和非聚簇索引（也叫：聚集和非聚集） # MyISAM 非聚簇索引\nMyISAM存储引擎采用的是非聚簇索引，非聚簇索引的主索引和辅助索引几乎是一样的，只是主索引不允许重复，不允许空值，他们的叶子结点的key都存储指向键值对应的数据的物理地址。非聚簇索引的数据表和索引表是分开存储的。非聚簇索引中的数据是根据数据的插入顺序保存。因此非聚簇索引更适合单个数据的查询。插入顺序不受键值影响。\nInnoDB 聚簇索引\n聚簇索引的主索引的叶子结点存储的是键值对应的数据本身，辅助索引的叶子结点存储的是键值对应的数据的主键键值。因此主键的值长度越小越好，类型越简单越好。聚簇索引的数据和主键索引存储在一起。聚簇索引的数据是根据主键的顺序保存。因此适合按主键索引的区间查找，可以有更少的磁盘I/O，加快查询速度。\n但是也是因为这个原因，聚簇索引的插入顺序最好按照主键单调的顺序插入，否则会频繁的引起页分裂，严重影响性能。在InnoDB中，如果只需要查找索引的列，就尽量不要加入其它的列，这样会提高查询效率。\n索引类型 # 主键索引：根据主键pk_clolum（length）建立索引，不允许重复，不允许空值。\n唯一索引：用来建立索引的列的值必须是唯一的，允许空值。\n全文索引：文本对象的列构建的索引，倒排索引。\n组合索引：用多个列组合构建的索引，这多个列中的值不允许有空值\nHash索引，BTree索引，前缀索引\n索引的优点 # 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。 可以大大加快数据的检索速度，这也是创建索引的最主要的原因。 可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义。 在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间。 通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。 索引优化 # TODO:\nQ\u0026amp;A # 为什么不对每一列都创建索引? # 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。 索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。 一个表最多16列索引。 应该选择哪些列来创建索引？ # 在经常需要搜索的列上，可以加快搜索的速度； 在作为主键的列上，强制该列的唯一性和组织表中数据的排列结构； 在经常用在连接的列上，这些列主要是一些外键，可以加快连接的速度； 在经常需要根据范围进行搜索的列上创建索引，因为索引已经排序，其指定的范围是连续的； 在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间；在经常使用在WHERE子句中的列上面创建索引，加快条件的判断速度。 哪些列不应该创建索引？ # 对于那些在查询中很少使用或者参考的列不应该创建索引。这些列很少使用到，因此有索引或者无索引，并不能提高查询速度。反而由于增加了索引，降低了系统的维护速度和增大了空间需求。 对于那些只有很少数据值的列也不应该增加索引。这是因为，由于这些列的取值很少，例如人事表的性别列，在查询的结果中，结果集的数据行占了表中数据行的很大比例，即需要在表中搜索的数据行的比例很大。增加索引，并不能明显加快检索速度。 对于那些定义为text, image和bit数据类型的列不应该增加索引。这是因为，这些列的数据量要么相当大，要么取值很少。 当修改性能远远大于检索性能时，不应该创建索引。这是因为，修改性能和检索性能是互相矛盾的。当增加索引时，会提高检索性能，但是会降低修改性能。当减少索引时，会提高修改性能，降低检索性能。因此，当修改性能远远大于检索性能时，不应该创建索引。 参考 # https://hit-alibaba.github.io/interview/basic/algo/Tree.html https://blog.csdn.net/tongdanping/article/details/79878302· "},{"id":70,"href":"/2018/01/11/RN%E5%8E%86%E9%99%A9%E8%AE%B0/","title":"RN历险记","section":"Posts","content":"讲述配置ReactNative的心酸历程\n程序猿长征第一步 # 根据官方文档来安装RN, 以及巨大无比的Xcode Ver9.0.0\n错误一：Build Fail # 可能描述不太一致, 但是原因都差不多, 文件缺失。 菜鸟想必看到这些个报错, 两眼一懵逼, 啥子情况, 怎么和官方的描述不一致, 一个及其简单的RN-Demo, 我就是想跑一下的喂！\n我遇到的情况, 分为两种：\n其一是安装很慢, 之后失败 其二是安装很快, 然后失败\n经过反复的“瞎子”调整, 在多次更换react, react-native版本, 求助Google大叔无果之后。我开始了阅读输出日志的漫漫长路, 终于发现了build失败的元凶, boost/xxx.hpp not found为啥找不到呢, 去文件夹一看, 才发现这些文件真的不存在\u0026hellip;\u0026hellip;\n好了知道错误, 就再Google下咯（其实我还去改过这些#includ\u0026lt;boost/config/user.hpp\u0026gt; 0\u0026lt;~\u0026gt;0）这里就直接给出我找的结果吧: http://cdn2.jianshu.io/p/2ef019a7e82a\n总的说来, 就是自动下载的的第三方库是残缺的\n错误二：CFBundleIdentifier not Found # 第二错误也是困扰了比较多人, 我遇到的只是导致这个情况的其中之一\n通过查看输出日志, 并没有发现什么有用的信息, 提示的是Command Fail , balabala… 手动搜索了一下PlistBuddy, 了解了下用法, 然后我手动执行了下命令, 居然可以！！！！什么情况, 那为什么提示错误信息？果断进入到文件夹中查看,果然文件是存在的那么为啥一个可以, 一个不可以呢？到这里, 大致猜到原因了, 没有找到文件\n再次以此为点求助Google大叔: http://blog.csdn.net/ohyeahhhh/article/details/54691512\n这个坑就是, Xcode编译保存的路径和react-native-cli寻找的路径不一致, 通过修改路径就OK啦, 还有其他原因导致的这个fail 请参阅链接, 先搞清楚react-native run-ios做了啥事.\n中间更多细节”虐“去~~~~, 感谢预先踩坑的前辈, 最终项目成功运行啦！\n心得：\n~不要怕麻烦, 先看错误日志, 定位错误, 那样检索的时候更有方向, 范围更小 ~对于报错不清楚的command fail, 尽量自己去执行, 明确错误（结合查看文件判断） ~好好用Google ~笑着活下去 配置 参数 OS MacPro 10.13.1 Node v8.3.0 Npm v5.6.0 React-native 0.46.4 React-native-cli 0.2.1 "}]